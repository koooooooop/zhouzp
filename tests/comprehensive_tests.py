"""
MÂ²-MOEP å…¨æ–¹ä½æµ‹è¯•å¥—ä»¶
æµ‹è¯•è¦†ç›–ï¼šFlowæ¨¡å‹ã€é—¨æ§ã€ä¸“å®¶ç½‘ç»œã€æŸå¤±å‡½æ•°ã€Top-kç¨€ç–ã€å¯å­¦ä¹ Î”ç­‰
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import warnings

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from models.m2_moep import M2_MOEP
from models.flow import PowerfulNormalizingFlow, FlowLayer
from models.gating import GatingEncoder
from models.expert import FFTmsMambaExpert
from utils.losses import CompositeLoss, TripletLoss
from utils.metrics import calculate_metrics, compute_expert_metrics


class ComprehensiveTestSuite:
    """å…¨æ–¹ä½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.test_results = []
        self.config = self._get_test_config()
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _get_test_config(self):
        """æµ‹è¯•é…ç½®"""
        return {
            'model': {
                'input_dim': 6,
                'output_dim': 6,
                'hidden_dim': 128,
                'num_experts': 4,
                'seq_len': 24,
                'pred_len': 12,
                'expert_params': {
                    'mamba_d_model': 128,
                    'mamba_scales': [1, 2, 4]
                },
                'flow': {
                    'latent_dim': 64,
                    'use_pretrained': False,
                    'hidden_dim': 128,
                    'num_coupling_layers': 4
                },
                'diversity': {
                    'prototype_dim': 32,
                    'num_prototypes': 8,
                    'force_diversity': True
                },
                'temperature': {
                    'initial': 1.0,
                    'min': 0.1,
                    'max': 10.0
                },
                'triplet': {
                    'margin': 0.5,
                    'mining_strategy': 'batch_hard'
                },
                'top_k': 3,  # å¯ç”¨Top-kç¨€ç–
                'embedding_dim': 64
            },
            'training': {
                'loss_weights': {
                    'init_sigma_rc': 1.0,
                    'init_sigma_cl': 1.0,
                    'init_sigma_pr': 1.0,
                    'init_sigma_consistency': 1.0,
                    'init_sigma_balance': 1.0
                },
                'triplet_margin': 0.5,
                'aux_loss_weight': 0.01
            },
            'data': {
                'seq_len': 24,
                'pred_len': 12,
                'batch_size': 8
            }
        }
    
    def log_test(self, test_name, status, details=""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'test': test_name,
            'status': status,
            'details': details
        }
        self.test_results.append(result)
        status_icon = "âœ…" if status == "PASS" else "âŒ"
        print(f"{status_icon} {test_name}: {details}")
    
    def test_flow_model(self):
        """æµ‹è¯•Flowæ¨¡å‹åŠŸèƒ½"""
        try:
            # åˆ›å»ºFlowæ¨¡å‹
            input_dim = self.config['model']['seq_len'] * self.config['model']['input_dim']
            flow = PowerfulNormalizingFlow(
                input_dim=input_dim,
                latent_dim=self.config['model']['flow']['latent_dim'],
                hidden_dim=self.config['model']['flow']['hidden_dim']
            ).to(self.device)
            
            batch_size = 4
            x = torch.randn(batch_size, input_dim).to(self.device)
            
            # æµ‹è¯•ç¼–ç 
            z = flow.encode(x)
            assert z.shape == (batch_size, self.config['model']['flow']['latent_dim'])
            
            # æµ‹è¯•é‡æ„
            x_recon = flow.reconstruct(x)
            assert x_recon.shape == x.shape
            
            # æµ‹è¯•æ•°å€¼ç¨³å®šæ€§
            assert torch.isfinite(z).all(), "Flowç¼–ç åŒ…å«NaN/Inf"
            assert torch.isfinite(x_recon).all(), "Flowé‡æ„åŒ…å«NaN/Inf"
            
            # æµ‹è¯•FlowLayerçš„log_detä¿®å¤
            flow_layer = FlowLayer(input_dim).to(self.device)
            z_layer, log_det = flow_layer(x)
            assert log_det.shape == (batch_size,), f"log_detå½¢çŠ¶é”™è¯¯: {log_det.shape}"
            
            self.log_test("Flowæ¨¡å‹æµ‹è¯•", "PASS", f"ç¼–ç : {z.shape}, é‡æ„: {x_recon.shape}")
            
        except Exception as e:
            self.log_test("Flowæ¨¡å‹æµ‹è¯•", "FAIL", str(e))
    
    def test_gating_network(self):
        """æµ‹è¯•é—¨æ§ç½‘ç»œ"""
        try:
            gating = GatingEncoder(self.config).to(self.device)
            
            batch_size = 4
            latent_dim = self.config['model']['flow']['latent_dim']
            z_latent = torch.randn(batch_size, latent_dim).to(self.device)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            gating_output = gating(z_latent)
            assert gating_output.shape == (batch_size, self.config['model']['num_experts'])
            
            # æµ‹è¯•åµŒå…¥æå–
            embeddings = gating.get_embeddings(z_latent)
            assert embeddings.shape == (batch_size, self.config['model']['embedding_dim'])
            
            # æµ‹è¯•ä¸“å®¶æƒé‡è®¡ç®—
            expert_weights = F.softmax(gating_output, dim=-1)
            assert torch.allclose(expert_weights.sum(dim=-1), torch.ones(batch_size).to(self.device), atol=1e-6)
            
            self.log_test("é—¨æ§ç½‘ç»œæµ‹è¯•", "PASS", f"è¾“å‡º: {gating_output.shape}, åµŒå…¥: {embeddings.shape}")
            
        except Exception as e:
            self.log_test("é—¨æ§ç½‘ç»œæµ‹è¯•", "FAIL", str(e))
    
    def test_expert_network(self):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œï¼ˆåŒ…æ‹¬å¯å­¦ä¹ Î”ï¼‰"""
        try:
            expert = FFTmsMambaExpert(self.config).to(self.device)
            
            batch_size = 4
            seq_len = self.config['model']['seq_len']
            input_dim = self.config['model']['input_dim']
            pred_len = self.config['model']['pred_len']
            
            x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            output = expert(x)
            expected_shape = (batch_size, pred_len, self.config['model']['output_dim'])
            assert output.shape == expected_shape, f"ä¸“å®¶è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} vs {expected_shape}"
            
            # æµ‹è¯•å¯å­¦ä¹ Î”å‚æ•°
            assert hasattr(expert, 'learnable_deltas'), "ç¼ºå°‘å¯å­¦ä¹ Î”å‚æ•°"
            assert expert.learnable_deltas.requires_grad, "Î”å‚æ•°ä¸å¯è®­ç»ƒ"
            
            # æµ‹è¯•FFTèåˆ
            projected = expert.input_projection(x)
            fused_features = expert._early_fft_fusion(projected)
            expected_fused_shape = (batch_size, seq_len, expert.d_model)
            assert fused_features.shape == expected_fused_shape
            
            self.log_test("ä¸“å®¶ç½‘ç»œæµ‹è¯•", "PASS", f"è¾“å‡º: {output.shape}, FFTèåˆ: {fused_features.shape}")
            
        except Exception as e:
            self.log_test("ä¸“å®¶ç½‘ç»œæµ‹è¯•", "FAIL", str(e))
    
    def test_top_k_sparse_gating(self):
        """æµ‹è¯•Top-kç¨€ç–é—¨æ§"""
        try:
            model = M2_MOEP(self.config).to(self.device)
            model.eval()
            
            batch_size = 4
            seq_len = self.config['model']['seq_len']
            input_dim = self.config['model']['input_dim']
            
            x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            
            with torch.no_grad():
                output = model(x, return_aux_info=True)
                expert_weights = output['aux_info']['expert_weights']
                
                # æ£€æŸ¥Top-kç¨€ç–æ€§
                top_k = self.config['model']['top_k']
                for i in range(batch_size):
                    non_zero_count = (expert_weights[i] > 1e-8).sum().item()
                    assert non_zero_count <= top_k, f"æ ·æœ¬{i}æ¿€æ´»äº†{non_zero_count}ä¸ªä¸“å®¶ï¼Œè¶…è¿‡top_k={top_k}"
                
                # æ£€æŸ¥æƒé‡å½’ä¸€åŒ–
                weight_sums = expert_weights.sum(dim=-1)
                assert torch.allclose(weight_sums, torch.ones(batch_size).to(self.device), atol=1e-6), "Top-kæƒé‡æœªæ­£ç¡®å½’ä¸€åŒ–"
            
            self.log_test("Top-kç¨€ç–é—¨æ§æµ‹è¯•", "PASS", f"æ¯æ ·æœ¬æœ€å¤šæ¿€æ´»{top_k}ä¸ªä¸“å®¶")
            
        except Exception as e:
            self.log_test("Top-kç¨€ç–é—¨æ§æµ‹è¯•", "FAIL", str(e))
    
    def test_uncertainty_weighted_loss(self):
        """æµ‹è¯•ä¸ç¡®å®šæ€§åŠ æƒæŸå¤±"""
        try:
            criterion = CompositeLoss(self.config).to(self.device)
            
            # æ£€æŸ¥å¯å­¦ä¹ Ïƒå‚æ•°
            sigma_params = ['log_sigma_rc', 'log_sigma_cl', 'log_sigma_pr', 'log_sigma_cons', 'log_sigma_bal']
            for param_name in sigma_params:
                assert hasattr(criterion, param_name), f"ç¼ºå°‘{param_name}å‚æ•°"
                param = getattr(criterion, param_name)
                assert param.requires_grad, f"{param_name}ä¸å¯è®­ç»ƒ"
            
            # æµ‹è¯•æŸå¤±è®¡ç®—
            batch_size = 4
            pred_len = self.config['model']['pred_len']
            output_dim = self.config['model']['output_dim']
            
            predictions = torch.randn(batch_size, pred_len, output_dim).to(self.device)
            targets = torch.randn(batch_size, pred_len, output_dim).to(self.device)
            
            # æ¨¡æ‹Ÿè¾…åŠ©ä¿¡æ¯
            aux_info = {
                'expert_weights': torch.softmax(torch.randn(batch_size, 4).to(self.device), dim=-1),
                'expert_features': torch.randn(batch_size, 32).to(self.device),
                'gating_embeddings': torch.randn(batch_size, 64).to(self.device),
                'reconstruction_loss': torch.tensor(0.1).to(self.device),
                'triplet_loss': torch.tensor(0.05).to(self.device),
                'load_balance_loss': torch.tensor(0.02).to(self.device),
                'prototype_loss': torch.tensor(0.03).to(self.device)
            }
            
            losses = criterion(predictions, targets, aux_info)
            
            # æ£€æŸ¥æ‰€æœ‰æŸå¤±é¡¹
            expected_keys = ['prediction', 'reconstruction', 'triplet', 'contrastive', 
                           'consistency', 'load_balance', 'prototype', 'total']
            for key in expected_keys:
                assert key in losses, f"ç¼ºå°‘æŸå¤±é¡¹: {key}"
                assert torch.isfinite(losses[key]), f"{key}æŸå¤±åŒ…å«NaN/Inf"
            
            self.log_test("ä¸ç¡®å®šæ€§åŠ æƒæŸå¤±æµ‹è¯•", "PASS", f"æ€»æŸå¤±: {losses['total']:.4f}")
            
        except Exception as e:
            self.log_test("ä¸ç¡®å®šæ€§åŠ æƒæŸå¤±æµ‹è¯•", "FAIL", str(e))
    
    def test_kl_consistency_loss(self):
        """æµ‹è¯•KLä¸€è‡´æ€§æŸå¤±"""
        try:
            criterion = CompositeLoss(self.config).to(self.device)
            
            batch_size = 6
            num_experts = 4
            embedding_dim = 64
            
            # åˆ›å»ºç›¸ä¼¼çš„åµŒå…¥å¯¹
            embeddings = torch.randn(batch_size, embedding_dim).to(self.device)
            embeddings[1] = embeddings[0] + 0.1 * torch.randn(embedding_dim).to(self.device)  # ç›¸ä¼¼æ ·æœ¬
            
            routing_weights = torch.softmax(torch.randn(batch_size, num_experts).to(self.device), dim=-1)
            routing_weights[1] = routing_weights[0] + 0.05 * torch.randn(num_experts).to(self.device)  # ç›¸ä¼¼è·¯ç”±
            routing_weights[1] = torch.softmax(routing_weights[1], dim=-1)
            
            # è®¡ç®—KLä¸€è‡´æ€§æŸå¤±
            kl_loss = criterion.compute_kl_consistency_loss(routing_weights, embeddings)
            
            assert torch.isfinite(kl_loss), "KLä¸€è‡´æ€§æŸå¤±åŒ…å«NaN/Inf"
            assert kl_loss >= 0, "KLæ•£åº¦åº”ä¸ºéè´Ÿ"
            
            self.log_test("KLä¸€è‡´æ€§æŸå¤±æµ‹è¯•", "PASS", f"KLæŸå¤±: {kl_loss:.4f}")
            
        except Exception as e:
            self.log_test("KLä¸€è‡´æ€§æŸå¤±æµ‹è¯•", "FAIL", str(e))
    
    def test_triplet_mining(self):
        """æµ‹è¯•åŸºäºé¢„æµ‹æ€§èƒ½çš„ä¸‰å…ƒç»„æŒ–æ˜"""
        try:
            model = M2_MOEP(self.config).to(self.device)
            
            batch_size = 6
            seq_len = self.config['model']['seq_len']
            input_dim = self.config['model']['input_dim']
            pred_len = self.config['model']['pred_len']
            output_dim = self.config['model']['output_dim']
            num_experts = self.config['model']['num_experts']
            
            x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            ground_truth = torch.randn(batch_size, pred_len, output_dim).to(self.device)
            expert_weights = torch.softmax(torch.randn(batch_size, num_experts).to(self.device), dim=-1)
            expert_predictions = torch.randn(batch_size, num_experts, pred_len, output_dim).to(self.device)
            
            # æµ‹è¯•ä¸‰å…ƒç»„æŒ–æ˜
            triplets = model.mine_triplets_based_on_prediction_performance(
                x, expert_weights, expert_predictions, ground_truth
            )
            
            # éªŒè¯ä¸‰å…ƒç»„æ ¼å¼
            for triplet in triplets:
                assert len(triplet) == 3, "ä¸‰å…ƒç»„åº”åŒ…å«3ä¸ªå…ƒç´ "
                anchor, pos, neg = triplet
                assert 0 <= anchor < batch_size
                assert 0 <= pos < batch_size
                assert 0 <= neg < batch_size
                assert anchor != pos and anchor != neg, "é”šç‚¹ä¸åº”ä¸æ­£è´Ÿæ ·æœ¬ç›¸åŒ"
            
            self.log_test("ä¸‰å…ƒç»„æŒ–æ˜æµ‹è¯•", "PASS", f"æŒ–æ˜åˆ°{len(triplets)}ä¸ªä¸‰å…ƒç»„")
            
        except Exception as e:
            self.log_test("ä¸‰å…ƒç»„æŒ–æ˜æµ‹è¯•", "FAIL", str(e))
    
    def test_end_to_end_training(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹"""
        try:
            model = M2_MOEP(self.config).to(self.device)
            criterion = CompositeLoss(self.config).to(self.device)
            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
            
            batch_size = 4
            seq_len = self.config['model']['seq_len']
            input_dim = self.config['model']['input_dim']
            pred_len = self.config['model']['pred_len']
            output_dim = self.config['model']['output_dim']
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            model.train()
            for step in range(3):
                optimizer.zero_grad()
                
                x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
                y = torch.randn(batch_size, pred_len, output_dim).to(self.device)
                
                # å‰å‘ä¼ æ’­
                output = model(x, ground_truth=y, return_aux_info=True)
                predictions = output['predictions']
                aux_info = output['aux_info']
                
                # æŸå¤±è®¡ç®—
                losses = criterion(predictions, y, aux_info)
                
                # åå‘ä¼ æ’­
                losses['total'].backward()
                
                # æ¢¯åº¦è£å‰ª
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                assert torch.isfinite(grad_norm), f"æ­¥éª¤{step}æ¢¯åº¦åŒ…å«NaN/Inf"
                
                optimizer.step()
                
                # æ¸©åº¦è°ƒåº¦
                expert_entropy = -torch.sum(
                    aux_info['expert_weights'].mean(0) * 
                    torch.log(aux_info['expert_weights'].mean(0) + 1e-8)
                )
                model.update_temperature_schedule(step, expert_entropy)
            
            self.log_test("ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•", "PASS", f"å®Œæˆ3æ­¥è®­ç»ƒï¼Œæœ€ç»ˆæŸå¤±: {losses['total']:.4f}")
            
        except Exception as e:
            self.log_test("ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•", "FAIL", str(e))
    
    def test_metrics_calculation(self):
        """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
        try:
            batch_size = 8
            pred_len = 12
            input_dim = 6
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            predictions = torch.randn(batch_size, pred_len, input_dim)
            targets = torch.randn(batch_size, pred_len, input_dim)
            expert_weights = torch.softmax(torch.randn(batch_size, 4), dim=-1)
            
            # æµ‹è¯•é¢„æµ‹æŒ‡æ ‡
            pred_metrics = calculate_metrics(predictions.numpy(), targets.numpy())
            expected_metrics = ['MSE', 'RMSE', 'MAE', 'MAPE', 'R2']
            for metric in expected_metrics:
                assert metric in pred_metrics, f"ç¼ºå°‘æŒ‡æ ‡: {metric}"
                assert np.isfinite(pred_metrics[metric]), f"{metric}åŒ…å«NaN/Inf"
            
            # æµ‹è¯•ä¸“å®¶æŒ‡æ ‡
            expert_metrics = compute_expert_metrics(expert_weights)
            expected_expert_metrics = ['expert_entropy', 'normalized_entropy', 'gini_coefficient', 'active_experts']
            for metric in expected_expert_metrics:
                assert metric in expert_metrics, f"ç¼ºå°‘ä¸“å®¶æŒ‡æ ‡: {metric}"
                assert np.isfinite(expert_metrics[metric]), f"ä¸“å®¶{metric}åŒ…å«NaN/Inf"
            
            self.log_test("æŒ‡æ ‡è®¡ç®—æµ‹è¯•", "PASS", f"RMSE: {pred_metrics['RMSE']:.4f}, ä¸“å®¶ç†µ: {expert_metrics['expert_entropy']:.4f}")
            
        except Exception as e:
            self.log_test("æŒ‡æ ‡è®¡ç®—æµ‹è¯•", "FAIL", str(e))
    
    def test_model_serialization(self):
        """æµ‹è¯•æ¨¡å‹åºåˆ—åŒ–"""
        try:
            model = M2_MOEP(self.config)
            
            # ä¿å­˜æ¨¡å‹
            torch.save(model.state_dict(), '/tmp/test_model.pth')
            
            # åŠ è½½æ¨¡å‹
            model_new = M2_MOEP(self.config)
            model_new.load_state_dict(torch.load('/tmp/test_model.pth'))
            
            # éªŒè¯å‚æ•°ä¸€è‡´æ€§
            for (name1, param1), (name2, param2) in zip(model.named_parameters(), model_new.named_parameters()):
                assert name1 == name2, f"å‚æ•°åä¸åŒ¹é…: {name1} vs {name2}"
                assert torch.allclose(param1, param2), f"å‚æ•°å€¼ä¸åŒ¹é…: {name1}"
            
            # æ¸…ç†
            os.remove('/tmp/test_model.pth')
            
            self.log_test("æ¨¡å‹åºåˆ—åŒ–æµ‹è¯•", "PASS", "ä¿å­˜å’ŒåŠ è½½æˆåŠŸ")
            
        except Exception as e:
            self.log_test("æ¨¡å‹åºåˆ—åŒ–æµ‹è¯•", "FAIL", str(e))
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹MÂ²-MOEPå…¨æ–¹ä½æµ‹è¯•...")
        print("=" * 60)
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        test_methods = [
            self.test_flow_model,
            self.test_gating_network,
            self.test_expert_network,
            self.test_top_k_sparse_gating,
            self.test_uncertainty_weighted_loss,
            self.test_kl_consistency_loss,
            self.test_triplet_mining,
            self.test_end_to_end_training,
            self.test_metrics_calculation,
            self.test_model_serialization
        ]
        
        for test_method in test_methods:
            try:
                test_method()
            except Exception as e:
                self.log_test(test_method.__name__, "FAIL", f"æœªæ•è·å¼‚å¸¸: {str(e)}")
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['status'] == 'PASS')
        failed_tests = total_tests - passed_tests
        
        print("=" * 60)
        print(f"ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   é€šè¿‡: {passed_tests} âœ…")
        print(f"   å¤±è´¥: {failed_tests} âŒ")
        print(f"   æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        
        if failed_tests == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é¡¹ç›®ä»£ç è´¨é‡è‰¯å¥½ã€‚")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚")
            
        return failed_tests == 0


if __name__ == "__main__":
    # å¿½ç•¥è­¦å‘Š
    warnings.filterwarnings("ignore")
    
    test_suite = ComprehensiveTestSuite()
    success = test_suite.run_all_tests()
    
    # é€€å‡ºç 
    sys.exit(0 if success else 1) 