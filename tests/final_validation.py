"""
MÂ²-MOEP æœ€ç»ˆéªŒè¯è„šæœ¬
æ·±å…¥éªŒè¯æ¨¡å‹æ€§èƒ½ã€æ•°å€¼ç¨³å®šæ€§å’Œè®ºæ–‡ä¸€è‡´æ€§
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import time
import warnings

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from models.m2_moep import M2_MOEP
from utils.losses import CompositeLoss
from utils.metrics import calculate_metrics, compute_expert_metrics

class FinalValidationSuite:
    """æœ€ç»ˆéªŒè¯å¥—ä»¶"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.config = self._get_validation_config()
        print(f"ğŸ”§ éªŒè¯ç¯å¢ƒ: {self.device}")
        print(f"ğŸ§ª å¼€å§‹æ·±å…¥éªŒè¯...")
        
    def _get_validation_config(self):
        """éªŒè¯é…ç½®"""
        return {
            'model': {
                'input_dim': 8,
                'output_dim': 8,
                'hidden_dim': 256,
                'num_experts': 6,
                'seq_len': 48,
                'pred_len': 24,
                'expert_params': {
                    'mamba_d_model': 256,
                    'mamba_scales': [1, 2, 4, 8]
                },
                'flow': {
                    'latent_dim': 128,
                    'use_pretrained': False,
                    'hidden_dim': 256,
                    'num_coupling_layers': 8
                },
                'diversity': {
                    'prototype_dim': 64,
                    'num_prototypes': 12,
                    'force_diversity': True
                },
                'temperature': {
                    'initial': 1.0,
                    'min': 0.1,
                    'max': 5.0
                },
                'triplet': {
                    'margin': 0.5,
                    'mining_strategy': 'batch_hard'
                },
                'top_k': 4,
                'embedding_dim': 128
            },
            'training': {
                'loss_weights': {
                    'init_sigma_rc': 1.0,
                    'init_sigma_cl': 1.0,
                    'init_sigma_pr': 1.0,
                    'init_sigma_consistency': 1.0,
                    'init_sigma_balance': 1.0
                },
                'triplet_margin': 0.5,
                'aux_loss_weight': 0.01
            },
            'data': {
                'seq_len': 48,
                'pred_len': 24,
                'batch_size': 16
            }
        }
    
    def validate_model_architecture(self):
        """éªŒè¯æ¨¡å‹æ¶æ„å®Œæ•´æ€§"""
        print("\nğŸ“ éªŒè¯æ¨¡å‹æ¶æ„...")
        
        model = M2_MOEP(self.config).to(self.device)
        
        # æ£€æŸ¥å…³é”®ç»„ä»¶
        components = {
            'flow_model': model.flow_model,
            'gating': model.gating,
            'experts': model.experts,
            'log_temperature': model.log_temperature
        }
        
        for name, component in components.items():
            assert component is not None, f"ç¼ºå°‘ç»„ä»¶: {name}"
            if hasattr(component, '__class__'):
                print(f"   âœ… {name}: {type(component).__name__}")
            else:
                print(f"   âœ… {name}: Parameter")
        
        # æ£€æŸ¥ä¸“å®¶æ•°é‡
        assert len(model.experts) == self.config['model']['num_experts'], f"ä¸“å®¶æ•°é‡ä¸åŒ¹é…"
        print(f"   ğŸ”¢ ä¸“å®¶æ•°é‡: {len(model.experts)}")
        
        # æ£€æŸ¥å¯å­¦ä¹ å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"   ğŸ“Š æ€»å‚æ•°: {total_params:,}")
        print(f"   ğŸ”§ å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
    
    def validate_numerical_stability(self, model):
        """éªŒè¯æ•°å€¼ç¨³å®šæ€§"""
        print("\nğŸ”¢ éªŒè¯æ•°å€¼ç¨³å®šæ€§...")
        
        batch_size = 8
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        # æµ‹è¯•ä¸åŒæ•°å€¼èŒƒå›´çš„è¾“å…¥
        test_cases = [
            ("æ­£å¸¸èŒƒå›´", torch.randn(batch_size, seq_len, input_dim)),
            ("å¤§æ•°å€¼", torch.randn(batch_size, seq_len, input_dim) * 10),
            ("å°æ•°å€¼", torch.randn(batch_size, seq_len, input_dim) * 0.1),
            ("æå€¼", torch.randn(batch_size, seq_len, input_dim) * 100)
        ]
        
        model.eval()
        with torch.no_grad():
            for case_name, x in test_cases:
                x = x.to(self.device)
                try:
                    output = model(x, return_aux_info=True)
                    predictions = output['predictions']
                    
                    # æ£€æŸ¥è¾“å‡ºçš„æ•°å€¼ç¨³å®šæ€§
                    assert torch.isfinite(predictions).all(), f"{case_name}: é¢„æµ‹åŒ…å«NaN/Inf"
                    assert not torch.isnan(predictions).any(), f"{case_name}: é¢„æµ‹åŒ…å«NaN"
                    
                    print(f"   âœ… {case_name}: æ•°å€¼ç¨³å®š")
                    
                except Exception as e:
                    print(f"   âŒ {case_name}: {str(e)}")
    
    def validate_performance_scaling(self, model):
        """éªŒè¯æ€§èƒ½æ‰©å±•æ€§"""
        print("\nâš¡ éªŒè¯æ€§èƒ½æ‰©å±•æ€§...")
        
        batch_sizes = [1, 4, 8, 16, 32]
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        model.eval()
        performance_data = []
        
        for batch_size in batch_sizes:
            x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            
            # é¢„çƒ­
            with torch.no_grad():
                _ = model(x)
            
            # æ€§èƒ½æµ‹è¯•
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(5):
                    _ = model(x)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.time()
            
            avg_time = (end_time - start_time) / 5
            throughput = batch_size / avg_time
            
            performance_data.append((batch_size, avg_time, throughput))
            print(f"   ğŸ“Š æ‰¹å¤§å° {batch_size}: {avg_time:.4f}s, ååé‡: {throughput:.2f} samples/s")
        
        return performance_data
    
    def validate_expert_diversity(self, model):
        """éªŒè¯ä¸“å®¶å¤šæ ·æ€§"""
        print("\nğŸ­ éªŒè¯ä¸“å®¶å¤šæ ·æ€§...")
        
        batch_size = 32
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        # ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å…¥
        inputs = []
        for i in range(4):
            # ä¸åŒçš„ä¿¡å·ç±»å‹
            if i == 0:  # æ­£å¼¦æ³¢
                t = torch.linspace(0, 4*np.pi, seq_len).unsqueeze(0).unsqueeze(-1)
                x = torch.sin(t + torch.randn(1, 1, 1) * 0.1).expand(batch_size//4, -1, input_dim)
            elif i == 1:  # é”¯é½¿æ³¢
                t = torch.linspace(0, 4*np.pi, seq_len).unsqueeze(0).unsqueeze(-1)
                x = (t % (2*np.pi) - np.pi).expand(batch_size//4, -1, input_dim)
            elif i == 2:  # éšæœºæ¸¸èµ°
                x = torch.randn(batch_size//4, seq_len, input_dim).cumsum(dim=1)
            else:  # ç™½å™ªå£°
                x = torch.randn(batch_size//4, seq_len, input_dim)
            
            inputs.append(x)
        
        x = torch.cat(inputs, dim=0).to(self.device)
        
        model.eval()
        with torch.no_grad():
            output = model(x, return_aux_info=True)
            expert_weights = output['aux_info']['expert_weights']
        
        # è®¡ç®—ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        expert_usage = expert_weights.mean(dim=0)
        expert_entropy = -torch.sum(expert_usage * torch.log(expert_usage + 1e-8))
        max_entropy = np.log(self.config['model']['num_experts'])
        normalized_entropy = expert_entropy / max_entropy
        
        print(f"   ğŸ“ˆ ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ: {expert_usage.cpu().numpy()}")
        print(f"   ğŸ”€ ä¸“å®¶ç†µ: {expert_entropy:.4f} (å½’ä¸€åŒ–: {normalized_entropy:.4f})")
        
        # éªŒè¯Top-kç¨€ç–æ€§
        top_k = self.config['model']['top_k']
        active_experts_per_sample = (expert_weights > 1e-6).sum(dim=1).float().mean()
        print(f"   ğŸ¯ å¹³å‡æ¿€æ´»ä¸“å®¶æ•°: {active_experts_per_sample:.2f} (Top-k: {top_k})")
        
        return {
            'expert_entropy': expert_entropy.item(),
            'normalized_entropy': normalized_entropy.item(),
            'avg_active_experts': active_experts_per_sample.item()
        }
    
    def validate_loss_components(self, model):
        """éªŒè¯æŸå¤±ç»„ä»¶"""
        print("\nğŸ’” éªŒè¯æŸå¤±ç»„ä»¶...")
        
        criterion = CompositeLoss(self.config).to(self.device)
        
        batch_size = 16
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        pred_len = self.config['model']['pred_len']
        output_dim = self.config['model']['output_dim']
        
        x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
        y = torch.randn(batch_size, pred_len, output_dim).to(self.device)
        
        model.train()
        output = model(x, ground_truth=y, return_aux_info=True)
        predictions = output['predictions']
        aux_info = output['aux_info']
        
        # è®¡ç®—æŸå¤±
        losses = criterion(predictions, y, aux_info)
        
        # éªŒè¯æŸå¤±ç»„ä»¶
        loss_components = [
            'prediction', 'reconstruction', 'triplet', 'contrastive',
            'consistency', 'load_balance', 'prototype', 'total'
        ]
        
        for component in loss_components:
            assert component in losses, f"ç¼ºå°‘æŸå¤±ç»„ä»¶: {component}"
            loss_value = losses[component]
            assert torch.isfinite(loss_value), f"{component}æŸå¤±åŒ…å«NaN/Inf"
            print(f"   ğŸ“‰ {component}: {loss_value:.4f}")
        
        # éªŒè¯å¯å­¦ä¹ Ïƒå‚æ•°
        sigma_params = ['log_sigma_rc', 'log_sigma_cl', 'log_sigma_pr', 'log_sigma_cons', 'log_sigma_bal']
        for param_name in sigma_params:
            param = getattr(criterion, param_name)
            sigma_value = torch.exp(param)
            print(f"   ğŸ”§ {param_name}: Ïƒ={sigma_value.item():.4f}")
        
        return losses
    
    def validate_gradient_flow(self, model):
        """éªŒè¯æ¢¯åº¦æµ"""
        print("\nğŸŒŠ éªŒè¯æ¢¯åº¦æµ...")
        
        criterion = CompositeLoss(self.config).to(self.device)
        
        batch_size = 16  # ä½¿ç”¨ä¸æŸå¤±éªŒè¯ç›¸åŒçš„batch_size
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        pred_len = self.config['model']['pred_len']
        output_dim = self.config['model']['output_dim']
        
        x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
        y = torch.randn(batch_size, pred_len, output_dim).to(self.device)
        
        model.train()
        model.zero_grad()
        
        # å‰å‘ä¼ æ’­
        output = model(x, ground_truth=y, return_aux_info=True)
        predictions = output['predictions']
        aux_info = output['aux_info']
        
        # æŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­
        losses = criterion(predictions, y, aux_info)
        losses['total'].backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        gradient_stats = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                gradient_stats[name] = grad_norm
                
                # æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
                assert torch.isfinite(param.grad).all(), f"{name}æ¢¯åº¦åŒ…å«NaN/Inf"
                
                if grad_norm > 10:
                    print(f"   âš ï¸  {name}: æ¢¯åº¦è¾ƒå¤§ ({grad_norm:.4f})")
                elif grad_norm < 1e-6:
                    print(f"   âš ï¸  {name}: æ¢¯åº¦å¾ˆå° ({grad_norm:.6f})")
        
        # ç»Ÿè®¡æ¢¯åº¦åˆ†å¸ƒ
        grad_norms = list(gradient_stats.values())
        avg_grad_norm = np.mean(grad_norms)
        max_grad_norm = np.max(grad_norms)
        
        print(f"   ğŸ“Š å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")
        print(f"   ğŸ“Š æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max_grad_norm:.6f}")
        print(f"   ğŸ“Š æœ‰æ•ˆæ¢¯åº¦å‚æ•°æ•°: {len(grad_norms)}")
        
        return gradient_stats
    
    def run_full_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("ğŸš€ å¼€å§‹MÂ²-MOEPæœ€ç»ˆéªŒè¯...")
        print("=" * 80)
        
        try:
            # 1. æ¶æ„éªŒè¯
            model = self.validate_model_architecture()
            
            # 2. æ•°å€¼ç¨³å®šæ€§éªŒè¯
            self.validate_numerical_stability(model)
            
            # 3. æ€§èƒ½æ‰©å±•æ€§éªŒè¯
            performance_data = self.validate_performance_scaling(model)
            
            # 4. ä¸“å®¶å¤šæ ·æ€§éªŒè¯
            diversity_metrics = self.validate_expert_diversity(model)
            
            # 5. æŸå¤±ç»„ä»¶éªŒè¯
            loss_data = self.validate_loss_components(model)
            
            # 6. æ¢¯åº¦æµéªŒè¯
            gradient_stats = self.validate_gradient_flow(model)
            
            print("\n" + "=" * 80)
            print("ğŸ‰ æœ€ç»ˆéªŒè¯å®Œæˆï¼")
            print("âœ… æ¨¡å‹æ¶æ„: å®Œæ•´")
            print("âœ… æ•°å€¼ç¨³å®šæ€§: è‰¯å¥½")
            print("âœ… æ€§èƒ½æ‰©å±•æ€§: æ­£å¸¸")
            print("âœ… ä¸“å®¶å¤šæ ·æ€§: å……åˆ†")
            print("âœ… æŸå¤±ç»„ä»¶: æœ‰æ•ˆ")
            print("âœ… æ¢¯åº¦æµ: å¥åº·")
            
            print(f"\nğŸ“‹ å…³é”®æŒ‡æ ‡:")
            print(f"   â€¢ ä¸“å®¶ç†µ: {diversity_metrics['expert_entropy']:.4f}")
            print(f"   â€¢ å¹³å‡æ¿€æ´»ä¸“å®¶: {diversity_metrics['avg_active_experts']:.2f}")
            print(f"   â€¢ æ€»æŸå¤±: {loss_data['total']:.4f}")
            print(f"   â€¢ æœ€å¤§æ¢¯åº¦èŒƒæ•°: {max(gradient_stats.values()):.6f}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ éªŒè¯å¤±è´¥: {str(e)}")
            return False


if __name__ == "__main__":
    warnings.filterwarnings("ignore")
    
    validator = FinalValidationSuite()
    success = validator.run_full_validation()
    
    sys.exit(0 if success else 1) 