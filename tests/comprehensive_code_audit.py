"""
MÂ²-MOEP å…¨é¢ä»£ç å®¡è®¡å’Œæµ‹è¯•å¥—ä»¶
åŒ…æ‹¬ä»£ç è´¨é‡æ£€æŸ¥ã€è¾¹ç•Œæ¡ä»¶æµ‹è¯•ã€é›†æˆæµ‹è¯•å’Œæ€§èƒ½åˆ†æ
"""

import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import time
import warnings
import traceback
from typing import Dict, List, Tuple, Any
import inspect

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from models.m2_moep import M2_MOEP
from models.flow import PowerfulNormalizingFlow, FlowLayer
from models.gating import GatingEncoder
from models.expert import FFTmsMambaExpert
from utils.losses import CompositeLoss, TripletLoss
from utils.metrics import calculate_metrics, compute_expert_metrics
from data.universal_dataset import UniversalDataModule
from train import M2MOEPTrainer


class ComprehensiveCodeAudit:
    """å…¨é¢ä»£ç å®¡è®¡å¥—ä»¶"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.config = self._get_comprehensive_config()
        self.test_results = []
        self.performance_metrics = {}
        self.code_quality_issues = []
        
        print(f"ğŸ”§ å®¡è®¡ç¯å¢ƒ: {self.device}")
        print(f"ğŸ” å¼€å§‹å…¨é¢ä»£ç å®¡è®¡...")
        
    def _get_comprehensive_config(self):
        """è·å–å…¨é¢æµ‹è¯•é…ç½®"""
        return {
            'model': {
                'input_dim': 10,
                'output_dim': 10,
                'hidden_dim': 256,  # å‡å°æ¨¡å‹å¤§å°
                'num_experts': 4,   # å‡å°‘ä¸“å®¶æ•°é‡
                'seq_len': 24,      # å‡å°åºåˆ—é•¿åº¦
                'pred_len': 12,     # å‡å°é¢„æµ‹é•¿åº¦
                'expert_params': {
                    'mamba_d_model': 256,
                    'mamba_scales': [1, 2, 4]
                },
                'flow': {
                    'latent_dim': 128,
                    'use_pretrained': False,
                    'hidden_dim': 256,
                    'num_coupling_layers': 6
                },
                'diversity': {
                    'prototype_dim': 64,
                    'num_prototypes': 8,
                    'force_diversity': True,
                    'diversity_weight': 0.1
                },
                'temperature': {
                    'initial': 1.0,
                    'min': 0.1,
                    'max': 10.0,
                    'decay': 0.98
                },
                'triplet': {
                    'margin': 0.5,
                    'mining_strategy': 'batch_hard'
                },
                'top_k': 3,
                'embedding_dim': 128
            },
            'training': {
                'learning_rate': 1e-4,  # é™ä½å­¦ä¹ ç‡
                'weight_decay': 1e-5,   # é™ä½æƒé‡è¡°å‡
                'gradient_clip': 1.0,   # é™ä½æ¢¯åº¦è£å‰ª
                'epochs': 10,
                'batch_size': 16,
                'loss_weights': {
                    'init_sigma_rc': 1.0,
                    'init_sigma_cl': 1.0,
                    'init_sigma_pr': 1.0,
                    'init_sigma_consistency': 1.0,
                    'init_sigma_balance': 1.0
                },
                'triplet_margin': 0.5,
                'aux_loss_weight': 0.01
            },
            'data': {
                'seq_len': 24,
                'pred_len': 12,
                'batch_size': 16,
                'num_workers': 2
            }
        }
    
    def log_test(self, test_name: str, status: str, details: str = "", severity: str = "INFO"):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            'test': test_name,
            'status': status,
            'details': details,
            'severity': severity
        }
        self.test_results.append(result)
        
        status_icon = "âœ…" if status == "PASS" else "âŒ" if status == "FAIL" else "âš ï¸"
        print(f"{status_icon} {test_name}: {details}")
        
        if status == "FAIL" and severity == "CRITICAL":
            self.code_quality_issues.append(f"CRITICAL: {test_name} - {details}")
    
    def test_code_structure_and_imports(self):
        """æµ‹è¯•ä»£ç ç»“æ„å’Œå¯¼å…¥"""
        print("\nğŸ“ æµ‹è¯•ä»£ç ç»“æ„å’Œå¯¼å…¥...")
        
        try:
            # æµ‹è¯•æ ¸å¿ƒæ¨¡å—å¯¼å…¥
            from models.m2_moep import M2_MOEP
            from models.flow import PowerfulNormalizingFlow
            from models.gating import GatingEncoder
            from models.expert import FFTmsMambaExpert
            from utils.losses import CompositeLoss
            from utils.metrics import calculate_metrics
            
            self.log_test("æ ¸å¿ƒæ¨¡å—å¯¼å…¥", "PASS", "æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            # æ£€æŸ¥ç±»å®šä¹‰å®Œæ•´æ€§
            required_methods = {
                'M2_MOEP': ['forward', '__init__', 'update_temperature_schedule'],
                'PowerfulNormalizingFlow': ['forward', 'inverse', 'encode', 'decode'],
                'GatingEncoder': ['forward', 'get_embeddings'],
                'FFTmsMambaExpert': ['forward', '_early_fft_fusion'],
                'CompositeLoss': ['forward', 'compute_kl_consistency_loss']
            }
            
            for class_name, methods in required_methods.items():
                cls = globals()[class_name.split('.')[-1]]
                for method in methods:
                    if not hasattr(cls, method):
                        self.log_test(f"{class_name}æ–¹æ³•æ£€æŸ¥", "FAIL", 
                                    f"ç¼ºå°‘æ–¹æ³•: {method}", "CRITICAL")
                    else:
                        # æ£€æŸ¥æ–¹æ³•ç­¾å
                        sig = inspect.signature(getattr(cls, method))
                        if len(sig.parameters) == 0 and method != '__init__':
                            self.log_test(f"{class_name}.{method}ç­¾å", "WARN", 
                                        "æ–¹æ³•å¯èƒ½ç¼ºå°‘å¿…è¦å‚æ•°")
            
            self.log_test("ç±»æ–¹æ³•å®Œæ•´æ€§", "PASS", "æ‰€æœ‰å¿…è¦æ–¹æ³•å­˜åœ¨")
            
        except Exception as e:
            self.log_test("ä»£ç ç»“æ„æ£€æŸ¥", "FAIL", str(e), "CRITICAL")
    
    def test_model_initialization_edge_cases(self):
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–çš„è¾¹ç•Œæ¡ä»¶"""
        print("\nğŸ—ï¸ æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–è¾¹ç•Œæ¡ä»¶...")
        
        edge_configs = [
            # æœ€å°é…ç½®
            {
                'name': 'æœ€å°é…ç½®',
                'config': {
                    'model': {
                        'input_dim': 1, 'output_dim': 1, 'hidden_dim': 8,
                        'num_experts': 2, 'seq_len': 4, 'pred_len': 2,
                        'expert_params': {'mamba_d_model': 8, 'mamba_scales': [1]},
                        'flow': {'latent_dim': 4, 'hidden_dim': 8},
                        'embedding_dim': 8, 'top_k': 1
                    },
                    'training': {'loss_weights': {}}, 'data': {}
                }
            },
            # å¤§è§„æ¨¡é…ç½®
            {
                'name': 'å¤§è§„æ¨¡é…ç½®',
                'config': {
                    'model': {
                        'input_dim': 50, 'output_dim': 50, 'hidden_dim': 1024,
                        'num_experts': 16, 'seq_len': 200, 'pred_len': 100,
                        'expert_params': {'mamba_d_model': 1024, 'mamba_scales': [1, 2, 4, 8]},
                        'flow': {'latent_dim': 512, 'hidden_dim': 1024},
                        'embedding_dim': 512, 'top_k': 8
                    },
                    'training': {'loss_weights': {}}, 'data': {}
                }
            },
            # ä¸å¹³è¡¡é…ç½®
            {
                'name': 'ä¸å¹³è¡¡é…ç½®',
                'config': {
                    'model': {
                        'input_dim': 100, 'output_dim': 1, 'hidden_dim': 64,
                        'num_experts': 3, 'seq_len': 10, 'pred_len': 50,
                        'expert_params': {'mamba_d_model': 32, 'mamba_scales': [1, 4, 16]},
                        'flow': {'latent_dim': 16, 'hidden_dim': 32},
                        'embedding_dim': 16, 'top_k': 2
                    },
                    'training': {'loss_weights': {}}, 'data': {}
                }
            }
        ]
        
        for edge_case in edge_configs:
            try:
                model = M2_MOEP(edge_case['config']).to(self.device)
                
                # æµ‹è¯•å‰å‘ä¼ æ’­
                batch_size = 2
                seq_len = edge_case['config']['model']['seq_len']
                input_dim = edge_case['config']['model']['input_dim']
                
                x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
                output = model(x, return_aux_info=True)
                
                self.log_test(f"è¾¹ç•Œæ¡ä»¶-{edge_case['name']}", "PASS", 
                            f"æ¨¡å‹åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­æˆåŠŸ")
                
            except Exception as e:
                self.log_test(f"è¾¹ç•Œæ¡ä»¶-{edge_case['name']}", "FAIL", str(e))
    
    def test_numerical_stability_extreme_cases(self):
        """æµ‹è¯•æç«¯æ•°å€¼æ¡ä»¶ä¸‹çš„ç¨³å®šæ€§"""
        print("\nğŸ”¢ æµ‹è¯•æç«¯æ•°å€¼ç¨³å®šæ€§...")
        
        model = M2_MOEP(self.config).to(self.device)
        model.eval()
        
        extreme_cases = [
            ("é›¶è¾“å…¥", torch.zeros),
            ("æå¤§å€¼", lambda b, s, d: torch.full((b, s, d), 1e6)),
            ("æå°å€¼", lambda b, s, d: torch.full((b, s, d), 1e-6)),
            ("NaNè¾“å…¥", lambda b, s, d: torch.full((b, s, d), float('nan'))),
            ("Infè¾“å…¥", lambda b, s, d: torch.full((b, s, d), float('inf'))),
            ("æ··åˆæå€¼", lambda b, s, d: torch.cat([
                torch.zeros(b//2, s, d),
                torch.full((b//2, s, d), 1e6)
            ], dim=0)),
            ("æ¢¯åº¦æ¶ˆå¤±æ¨¡æ‹Ÿ", lambda *args: torch.randn(*args) * 1e-10),
            ("æ¢¯åº¦çˆ†ç‚¸æ¨¡æ‹Ÿ", lambda *args: torch.randn(*args) * 1e10)
        ]
        
        batch_size = 4
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        with torch.no_grad():
            for case_name, input_generator in extreme_cases:
                try:
                    x = input_generator(batch_size, seq_len, input_dim).to(self.device)
                    
                    # è·³è¿‡NaNå’ŒInfè¾“å…¥çš„æµ‹è¯•ï¼ˆé¢„æœŸä¼šå¤±è´¥ï¼‰
                    if torch.isnan(x).any() or torch.isinf(x).any():
                        if case_name in ["NaNè¾“å…¥", "Infè¾“å…¥"]:
                            self.log_test(f"æå€¼ç¨³å®šæ€§-{case_name}", "SKIP", "é¢„æœŸå¤±è´¥çš„æµ‹è¯•")
                            continue
                    
                    output = model(x, return_aux_info=True)
                    predictions = output['predictions']
                    
                    # æ£€æŸ¥è¾“å‡ºçš„æ•°å€¼ç¨³å®šæ€§
                    if torch.isfinite(predictions).all():
                        self.log_test(f"æå€¼ç¨³å®šæ€§-{case_name}", "PASS", "è¾“å‡ºæ•°å€¼ç¨³å®š")
                    else:
                        self.log_test(f"æå€¼ç¨³å®šæ€§-{case_name}", "FAIL", 
                                    "è¾“å‡ºåŒ…å«NaN/Inf", "HIGH")
                    
                except Exception as e:
                    severity = "EXPECTED" if case_name in ["NaNè¾“å…¥", "Infè¾“å…¥"] else "HIGH"
                    self.log_test(f"æå€¼ç¨³å®šæ€§-{case_name}", "FAIL", str(e), severity)
    
    def test_memory_and_computational_efficiency(self):
        """æµ‹è¯•å†…å­˜å’Œè®¡ç®—æ•ˆç‡"""
        print("\nğŸ’¾ æµ‹è¯•å†…å­˜å’Œè®¡ç®—æ•ˆç‡...")
        
        model = M2_MOEP(self.config).to(self.device)
        
        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        batch_sizes = [1, 8, 16, 32, 64]
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        memory_usage = []
        computation_times = []
        
        for batch_size in batch_sizes:
            try:
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    initial_memory = torch.cuda.memory_allocated()
                
                x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
                
                # è®¡æ—¶
                start_time = time.time()
                
                model.eval()
                with torch.no_grad():
                    output = model(x, return_aux_info=True)
                
                end_time = time.time()
                computation_time = end_time - start_time
                
                # å†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    peak_memory = torch.cuda.max_memory_allocated()
                    memory_used = (peak_memory - initial_memory) / 1024**2  # MB
                    memory_usage.append((batch_size, memory_used))
                
                computation_times.append((batch_size, computation_time))
                
                # æ£€æŸ¥å†…å­˜æ³„éœ²
                del x, output
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                self.log_test(f"æ•ˆç‡æµ‹è¯•-æ‰¹å¤§å°{batch_size}", "PASS", 
                            f"æ—¶é—´: {computation_time:.4f}s, å†…å­˜: {memory_used:.2f}MB")
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    self.log_test(f"æ•ˆç‡æµ‹è¯•-æ‰¹å¤§å°{batch_size}", "FAIL", 
                                "GPUå†…å­˜ä¸è¶³", "HIGH")
                    break
                else:
                    self.log_test(f"æ•ˆç‡æµ‹è¯•-æ‰¹å¤§å°{batch_size}", "FAIL", str(e))
        
        # åˆ†ææ•ˆç‡è¶‹åŠ¿
        if len(computation_times) > 1:
            # æ£€æŸ¥è®¡ç®—æ—¶é—´æ˜¯å¦éšæ‰¹å¤§å°åˆç†å¢é•¿
            time_ratios = [computation_times[i][1] / computation_times[0][1] 
                          for i in range(1, len(computation_times))]
            batch_ratios = [computation_times[i][0] / computation_times[0][0] 
                           for i in range(1, len(computation_times))]
            
            efficiency_score = np.mean([t/b for t, b in zip(time_ratios, batch_ratios)])
            
            if efficiency_score < 1.5:
                self.log_test("è®¡ç®—æ•ˆç‡åˆ†æ", "PASS", 
                            f"æ•ˆç‡è¯„åˆ†: {efficiency_score:.2f} (è‰¯å¥½)")
            else:
                self.log_test("è®¡ç®—æ•ˆç‡åˆ†æ", "WARN", 
                            f"æ•ˆç‡è¯„åˆ†: {efficiency_score:.2f} (å¯ä¼˜åŒ–)")
        
        self.performance_metrics['memory_usage'] = memory_usage
        self.performance_metrics['computation_times'] = computation_times
    
    def test_gradient_flow_and_optimization(self):
        """æµ‹è¯•æ¢¯åº¦æµå’Œä¼˜åŒ–ç¨³å®šæ€§"""
        print("\nğŸŒŠ æµ‹è¯•æ¢¯åº¦æµå’Œä¼˜åŒ–...")
        
        model = M2_MOEP(self.config).to(self.device)
        criterion = CompositeLoss(self.config).to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        
        batch_size = 8
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        pred_len = self.config['model']['pred_len']
        output_dim = self.config['model']['output_dim']
        
        # æ¨¡æ‹Ÿå¤šæ­¥è®­ç»ƒ
        gradient_norms = []
        loss_values = []
        
        for step in range(10):
            optimizer.zero_grad()
            
            x = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            y = torch.randn(batch_size, pred_len, output_dim).to(self.device)
            
            # å‰å‘ä¼ æ’­
            model.train()
            output = model(x, ground_truth=y, return_aux_info=True)
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # æŸå¤±è®¡ç®—
            losses = criterion(predictions, y, aux_info)
            
            # åå‘ä¼ æ’­
            losses['total'].backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            total_grad_norm = 0
            param_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    total_grad_norm += grad_norm ** 2
                    param_count += 1
                    
                    # æ£€æŸ¥å¼‚å¸¸æ¢¯åº¦
                    if torch.isnan(param.grad).any():
                        self.log_test(f"æ¢¯åº¦æ£€æŸ¥-æ­¥éª¤{step}", "FAIL", 
                                    f"{name}åŒ…å«NaNæ¢¯åº¦", "CRITICAL")
                    elif torch.isinf(param.grad).any():
                        self.log_test(f"æ¢¯åº¦æ£€æŸ¥-æ­¥éª¤{step}", "FAIL", 
                                    f"{name}åŒ…å«Infæ¢¯åº¦", "CRITICAL")
            
            total_grad_norm = (total_grad_norm ** 0.5)
            gradient_norms.append(total_grad_norm)
            loss_values.append(losses['total'].item())
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            
            # æ£€æŸ¥å‚æ•°æ›´æ–°
            param_changed = False
            for param in model.parameters():
                if param.requires_grad and param.grad is not None:
                    if param.grad.abs().sum() > 1e-10:
                        param_changed = True
                        break
            
            if not param_changed:
                self.log_test(f"å‚æ•°æ›´æ–°-æ­¥éª¤{step}", "WARN", "å‚æ•°æœªæ›´æ–°")
        
        # åˆ†ææ¢¯åº¦æµå¥åº·åº¦
        avg_grad_norm = np.mean(gradient_norms)
        grad_stability = np.std(gradient_norms) / (avg_grad_norm + 1e-8)
        
        if avg_grad_norm > 1e-6 and avg_grad_norm < 100:
            self.log_test("æ¢¯åº¦èŒƒæ•°å¥åº·åº¦", "PASS", 
                        f"å¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")
        else:
            self.log_test("æ¢¯åº¦èŒƒæ•°å¥åº·åº¦", "WARN", 
                        f"æ¢¯åº¦èŒƒæ•°å¼‚å¸¸: {avg_grad_norm:.6f}")
        
        if grad_stability < 2.0:
            self.log_test("æ¢¯åº¦ç¨³å®šæ€§", "PASS", f"æ¢¯åº¦ç¨³å®šæ€§: {grad_stability:.3f}")
        else:
            self.log_test("æ¢¯åº¦ç¨³å®šæ€§", "WARN", f"æ¢¯åº¦ä¸ç¨³å®š: {grad_stability:.3f}")
        
        # æ£€æŸ¥æŸå¤±ä¸‹é™
        if len(loss_values) > 5:
            early_loss = np.mean(loss_values[:3])
            late_loss = np.mean(loss_values[-3:])
            if late_loss < early_loss:
                self.log_test("æŸå¤±æ”¶æ•›", "PASS", "æŸå¤±å‘ˆä¸‹é™è¶‹åŠ¿")
            else:
                self.log_test("æŸå¤±æ”¶æ•›", "WARN", "æŸå¤±æœªä¸‹é™")
    
    def test_expert_specialization_and_diversity(self):
        """æµ‹è¯•ä¸“å®¶ç‰¹åŒ–å’Œå¤šæ ·æ€§"""
        print("\nğŸ­ æµ‹è¯•ä¸“å®¶ç‰¹åŒ–å’Œå¤šæ ·æ€§...")
        
        model = M2_MOEP(self.config).to(self.device)
        model.eval()
        
        # ç”Ÿæˆä¸åŒç±»å‹çš„è¾“å…¥æ¨¡å¼
        batch_size = 64
        seq_len = self.config['model']['seq_len']
        input_dim = self.config['model']['input_dim']
        
        # åˆ›å»º5ç§ä¸åŒçš„ä¿¡å·æ¨¡å¼
        patterns = []
        pattern_names = []
        
        for i in range(5):
            pattern_batch_size = batch_size // 5
            
            if i == 0:  # æ­£å¼¦æ³¢
                t = torch.linspace(0, 4*np.pi, seq_len).unsqueeze(0).unsqueeze(-1)
                pattern = torch.sin(t + torch.randn(1, 1, 1) * 0.1)
                pattern = pattern.expand(pattern_batch_size, -1, input_dim)
                pattern_names.append("æ­£å¼¦æ³¢")
                
            elif i == 1:  # é”¯é½¿æ³¢
                t = torch.linspace(0, 4*np.pi, seq_len).unsqueeze(0).unsqueeze(-1)
                pattern = (t % (2*np.pi) - np.pi)
                pattern = pattern.expand(pattern_batch_size, -1, input_dim)
                pattern_names.append("é”¯é½¿æ³¢")
                
            elif i == 2:  # éšæœºæ¸¸èµ°
                pattern = torch.randn(pattern_batch_size, seq_len, input_dim).cumsum(dim=1)
                pattern_names.append("éšæœºæ¸¸èµ°")
                
            elif i == 3:  # é˜¶è·ƒå‡½æ•°
                pattern = torch.zeros(pattern_batch_size, seq_len, input_dim)
                pattern[:, seq_len//2:, :] = 1.0
                pattern_names.append("é˜¶è·ƒå‡½æ•°")
                
            else:  # ç™½å™ªå£°
                pattern = torch.randn(pattern_batch_size, seq_len, input_dim)
                pattern_names.append("ç™½å™ªå£°")
            
            patterns.append(pattern)
        
        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        x = torch.cat(patterns, dim=0).to(self.device)
        pattern_labels = []
        for i, name in enumerate(pattern_names):
            pattern_labels.extend([i] * (batch_size // 5))
        
        with torch.no_grad():
            output = model(x, return_aux_info=True)
            expert_weights = output['aux_info']['expert_weights']
        
        # åˆ†æä¸“å®¶ç‰¹åŒ–
        pattern_expert_usage = {}
        for i, pattern_name in enumerate(pattern_names):
            pattern_indices = [j for j, label in enumerate(pattern_labels) if label == i]
            pattern_weights = expert_weights[pattern_indices].mean(dim=0)
            pattern_expert_usage[pattern_name] = pattern_weights.cpu().numpy()
        
        # è®¡ç®—ä¸“å®¶ç‰¹åŒ–åº¦
        specialization_scores = []
        for expert_idx in range(self.config['model']['num_experts']):
            expert_usage_across_patterns = [
                pattern_expert_usage[pattern][expert_idx] 
                for pattern in pattern_names
            ]
            # ä½¿ç”¨æ ‡å‡†å·®è¡¡é‡ç‰¹åŒ–åº¦ï¼ˆæ ‡å‡†å·®è¶Šå¤§ï¼Œç‰¹åŒ–åº¦è¶Šé«˜ï¼‰
            specialization = np.std(expert_usage_across_patterns)
            specialization_scores.append(specialization)
        
        avg_specialization = np.mean(specialization_scores)
        
        if avg_specialization > 0.05:
            self.log_test("ä¸“å®¶ç‰¹åŒ–åº¦", "PASS", 
                        f"å¹³å‡ç‰¹åŒ–åº¦: {avg_specialization:.4f}")
        else:
            self.log_test("ä¸“å®¶ç‰¹åŒ–åº¦", "WARN", 
                        f"ä¸“å®¶ç‰¹åŒ–åº¦è¾ƒä½: {avg_specialization:.4f}")
        
        # æ£€æŸ¥ä¸“å®¶å¤šæ ·æ€§
        expert_similarity_matrix = np.corrcoef([
            list(pattern_expert_usage[pattern]) for pattern in pattern_names
        ])
        
        avg_similarity = np.mean(expert_similarity_matrix[np.triu_indices_from(expert_similarity_matrix, k=1)])
        
        if avg_similarity < 0.8:
            self.log_test("ä¸“å®¶å¤šæ ·æ€§", "PASS", 
                        f"å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
        else:
            self.log_test("ä¸“å®¶å¤šæ ·æ€§", "WARN", 
                        f"ä¸“å®¶è¿‡äºç›¸ä¼¼: {avg_similarity:.3f}")
        
        # æ‰“å°è¯¦ç»†çš„ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
        print("   ğŸ“Š ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ:")
        for pattern_name, usage in pattern_expert_usage.items():
            top_expert = np.argmax(usage)
            print(f"      {pattern_name}: ä¸»è¦ä¸“å®¶{top_expert} ({usage[top_expert]:.3f})")
    
    def test_loss_function_components(self):
        """æµ‹è¯•æŸå¤±å‡½æ•°å„ç»„ä»¶"""
        print("\nğŸ’” æµ‹è¯•æŸå¤±å‡½æ•°ç»„ä»¶...")
        
        criterion = CompositeLoss(self.config).to(self.device)
        
        batch_size = 16
        pred_len = self.config['model']['pred_len']
        output_dim = self.config['model']['output_dim']
        num_experts = self.config['model']['num_experts']
        embedding_dim = self.config['model']['embedding_dim']
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        predictions = torch.randn(batch_size, pred_len, output_dim).to(self.device)
        targets = torch.randn(batch_size, pred_len, output_dim).to(self.device)
        
        # åˆ›å»ºå®Œæ•´çš„è¾…åŠ©ä¿¡æ¯
        aux_info = {
            'expert_weights': torch.softmax(torch.randn(batch_size, num_experts).to(self.device), dim=-1),
            'expert_features': torch.randn(batch_size, 128).to(self.device),
            'gating_embeddings': torch.randn(batch_size, embedding_dim).to(self.device),
            'reconstruction_loss': torch.tensor(0.1).to(self.device),
            'triplet_loss': torch.tensor(0.05).to(self.device),
            'load_balance_loss': torch.tensor(0.02).to(self.device),
            'prototype_loss': torch.tensor(0.03).to(self.device)
        }
        
        # æµ‹è¯•æŸå¤±è®¡ç®—
        try:
            losses = criterion(predictions, targets, aux_info)
            
            # éªŒè¯æ‰€æœ‰æŸå¤±ç»„ä»¶
            required_components = [
                'prediction', 'reconstruction', 'triplet', 'contrastive',
                'consistency', 'load_balance', 'prototype', 'total'
            ]
            
            for component in required_components:
                if component not in losses:
                    self.log_test(f"æŸå¤±ç»„ä»¶-{component}", "FAIL", 
                                "ç»„ä»¶ç¼ºå¤±", "HIGH")
                elif not torch.isfinite(losses[component]):
                    self.log_test(f"æŸå¤±ç»„ä»¶-{component}", "FAIL", 
                                "åŒ…å«NaN/Inf", "HIGH")
                else:
                    self.log_test(f"æŸå¤±ç»„ä»¶-{component}", "PASS", 
                                f"å€¼: {losses[component]:.4f}")
            
            # æµ‹è¯•å¯å­¦ä¹ Ïƒå‚æ•°
            sigma_params = ['log_sigma_rc', 'log_sigma_cl', 'log_sigma_pr', 
                          'log_sigma_cons', 'log_sigma_bal']
            
            for param_name in sigma_params:
                if hasattr(criterion, param_name):
                    param = getattr(criterion, param_name)
                    if param.requires_grad:
                        sigma_value = torch.exp(param)
                        self.log_test(f"Ïƒå‚æ•°-{param_name}", "PASS", 
                                    f"Ïƒ={sigma_value.item():.4f}")
                    else:
                        self.log_test(f"Ïƒå‚æ•°-{param_name}", "FAIL", 
                                    "å‚æ•°ä¸å¯è®­ç»ƒ", "HIGH")
                else:
                    self.log_test(f"Ïƒå‚æ•°-{param_name}", "FAIL", 
                                "å‚æ•°ç¼ºå¤±", "HIGH")
            
            # æµ‹è¯•æŸå¤±å¹³è¡¡
            total_expected = (
                torch.exp(-2 * criterion.log_sigma_pr) * losses['prediction'] + criterion.log_sigma_pr +
                torch.exp(-2 * criterion.log_sigma_rc) * losses['reconstruction'] + criterion.log_sigma_rc +
                torch.exp(-2 * criterion.log_sigma_cl) * (losses['triplet'] + 0.5 * losses['contrastive']) + criterion.log_sigma_cl +
                torch.exp(-2 * criterion.log_sigma_cons) * losses['consistency'] + criterion.log_sigma_cons +
                torch.exp(-2 * criterion.log_sigma_bal) * losses['load_balance'] + criterion.log_sigma_bal +
                losses['prototype'] * 0.1
            )
            
            if torch.allclose(losses['total'], total_expected, rtol=1e-3):
                self.log_test("æŸå¤±å¹³è¡¡éªŒè¯", "PASS", "æ€»æŸå¤±è®¡ç®—æ­£ç¡®")
            else:
                self.log_test("æŸå¤±å¹³è¡¡éªŒè¯", "FAIL", 
                            f"æ€»æŸå¤±è®¡ç®—é”™è¯¯: {losses['total']:.4f} vs {total_expected:.4f}", "HIGH")
            
        except Exception as e:
            self.log_test("æŸå¤±å‡½æ•°æµ‹è¯•", "FAIL", str(e), "CRITICAL")
    
    def test_data_pipeline_integration(self):
        """æµ‹è¯•æ•°æ®ç®¡é“é›†æˆ"""
        print("\nğŸ“Š æµ‹è¯•æ•°æ®ç®¡é“é›†æˆ...")
        
        # åˆ›å»ºä¸´æ—¶æ•°æ®é…ç½®
        temp_config = self.config.copy()
        temp_config['data'].update({
            'dataset_type': 'synthetic',
            'data_path': 'synthetic',  # ä¿®å¤Noneé—®é¢˜
            'train_ratio': 0.7,
            'val_ratio': 0.2,
            'test_ratio': 0.1,
            'synthetic_samples': 2000,  # å¢åŠ æ ·æœ¬æ•°é‡
            'noise_level': 0.1
        })
        
        try:
            # æµ‹è¯•æ•°æ®æ¨¡å—åˆå§‹åŒ–
            data_module = UniversalDataModule(temp_config)
            
            # æ£€æŸ¥æ•°æ®åŠ è½½å™¨
            train_loader = data_module.get_train_loader()
            val_loader = data_module.get_val_loader()
            test_loader = data_module.get_test_loader()
            
            self.log_test("æ•°æ®åŠ è½½å™¨åˆ›å»º", "PASS", 
                        f"è®­ç»ƒ: {len(train_loader)}, éªŒè¯: {len(val_loader)}, æµ‹è¯•: {len(test_loader)}")
            
            # æµ‹è¯•æ•°æ®æ‰¹æ¬¡
            for batch_x, batch_y in train_loader:
                expected_shape_x = (temp_config['data']['batch_size'], 
                                  temp_config['model']['seq_len'], 
                                  temp_config['model']['input_dim'])
                expected_shape_y = (temp_config['data']['batch_size'], 
                                  temp_config['model']['pred_len'], 
                                  temp_config['model']['output_dim'])
                
                if batch_x.shape == expected_shape_x and batch_y.shape == expected_shape_y:
                    self.log_test("æ•°æ®å½¢çŠ¶éªŒè¯", "PASS", 
                                f"X: {batch_x.shape}, Y: {batch_y.shape}")
                else:
                    self.log_test("æ•°æ®å½¢çŠ¶éªŒè¯", "FAIL", 
                                f"å½¢çŠ¶ä¸åŒ¹é…: X: {batch_x.shape}, Y: {batch_y.shape}", "HIGH")
                break
            
            # æµ‹è¯•æ•°æ®æ•°å€¼èŒƒå›´
            data_stats = {
                'x_mean': batch_x.mean().item(),
                'x_std': batch_x.std().item(),
                'y_mean': batch_y.mean().item(),
                'y_std': batch_y.std().item()
            }
            
            if all(abs(stat) < 10 for stat in data_stats.values()):
                self.log_test("æ•°æ®æ•°å€¼èŒƒå›´", "PASS", 
                            f"ç»Ÿè®¡: {data_stats}")
            else:
                self.log_test("æ•°æ®æ•°å€¼èŒƒå›´", "WARN", 
                            f"æ•°å€¼èŒƒå›´è¾ƒå¤§: {data_stats}")
            
        except Exception as e:
            self.log_test("æ•°æ®ç®¡é“é›†æˆ", "FAIL", str(e), "HIGH")
    
    def test_end_to_end_training_simulation(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡æ‹Ÿ"""
        print("\nğŸš€ æµ‹è¯•ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡æ‹Ÿ...")
        
        try:
            # åˆ›å»ºç®€åŒ–çš„è®­ç»ƒé…ç½®
            train_config = self.config.copy()
            train_config['training']['epochs'] = 3
            train_config['data']['batch_size'] = 8
            train_config['data'].update({
                'dataset_type': 'synthetic',
                'data_path': 'synthetic',  # ä¿®å¤Noneé—®é¢˜
                'synthetic_samples': 1000,  # å¢åŠ æ ·æœ¬æ•°é‡
                'noise_level': 0.1
            })
            
            # åˆå§‹åŒ–è®­ç»ƒå™¨
            trainer = M2MOEPTrainer(train_config)
            
            # æ¨¡æ‹Ÿè®­ç»ƒå‡ ä¸ªepoch
            initial_loss = None
            final_loss = None
            
            for epoch in range(3):
                trainer.current_epoch = epoch
                
                # è®­ç»ƒä¸€ä¸ªepoch
                train_losses = trainer.train_epoch()
                val_losses, val_metrics = trainer.validate_epoch()
                
                if epoch == 0:
                    initial_loss = train_losses['total']
                if epoch == 2:
                    final_loss = train_losses['total']
                
                self.log_test(f"è®­ç»ƒEpoch{epoch}", "PASS", 
                            f"è®­ç»ƒæŸå¤±: {train_losses['total']:.4f}, éªŒè¯æŸå¤±: {val_losses['total']:.4f}")
            
            # æ£€æŸ¥è®­ç»ƒè¿›å±•
            if final_loss < initial_loss:
                self.log_test("è®­ç»ƒæ”¶æ•›æ€§", "PASS", 
                            f"æŸå¤±ä» {initial_loss:.4f} é™è‡³ {final_loss:.4f}")
            else:
                self.log_test("è®­ç»ƒæ”¶æ•›æ€§", "WARN", 
                            f"æŸå¤±æœªä¸‹é™: {initial_loss:.4f} -> {final_loss:.4f}")
            
            # æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
            save_path = "test_checkpoint.pth"
            trainer.save_checkpoint(is_best=True)
            
            if os.path.exists(os.path.join(trainer.save_dir, "best_model.pth")):
                self.log_test("æ¨¡å‹ä¿å­˜", "PASS", "æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ")
                
                # æµ‹è¯•åŠ è½½
                new_trainer = M2MOEPTrainer(train_config)
                checkpoint_path = os.path.join(trainer.save_dir, "best_model.pth")
                new_trainer.load_checkpoint(checkpoint_path)
                
                self.log_test("æ¨¡å‹åŠ è½½", "PASS", "æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
                
                # æ¸…ç†
                os.remove(checkpoint_path)
            else:
                self.log_test("æ¨¡å‹ä¿å­˜", "FAIL", "æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥", "HIGH")
            
        except Exception as e:
            self.log_test("ç«¯åˆ°ç«¯è®­ç»ƒ", "FAIL", str(e), "CRITICAL")
    
    def run_comprehensive_audit(self):
        """è¿è¡Œå…¨é¢å®¡è®¡"""
        print("ğŸš€ å¼€å§‹MÂ²-MOEPå…¨é¢ä»£ç å®¡è®¡...")
        print("=" * 100)
        
        start_time = time.time()
        
        try:
            # 1. ä»£ç ç»“æ„æ£€æŸ¥
            self.test_code_structure_and_imports()
            
            # 2. æ¨¡å‹åˆå§‹åŒ–è¾¹ç•Œæ¡ä»¶
            self.test_model_initialization_edge_cases()
            
            # 3. æ•°å€¼ç¨³å®šæ€§
            self.test_numerical_stability_extreme_cases()
            
            # 4. å†…å­˜å’Œè®¡ç®—æ•ˆç‡
            self.test_memory_and_computational_efficiency()
            
            # 5. æ¢¯åº¦æµå’Œä¼˜åŒ–
            self.test_gradient_flow_and_optimization()
            
            # 6. ä¸“å®¶ç‰¹åŒ–å’Œå¤šæ ·æ€§
            self.test_expert_specialization_and_diversity()
            
            # 7. æŸå¤±å‡½æ•°ç»„ä»¶
            self.test_loss_function_components()
            
            # 8. æ•°æ®ç®¡é“é›†æˆ
            self.test_data_pipeline_integration()
            
            # 9. ç«¯åˆ°ç«¯è®­ç»ƒæ¨¡æ‹Ÿ
            self.test_end_to_end_training_simulation()
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
            self.generate_audit_report(total_time)
            
            return len(self.code_quality_issues) == 0
            
        except Exception as e:
            print(f"\nâŒ å®¡è®¡è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
            traceback.print_exc()
            return False
    
    def generate_audit_report(self, total_time: float):
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“‹ å…¨é¢ä»£ç å®¡è®¡æŠ¥å‘Š")
        print("=" * 100)
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['status'] == 'PASS')
        failed_tests = sum(1 for result in self.test_results if result['status'] == 'FAIL')
        warned_tests = sum(1 for result in self.test_results if result['status'] == 'WARN')
        skipped_tests = sum(1 for result in self.test_results if result['status'] == 'SKIP')
        
        print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   é€šè¿‡: {passed_tests} âœ…")
        print(f"   å¤±è´¥: {failed_tests} âŒ")
        print(f"   è­¦å‘Š: {warned_tests} âš ï¸")
        print(f"   è·³è¿‡: {skipped_tests} â­ï¸")
        print(f"   æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # ä»£ç è´¨é‡è¯„ä¼°
        critical_issues = sum(1 for issue in self.code_quality_issues if 'CRITICAL' in issue)
        high_issues = len(self.code_quality_issues) - critical_issues
        
        print(f"\nğŸ” ä»£ç è´¨é‡è¯„ä¼°:")
        print(f"   ä¸¥é‡é—®é¢˜: {critical_issues}")
        print(f"   é«˜ä¼˜å…ˆçº§é—®é¢˜: {high_issues}")
        
        if critical_issues == 0 and high_issues == 0:
            print("   ğŸ‰ ä»£ç è´¨é‡: ä¼˜ç§€")
        elif critical_issues == 0 and high_issues <= 3:
            print("   âœ… ä»£ç è´¨é‡: è‰¯å¥½")
        elif critical_issues <= 2:
            print("   âš ï¸  ä»£ç è´¨é‡: éœ€è¦æ”¹è¿›")
        else:
            print("   âŒ ä»£ç è´¨é‡: å­˜åœ¨ä¸¥é‡é—®é¢˜")
        
        # æ€§èƒ½æŒ‡æ ‡
        if self.performance_metrics:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            if 'computation_times' in self.performance_metrics:
                times = self.performance_metrics['computation_times']
                if times:
                    avg_time_per_sample = np.mean([t[1]/t[0] for t in times])
                    print(f"   å¹³å‡æ¯æ ·æœ¬æ¨ç†æ—¶é—´: {avg_time_per_sample*1000:.2f}ms")
            
            if 'memory_usage' in self.performance_metrics:
                memory = self.performance_metrics['memory_usage']
                if memory:
                    avg_memory_per_sample = np.mean([m[1]/m[0] for m in memory])
                    print(f"   å¹³å‡æ¯æ ·æœ¬å†…å­˜ä½¿ç”¨: {avg_memory_per_sample:.2f}MB")
        
        # é—®é¢˜åˆ—è¡¨
        if self.code_quality_issues:
            print(f"\nâš ï¸  éœ€è¦å…³æ³¨çš„é—®é¢˜:")
            for i, issue in enumerate(self.code_quality_issues[:10], 1):
                print(f"   {i}. {issue}")
            if len(self.code_quality_issues) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.code_quality_issues) - 10} ä¸ªé—®é¢˜")
        
        # æ€»ç»“
        print(f"\nğŸ¯ å®¡è®¡ç»“è®º:")
        if failed_tests == 0 and critical_issues == 0:
            print("   âœ… ä»£ç å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒ")
        elif failed_tests <= 2 and critical_issues == 0:
            print("   âš ï¸  ä»£ç åŸºæœ¬å¯ç”¨ï¼Œå»ºè®®ä¿®å¤è­¦å‘Šé—®é¢˜")
        else:
            print("   âŒ ä»£ç éœ€è¦ä¿®å¤å…³é”®é—®é¢˜åæ‰èƒ½ä½¿ç”¨")


if __name__ == "__main__":
    warnings.filterwarnings("ignore")
    
    auditor = ComprehensiveCodeAudit()
    success = auditor.run_comprehensive_audit()
    
    sys.exit(0 if success else 1) 