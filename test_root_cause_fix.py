#!/usr/bin/env python3
"""
æ ¹å› ä¿®å¤éªŒè¯è„šæœ¬
æµ‹è¯•expert_embeddingsçš„æ­£ç¡®ç”Ÿæˆå’Œæ¢¯åº¦ç¨³å®šæ€§
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.m2_moep import M2_MOEP
from data.universal_dataset import UniversalDataModule
from utils.losses import CompositeLoss
from utils.metrics import calculate_metrics
from configs.config_generator import ConfigGenerator

def test_expert_embeddings_fix():
    """æµ‹è¯•expert_embeddingsç¼ºå¤±é—®é¢˜çš„ä¿®å¤"""
    print("ğŸ”¬ æµ‹è¯•expert_embeddingsç¼ºå¤±é—®é¢˜ä¿®å¤...")
    
    # åŠ è½½æ ¹å› ä¿®å¤é…ç½®
    config_path = "configs/weather_ultra_stable.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡æ£€æŸ¥
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–æ•°æ®æ¨¡å—
    data_module = UniversalDataModule(config)
    
    # æ›´æ–°é…ç½®ä¸­çš„å®é™…ç‰¹å¾æ•°
    actual_features = data_module.get_dataset_info()['num_features']
    config['model']['input_dim'] = actual_features
    config['model']['output_dim'] = actual_features
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = M2_MOEP(config).to(device)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è·å–æµ‹è¯•æ•°æ®
    train_loader = data_module.get_train_loader()
    batch_x, batch_y = next(iter(train_loader))
    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
    
    print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {batch_x.shape} -> {batch_y.shape}")
    
    # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        output = model(batch_x, ground_truth=batch_y, return_aux_info=True)
    
    # æ£€æŸ¥è¾“å‡ºç»“æ„
    print("ğŸ” æ£€æŸ¥æ¨¡å‹è¾“å‡ºç»“æ„:")
    print(f"   - predictions: {output['predictions'].shape}")
    print(f"   - aux_info keys: {list(output['aux_info'].keys())}")
    
    # ğŸ”§ å…³é”®æ£€æŸ¥ï¼šexpert_embeddingsæ˜¯å¦å­˜åœ¨
    aux_info = output['aux_info']
    if 'expert_embeddings' in aux_info:
        expert_embeddings = aux_info['expert_embeddings']
        print(f"âœ… expert_embeddingså­˜åœ¨: {expert_embeddings.shape}")
        
        # æ£€æŸ¥expert_embeddingsçš„æ•°å€¼å±æ€§
        if torch.isnan(expert_embeddings).any():
            print("âŒ expert_embeddingsåŒ…å«NaN")
            return False
        if torch.isinf(expert_embeddings).any():
            print("âŒ expert_embeddingsåŒ…å«Inf")
            return False
        
        # æ£€æŸ¥æ•°å€¼èŒƒå›´
        emb_min, emb_max = expert_embeddings.min().item(), expert_embeddings.max().item()
        print(f"   - æ•°å€¼èŒƒå›´: [{emb_min:.4f}, {emb_max:.4f}]")
        
        if abs(emb_min) > 100 or abs(emb_max) > 100:
            print("âŒ expert_embeddingsæ•°å€¼èŒƒå›´è¿‡å¤§")
            return False
        
        print("âœ… expert_embeddingsæ•°å€¼æ­£å¸¸")
        return True
    else:
        print("âŒ expert_embeddingsä»ç„¶ç¼ºå¤±")
        return False

def test_flow_stability():
    """æµ‹è¯•Flowæ¨¡å‹æ•°å€¼ç¨³å®šæ€§ä¿®å¤"""
    print("\nğŸ”¬ æµ‹è¯•Flowæ¨¡å‹æ•°å€¼ç¨³å®šæ€§ä¿®å¤...")
    
    # æµ‹è¯•Flowæ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬
    from models.flow import SimpleStableFlow
    
    # åˆ›å»ºæµ‹è¯•Flowæ¨¡å‹
    input_dim = 16 * 96  # hidden_dim * seq_len
    flow_model = SimpleStableFlow(input_dim, flow_layers=1)
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    test_input = torch.randn(batch_size, input_dim)
    
    print(f"æµ‹è¯•Flowæ¨¡å‹: è¾“å…¥å½¢çŠ¶ {test_input.shape}")
    
    try:
        # æµ‹è¯•å‰å‘ä¼ æ’­
        z, log_det = flow_model(test_input)
        print(f"âœ… Flowå‰å‘ä¼ æ’­æˆåŠŸ: {z.shape}")
        
        # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
        if torch.isnan(z).any() or torch.isinf(z).any():
            print("âŒ Flowè¾“å‡ºåŒ…å«NaN/Inf")
            return False
        
        if torch.isnan(log_det).any() or torch.isinf(log_det).any():
            print("âŒ Flow log_detåŒ…å«NaN/Inf")
            return False
        
        # æµ‹è¯•é‡æ„
        reconstructed = flow_model.reconstruct(test_input)
        print(f"âœ… Flowé‡æ„æˆåŠŸ: {reconstructed.shape}")
        
        # æ£€æŸ¥é‡æ„è´¨é‡
        recon_error = torch.mean((test_input - reconstructed) ** 2).item()
        print(f"   - é‡æ„è¯¯å·®: {recon_error:.6f}")
        
        if recon_error > 1.0:
            print("âŒ Flowé‡æ„è¯¯å·®è¿‡å¤§")
            return False
        
        print("âœ… Flowæ¨¡å‹æ•°å€¼ç¨³å®š")
        return True
        
    except Exception as e:
        print(f"âŒ Flowæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gradient_stability_after_fix():
    """æµ‹è¯•ä¿®å¤åçš„æ¢¯åº¦ç¨³å®šæ€§"""
    print("\nğŸ”¬ æµ‹è¯•ä¿®å¤åçš„æ¢¯åº¦ç¨³å®šæ€§...")
    
    # åŠ è½½æ ¹å› ä¿®å¤é…ç½®
    config_path = "configs/weather_ultra_stable.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡æ£€æŸ¥
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆå§‹åŒ–æ•°æ®æ¨¡å—
    data_module = UniversalDataModule(config)
    
    # æ›´æ–°é…ç½®ä¸­çš„å®é™…ç‰¹å¾æ•°
    actual_features = data_module.get_dataset_info()['num_features']
    config['model']['input_dim'] = actual_features
    config['model']['output_dim'] = actual_features
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = M2_MOEP(config).to(device)
    
    # åˆå§‹åŒ–æŸå¤±å‡½æ•°
    criterion = CompositeLoss(config)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate'],
        weight_decay=config['training'].get('weight_decay', 1e-4)
    )
    
    # è·å–è®­ç»ƒæ•°æ®
    train_loader = data_module.get_train_loader()
    
    # æ¢¯åº¦ç›‘æ§
    gradient_stats = []
    loss_stats = []
    
    print("ğŸ“Š å¼€å§‹3ä¸ªbatchçš„æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•...")
    
    model.train()
    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
        if batch_idx >= 3:  # åªæµ‹è¯•3ä¸ªbatch
            break
            
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        
        try:
            # ğŸ”§ å…³é”®æµ‹è¯•ï¼šä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹
            output = model(batch_x, ground_truth=batch_y, return_aux_info=True)
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # ğŸ”§ å…³é”®æ£€æŸ¥ï¼šexpert_embeddingsæ˜¯å¦æ­£ç¡®ä¼ é€’
            if 'expert_embeddings' not in aux_info:
                print(f"âŒ Batch {batch_idx}: expert_embeddingsä»ç„¶ç¼ºå¤±")
                return False
            
            expert_embeddings = aux_info['expert_embeddings']
            print(f"âœ… Batch {batch_idx}: expert_embeddingså­˜åœ¨ {expert_embeddings.shape}")
            
            # è®¡ç®—æŸå¤±
            losses = criterion(
                predictions=predictions,
                targets=batch_y,
                expert_weights=aux_info.get('expert_weights'),
                expert_embeddings=expert_embeddings,  # ğŸ”§ ç°åœ¨åº”è¯¥æ­£ç¡®ä¼ é€’
                flow_embeddings=None,
                flow_log_det=aux_info.get('flow_log_det')
            )
            total_loss = losses['total']
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            total_grad_norm = 0.0
            for param in model.parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    total_grad_norm += param_norm.item() ** 2
            total_grad_norm = total_grad_norm ** 0.5
            
            # è®°å½•ç»Ÿè®¡
            gradient_stats.append(total_grad_norm)
            loss_stats.append(total_loss.item())
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"âŒ Batch {batch_idx}: æŸå¤±åŒ…å«NaN/Inf")
                return False
            
            if torch.isnan(torch.tensor(total_grad_norm)) or torch.isinf(torch.tensor(total_grad_norm)):
                print(f"âŒ Batch {batch_idx}: æ¢¯åº¦åŒ…å«NaN/Inf")
                return False
            
            # ğŸ”§ å…³é”®æ£€æŸ¥ï¼šæ¢¯åº¦èŒƒæ•°åº”è¯¥æ˜¾è‘—å‡å°
            if total_grad_norm > 1.0:  # æ¯”ä¹‹å‰çš„10-80èŒƒå›´å¤§å¹…å‡å°
                print(f"âŒ Batch {batch_idx}: æ¢¯åº¦ä»ç„¶è¿‡å¤§ (norm={total_grad_norm:.4f})")
                return False
            
            optimizer.step()
            
            print(f"âœ… Batch {batch_idx}: Loss={total_loss.item():.6f}, GradNorm={total_grad_norm:.6f}")
            
        except Exception as e:
            print(f"âŒ Batch {batch_idx}: è®­ç»ƒå¤±è´¥ - {e}")
            return False
    
    # åˆ†æç»Ÿè®¡ç»“æœ
    print("\nğŸ“ˆ æ¢¯åº¦ç¨³å®šæ€§åˆ†æ:")
    print(f"   - å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(gradient_stats):.6f}")
    print(f"   - æœ€å¤§æ¢¯åº¦èŒƒæ•°: {np.max(gradient_stats):.6f}")
    print(f"   - æ¢¯åº¦èŒƒæ•°æ ‡å‡†å·®: {np.std(gradient_stats):.6f}")
    print(f"   - å¹³å‡æŸå¤±: {np.mean(loss_stats):.6f}")
    print(f"   - æŸå¤±æ ‡å‡†å·®: {np.std(loss_stats):.6f}")
    
    # ğŸ”§ å…³é”®åˆ¤æ–­ï¼šæ¢¯åº¦èŒƒæ•°åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
    if np.max(gradient_stats) < 1.0 and np.std(gradient_stats) < 0.5:
        print("âœ… æ¢¯åº¦ç¨³å®šæ€§æ˜¾è‘—æ”¹å–„ï¼")
        return True
    else:
        print("âŒ æ¢¯åº¦ä»ç„¶ä¸å¤Ÿç¨³å®š")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ MÂ²-MOEPæ ¹å› ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("expert_embeddingsç¼ºå¤±ä¿®å¤", test_expert_embeddings_fix),
        ("Flowæ¨¡å‹æ•°å€¼ç¨³å®šæ€§", test_flow_stability),
        ("ä¿®å¤åæ¢¯åº¦ç¨³å®šæ€§", test_gradient_stability_after_fix),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ€»ç»“ç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ æ ¹å› ä¿®å¤æµ‹è¯•ç»“æœæ€»ç»“:")
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   - {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æ ¹å› é—®é¢˜å·²ä¿®å¤ï¼æ¢¯åº¦çˆ†ç‚¸é—®é¢˜å½»åº•è§£å†³ï¼")
        print("ğŸš€ å¯ä»¥å¼€å§‹æ­£å¸¸è®­ç»ƒäº†ï¼")
    else:
        print("âš ï¸  ä»æœ‰æ ¹å› é—®é¢˜éœ€è¦è§£å†³")
    
    return passed == len(results)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 