"""
é…ç½®ç”Ÿæˆå™¨ - ä¿®å¤è·¯å¾„æ¨¡å¼ç‰ˆæœ¬
"""

import os
import yaml
from typing import Dict, Any, Optional, List
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler


class ConfigGenerator:
    """é…ç½®ç”Ÿæˆå™¨ç±»"""
    
    # åŸºç¡€é…ç½®æ¨¡æ¿
    BASE_CONFIG = {
        'model': {
            'input_dim': 0,  # å°†æ ¹æ®æ•°æ®é›†åŠ¨æ€è®¾ç½®
            'output_dim': 0,  # å°†æ ¹æ®æ•°æ®é›†åŠ¨æ€è®¾ç½®
            'hidden_dim': 256,
            'num_experts': 4,
            'expert_hidden_dim': 128,
            'embedding_dim': 128,  # é—¨æ§ç½‘ç»œåµŒå…¥ç»´åº¦
            'dropout': 0.1,
            'activation': 'gelu',
            'expert_params': {
                'mamba_d_model': 256,
                'mamba_scales': [1, 2, 4]  # å¤šå°ºåº¦å¤„ç†
            },
            # Flowæ¨¡å‹é…ç½®
            'flow': {
                'latent_dim': 256,
                'use_pretrained': True,
                'hidden_dim': 256,
                'num_coupling_layers': 6
            },
            # Triplet Lossé…ç½®
            'triplet': {
                'margin': 0.5,
                'mining_strategy': 'batch_hard',
                'loss_weight': 1.0,
                'performance_window': 100
            },
            # å¤šæ ·æ€§é…ç½®
            'diversity': {
                'prototype_dim': 64,
                'num_prototypes': 8,  # num_experts * 2
                'diversity_weight': 0.1,
                'force_diversity': True
            },
            # æ¸©åº¦è°ƒåº¦é…ç½®
            'temperature': {
                'initial': 1.0,
                'min': 0.1,
                'max': 10.0,
                'decay': 0.95,
                'schedule': 'exponential'
            }
        },
        'data': {
            'seq_len': 96,
            'pred_len': 96,
            'batch_size': 32,
            'num_workers': 2,
            'pin_memory': True,
            'train_ratio': 0.7,
            'val_ratio': 0.2,
            'test_ratio': 0.1,
            'scaler_type': 'standard',
            'normalize': True
        },
        'training': {
            'epochs': 50,
            'batch_size': 16,
            'learning_rate': 0.0001,
            'optimizer': 'Adam',
            'weight_decay': 1e-4,
            'gradient_clip': 1.0,
            'min_lr': 1e-6,
            'patience': 10,
            'flow_model_path': 'flow_model_default.pth',
            'triplet_margin': 0.5,
            'use_reconstruction_loss': True,
            'use_consistency_loss': True,
            'use_load_balancing': True,
            'consistency_temperature': 0.1,
            'aux_loss_weight': 0.01,
            'loss_weights': {
                'prediction': 1.0,      # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸€è‡´çš„å‘½å
                'reconstruction': 0.1,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸€è‡´çš„å‘½å
                'triplet': 0.1,         # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸€è‡´çš„å‘½å
                'prototype_reg': 0.01,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸€è‡´çš„å‘½å
                'load_balance': 0.01    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸€è‡´çš„å‘½å
            }
        },
        'evaluation': {
            'metrics': ['mse', 'mae', 'mape'],
            'save_predictions': True,
            'save_expert_analysis': True
        }
    }
    
    # æ•°æ®é›†ç‰¹å®šé…ç½® - ä¿®æ­£å®é™…è·¯å¾„
    DATASET_CONFIGS = {
        'electricity': {
            'description': '321ä¸ªå®¢æˆ·çš„æ¯å°æ—¶ç”¨ç”µé‡æ•°æ®',
            'expected_features': 321,
            'data_path': 'dataset/electricity/electricity.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'traffic': {
            'description': '862ä¸ªä¼ æ„Ÿå™¨æµ‹é‡çš„æ¯å°æ—¶é“è·¯å ç”¨ç‡',
            'expected_features': 862,
            'data_path': 'dataset/traffic/traffic.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'weather': {
            'description': 'æ°”è±¡ç«™21ä¸ªæ°”è±¡å› å­æ•°æ®',
            'expected_features': 21,
            'data_path': 'dataset/weather/weather.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'ETTh1': {
            'description': '7ä¸ªå› ç´ çš„å˜å‹å™¨æ¸©åº¦å˜åŒ–',
            'expected_features': 7,
            'data_path': 'dataset/ETT/ETTh1.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'ETTh2': {
            'description': '7ä¸ªå› ç´ çš„å˜å‹å™¨æ¸©åº¦å˜åŒ–',
            'expected_features': 7,
            'data_path': 'dataset/ETT/ETTh2.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'ETTm1': {
            'description': '7ä¸ªå› ç´ çš„å˜å‹å™¨æ¸©åº¦å˜åŒ–',
            'expected_features': 7,
            'data_path': 'dataset/ETT/ETTm1.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'ETTm2': {
            'description': '7ä¸ªå› ç´ çš„å˜å‹å™¨æ¸©åº¦å˜åŒ–',
            'expected_features': 7,
            'data_path': 'dataset/ETT/ETTm2.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'exchange_rate': {
            'description': '8ä¸ªå›½å®¶çš„æ±‡ç‡å˜åŒ–',
            'expected_features': 8,
            'data_path': 'dataset/exchange/exchange_rate.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'illness': {
            'description': 'æµæ„Ÿæ‚£è€…æ¯”ä¾‹å’Œæ•°é‡',
            'expected_features': 7,
            'data_path': 'dataset/illness/national_illness.csv',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'Solar': {
            'description': '137ä¸ªå‘ç”µç«™å‘ç”µé‡',
            'expected_features': 137,
            'data_path': 'dataset/Solar/solar_AL.txt',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        },
        'PEMS': {
            'description': '5åˆ†é’Ÿçª—å£æ”¶é›†çš„å…¬å…±äº¤é€šç½‘ç»œæ•°æ®',
            'expected_features': 307,
            'data_path': 'dataset/PEMS/PEMS04.npz',
            'seq_len': 96,
            'pred_len': 96,
            'scaler_type': 'standard'
        }
    }
    
    @classmethod
    def generate_config(cls, dataset_name: str, **kwargs) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®é›†é…ç½®
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            **kwargs: é¢å¤–çš„é…ç½®å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„é…ç½®å­—å…¸
        """
        if dataset_name not in cls.DATASET_CONFIGS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
        
        # è·å–æ•°æ®é›†ç‰¹å®šé…ç½®
        dataset_config = cls.DATASET_CONFIGS[dataset_name]
        
        # å¤åˆ¶åŸºç¡€é…ç½®
        config = cls._deep_copy_dict(cls.BASE_CONFIG)
        
        # æ›´æ–°æ¨¡å‹é…ç½®
        config['model']['input_dim'] = dataset_config['expected_features']
        config['model']['output_dim'] = dataset_config['expected_features']
        config['model']['seq_len'] = dataset_config.get('seq_len', 96)
        config['model']['pred_len'] = dataset_config.get('pred_len', 96)
        
        # æ›´æ–°æ•°æ®é…ç½®
        config['data']['dataset_name'] = dataset_name
        config['data']['data_path'] = dataset_config['data_path']
        config['data']['seq_len'] = dataset_config.get('seq_len', 96)
        config['data']['pred_len'] = dataset_config.get('pred_len', 96)
        config['data']['scaler_type'] = dataset_config.get('scaler_type', 'standard')
        
        # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if dataset_config['expected_features'] > 500:
            config['data']['batch_size'] = 16  # å¤§æ•°æ®é›†ä½¿ç”¨å°æ‰¹æ¬¡
        elif dataset_config['expected_features'] > 100:
            config['data']['batch_size'] = 32  # ä¸­ç­‰æ•°æ®é›†
        else:
            config['data']['batch_size'] = 64  # å°æ•°æ®é›†ä½¿ç”¨å¤§æ‰¹æ¬¡
        
        # åº”ç”¨é¢å¤–å‚æ•°
        config = cls._apply_kwargs(config, kwargs)
        
        return config
    
    @classmethod
    def _deep_copy_dict(cls, d: Dict) -> Dict:
        """æ·±æ‹·è´å­—å…¸"""
        import copy
        return copy.deepcopy(d)
    
    @classmethod
    def _apply_kwargs(cls, config: Dict, kwargs: Dict) -> Dict:
        """åº”ç”¨é¢å¤–çš„é…ç½®å‚æ•°"""
        for key, value in kwargs.items():
            if '.' in key:
                # æ”¯æŒåµŒå¥—é”®ï¼Œå¦‚ 'model.hidden_dim'
                keys = key.split('.')
                current = config
                for k in keys[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                current[keys[-1]] = value
            else:
                config[key] = value
        
        return config
    
    @classmethod
    def save_config(cls, config: Dict, filepath: str):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    @classmethod
    def load_config(cls, filepath: str) -> Dict:
        """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    @classmethod
    def get_supported_datasets(cls) -> List[str]:
        """è·å–æ”¯æŒçš„æ•°æ®é›†åˆ—è¡¨"""
        return list(cls.DATASET_CONFIGS.keys())
    
    @classmethod
    def get_dataset_info(cls, dataset_name: str) -> Dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if dataset_name not in cls.DATASET_CONFIGS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
        
        return cls.DATASET_CONFIGS[dataset_name].copy()

    @classmethod
    def validate_config(cls, config: Dict) -> bool:
        """éªŒè¯é…ç½®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§"""
        try:
            # æ£€æŸ¥å¿…éœ€çš„é¡¶çº§é”®
            required_keys = ['model', 'data', 'training']
            for key in required_keys:
                if key not in config:
                    print(f"é”™è¯¯: ç¼ºå°‘å¿…éœ€çš„é…ç½®èŠ‚: {key}")
                    return False
            
            # éªŒè¯æ¨¡å‹é…ç½®
            model_config = config['model']
            required_model_keys = ['input_dim', 'output_dim', 'hidden_dim', 'num_experts']
            for key in required_model_keys:
                if key not in model_config or model_config[key] <= 0:
                    print(f"é”™è¯¯: æ¨¡å‹é…ç½®ä¸­{key}æ— æ•ˆ")
                    return False
            
            # éªŒè¯æ•°æ®é…ç½®
            data_config = config['data']
            if data_config.get('seq_len', 0) <= 0 or data_config.get('pred_len', 0) <= 0:
                print("é”™è¯¯: åºåˆ—é•¿åº¦æˆ–é¢„æµ‹é•¿åº¦æ— æ•ˆ")
                return False
            
            # éªŒè¯è®­ç»ƒé…ç½®
            training_config = config['training']
            if training_config.get('epochs', 0) <= 0:
                print("é”™è¯¯: è®­ç»ƒè½®æ•°æ— æ•ˆ")
                return False
            
            if training_config.get('learning_rate', 0) <= 0:
                print("é”™è¯¯: å­¦ä¹ ç‡æ— æ•ˆ")
                return False
            
            print("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False

    @classmethod
    def get_config_template(cls) -> Dict:
        """è·å–é…ç½®æ¨¡æ¿çš„æ·±æ‹·è´"""
        return cls._deep_copy_dict(cls.BASE_CONFIG)


def main():
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆæ‰€æœ‰æ•°æ®é›†çš„é…ç½®æ–‡ä»¶"""
    generator = ConfigGenerator()
    
    # åˆ›å»ºé…ç½®ç›®å½•
    os.makedirs('configs/datasets', exist_ok=True)
    
    # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆé…ç½®
    for dataset_name in generator.get_supported_datasets():
        print(f"ç”Ÿæˆæ•°æ®é›† {dataset_name} çš„é…ç½®...")
        
        # ç”Ÿæˆé…ç½®
        config = generator.generate_config(dataset_name)
        
        # ä¿å­˜é…ç½®
        config_path = f'configs/datasets/{dataset_name}_config.yaml'
        generator.save_config(config, config_path)
        
        print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    print("æ‰€æœ‰æ•°æ®é›†é…ç½®ç”Ÿæˆå®Œæˆï¼")


if __name__ == '__main__':
    main()