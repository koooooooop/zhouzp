#!/usr/bin/env python3
"""
æ¢¯åº¦çˆ†ç‚¸ä¿®å¤éªŒè¯è„šæœ¬
æµ‹è¯•ä¿®å¤åçš„ä»£ç æ˜¯å¦èƒ½ç¨³å®šè®­ç»ƒ
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.m2_moep import M2_MOEP
from data.universal_dataset import UniversalDataModule
from utils.losses import CompositeLoss
from utils.metrics import calculate_metrics
from configs.config_generator import ConfigGenerator

def test_gradient_stability():
    """æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§"""
    print("ğŸ”¬ å¼€å§‹æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•...")
    
    # åŠ è½½è¶…ç¨³å®šé…ç½®
    config_path = "configs/weather_stable.yaml"
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # è®¾å¤‡æ£€æŸ¥
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–æ•°æ®æ¨¡å—
    data_module = UniversalDataModule(config)
    
    # æ›´æ–°é…ç½®ä¸­çš„å®é™…ç‰¹å¾æ•°
    actual_features = data_module.get_dataset_info()['num_features']
    config['model']['input_dim'] = actual_features
    config['model']['output_dim'] = actual_features
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = M2_MOEP(config).to(device)
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # åˆå§‹åŒ–æŸå¤±å‡½æ•°
    criterion = CompositeLoss(config)
    
    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        model.parameters(),
        lr=config['training']['learning_rate'],
        weight_decay=config['training'].get('weight_decay', 1e-4)
    )
    
    # è·å–è®­ç»ƒæ•°æ®
    train_loader = data_module.get_train_loader()
    
    # æ¢¯åº¦ç›‘æ§
    gradient_stats = []
    loss_stats = []
    
    print("ğŸ“Š å¼€å§‹5ä¸ªbatchçš„æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•...")
    
    model.train()
    for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
        if batch_idx >= 5:  # åªæµ‹è¯•5ä¸ªbatch
            break
            
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        
        try:
            # æ¨¡å‹é¢„æµ‹
            output = model(batch_x, ground_truth=batch_y, return_aux_info=True)
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # è®¡ç®—æŸå¤±
            losses = criterion(
                predictions=predictions,
                targets=batch_y,
                expert_weights=aux_info.get('expert_weights'),
                expert_embeddings=aux_info.get('expert_embeddings'),
                flow_embeddings=None,
                flow_log_det=aux_info.get('flow_log_det')
            )
            total_loss = losses['total']
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # è®¡ç®—æ¢¯åº¦èŒƒæ•°
            total_grad_norm = 0.0
            param_count = 0
            for param in model.parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    total_grad_norm += param_norm.item() ** 2
                    param_count += 1
            total_grad_norm = total_grad_norm ** 0.5
            
            # è®°å½•ç»Ÿè®¡
            gradient_stats.append(total_grad_norm)
            loss_stats.append(total_loss.item())
            
            # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                print(f"âŒ Batch {batch_idx}: æŸå¤±åŒ…å«NaN/Inf")
                return False
            
            if torch.isnan(torch.tensor(total_grad_norm)) or torch.isinf(torch.tensor(total_grad_norm)):
                print(f"âŒ Batch {batch_idx}: æ¢¯åº¦åŒ…å«NaN/Inf")
                return False
            
            if total_grad_norm > 100.0:
                print(f"âŒ Batch {batch_idx}: æ¢¯åº¦çˆ†ç‚¸ (norm={total_grad_norm:.4f})")
                return False
            
            # åº”ç”¨æ¢¯åº¦è£å‰ª
            grad_clip_threshold = config['training']['gradient_clip']
            if total_grad_norm > grad_clip_threshold:
                clip_factor = grad_clip_threshold / (total_grad_norm + 1e-6)
                for param in model.parameters():
                    if param.grad is not None:
                        param.grad.data.mul_(clip_factor)
                print(f"ğŸ”§ Batch {batch_idx}: æ¢¯åº¦è£å‰ª {total_grad_norm:.4f} -> {grad_clip_threshold:.4f}")
            
            optimizer.step()
            
            print(f"âœ… Batch {batch_idx}: Loss={total_loss.item():.4f}, GradNorm={total_grad_norm:.4f}")
            
        except Exception as e:
            print(f"âŒ Batch {batch_idx}: è®­ç»ƒå¤±è´¥ - {e}")
            return False
    
    # åˆ†æç»Ÿè®¡ç»“æœ
    print("\nğŸ“ˆ æ¢¯åº¦ç¨³å®šæ€§åˆ†æ:")
    print(f"   - å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(gradient_stats):.4f}")
    print(f"   - æœ€å¤§æ¢¯åº¦èŒƒæ•°: {np.max(gradient_stats):.4f}")
    print(f"   - æ¢¯åº¦èŒƒæ•°æ ‡å‡†å·®: {np.std(gradient_stats):.4f}")
    print(f"   - å¹³å‡æŸå¤±: {np.mean(loss_stats):.4f}")
    print(f"   - æŸå¤±æ ‡å‡†å·®: {np.std(loss_stats):.4f}")
    
    # åˆ¤æ–­ç¨³å®šæ€§
    if np.max(gradient_stats) < 10.0 and np.std(gradient_stats) < 5.0:
        print("âœ… æ¢¯åº¦ç¨³å®šæ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print("âŒ æ¢¯åº¦ä»ç„¶ä¸ç¨³å®š")
        return False

def test_fft_fusion():
    """æµ‹è¯•FFTèåˆçš„æ•°å€¼ç¨³å®šæ€§"""
    print("\nğŸ”¬ æµ‹è¯•FFTèåˆæ•°å€¼ç¨³å®šæ€§...")
    
    from models.expert import FFTmsMambaExpert
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = {
        'model': {
            'input_dim': 64,
            'output_dim': 64,
            'seq_len': 96,
            'pred_len': 96,
            'current_expert_id': 0,
            'expert_params': {
                'mamba_d_model': 32,
                'mamba_scales': [1, 2]
            }
        }
    }
    
    expert = FFTmsMambaExpert(config)
    
    # æµ‹è¯•è¾“å…¥
    batch_size = 4
    seq_len = 96
    input_dim = 64
    
    x = torch.randn(batch_size, seq_len, input_dim)
    
    try:
        # æµ‹è¯•FFTèåˆ
        x_proj = expert.input_projection(x)
        fused_x = expert._stable_fft_fusion(x_proj)
        
        # æ£€æŸ¥è¾“å‡º
        if torch.isnan(fused_x).any() or torch.isinf(fused_x).any():
            print("âŒ FFTèåˆè¾“å‡ºåŒ…å«NaN/Inf")
            return False
        
        if fused_x.shape != x_proj.shape:
            print(f"âŒ FFTèåˆç»´åº¦ä¸åŒ¹é…: {fused_x.shape} vs {x_proj.shape}")
            return False
        
        print("âœ… FFTèåˆæ•°å€¼ç¨³å®šæ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ FFTèåˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_loss_function():
    """æµ‹è¯•æŸå¤±å‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§"""
    print("\nğŸ”¬ æµ‹è¯•æŸå¤±å‡½æ•°æ•°å€¼ç¨³å®šæ€§...")
    
    config = {
        'training': {
            'loss_weights': {
                'prediction': 1.0,
                'reconstruction': 0.01,
                'triplet': 0.01
            },
            'triplet_margin': 0.1
        }
    }
    
    criterion = CompositeLoss(config)
    
    # æµ‹è¯•æ•°æ®
    batch_size = 8
    pred_len = 96
    output_dim = 21
    
    predictions = torch.randn(batch_size, pred_len, output_dim)
    targets = torch.randn(batch_size, pred_len, output_dim)
    expert_weights = torch.softmax(torch.randn(batch_size, 2), dim=1)
    expert_embeddings = torch.randn(batch_size, pred_len, 128)
    
    try:
        losses = criterion(
            predictions=predictions,
            targets=targets,
            expert_weights=expert_weights,
            expert_embeddings=expert_embeddings
        )
        
        total_loss = losses['total']
        
        # æ£€æŸ¥æŸå¤±
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("âŒ æŸå¤±å‡½æ•°è¾“å‡ºåŒ…å«NaN/Inf")
            return False
        
        if total_loss.item() > 100.0:
            print(f"âŒ æŸå¤±å€¼è¿‡å¤§: {total_loss.item()}")
            return False
        
        print(f"âœ… æŸå¤±å‡½æ•°æµ‹è¯•é€šè¿‡ï¼æ€»æŸå¤±: {total_loss.item():.4f}")
        return True
        
    except Exception as e:
        print(f"âŒ æŸå¤±å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ MÂ²-MOEPæ¢¯åº¦çˆ†ç‚¸ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("FFTèåˆç¨³å®šæ€§", test_fft_fusion),
        ("æŸå¤±å‡½æ•°ç¨³å®šæ€§", test_loss_function),
        ("æ¢¯åº¦ç¨³å®šæ€§", test_gradient_stability),
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“ç»“æœ
    print("\n" + "="*50)
    print("ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“:")
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   - {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¢¯åº¦çˆ†ç‚¸é—®é¢˜å·²ä¿®å¤ï¼")
    else:
        print("âš ï¸  ä»æœ‰é—®é¢˜éœ€è¦è§£å†³")
    
    return passed == len(results)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 