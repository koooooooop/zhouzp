#!/usr/bin/env python3
"""
MÂ²-MOEPè®­ç»ƒè„šæœ¬
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
import numpy as np
from tqdm import tqdm
import logging
from datetime import datetime
from typing import Dict, Optional, Tuple
import argparse
import json

from models.m2_moep import M2_MOEP
from data.universal_dataset import UniversalDataModule
from utils.metrics import calculate_metrics
from configs.config_generator import ConfigGenerator


class OutputManager:
    """
    ç»Ÿä¸€çš„è¾“å‡ºç®¡ç†å™¨
    è´Ÿè´£åˆ›å»ºå’Œç®¡ç†æ‰€æœ‰è¾“å‡ºç›®å½•
    """
    
    def __init__(self, experiment_name: str = None, base_output_dir: str = "output"):
        """
        åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨
        
        Args:
            experiment_name: å®éªŒåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•
        """
        self.base_output_dir = base_output_dir
        
        # ç”Ÿæˆå®éªŒåç§°
        if experiment_name is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            experiment_name = f"experiment_{timestamp}"
        
        self.experiment_name = experiment_name
        self.experiment_dir = os.path.join(base_output_dir, experiment_name)
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self._create_directory_structure()
    
    def _create_directory_structure(self):
        """åˆ›å»ºå®Œæ•´çš„ç›®å½•ç»“æ„"""
        directories = [
            self.experiment_dir,
            self.get_checkpoints_dir(),
            self.get_logs_dir(),
            self.get_evaluation_dir(),
            self.get_configs_dir(),
            self.get_plots_dir()
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        
        print(f" åˆ›å»ºå®éªŒç›®å½•: {self.experiment_dir}")
    
    def get_checkpoints_dir(self) -> str:
        """è·å–æ£€æŸ¥ç‚¹ç›®å½•"""
        return os.path.join(self.experiment_dir, "checkpoints")
    
    def get_logs_dir(self) -> str:
        """è·å–æ—¥å¿—ç›®å½•"""
        return os.path.join(self.experiment_dir, "logs")
    
    def get_evaluation_dir(self) -> str:
        """è·å–è¯„ä¼°ç»“æœç›®å½•"""
        return os.path.join(self.experiment_dir, "evaluation")
    
    def get_configs_dir(self) -> str:
        """è·å–é…ç½®æ–‡ä»¶ç›®å½•"""
        return os.path.join(self.experiment_dir, "configs")
    
    def get_plots_dir(self) -> str:
        """è·å–å›¾è¡¨ç›®å½•"""
        return os.path.join(self.experiment_dir, "plots")
    
    def get_experiment_info_path(self) -> str:
        """è·å–å®éªŒä¿¡æ¯æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.experiment_dir, "experiment_info.json")
    
    def save_experiment_info(self, config: Dict, model_info: Dict = None):
        """ä¿å­˜å®éªŒä¿¡æ¯"""
        info = {
            "experiment_name": self.experiment_name,
            "created_at": datetime.now().isoformat(),
            "config": config,
            "model_info": model_info,
            "directory_structure": {
                "checkpoints": self.get_checkpoints_dir(),
                "logs": self.get_logs_dir(),
                "evaluation": self.get_evaluation_dir(),
                "configs": self.get_configs_dir(),
                "plots": self.get_plots_dir()
            }
        }
        
        with open(self.get_experiment_info_path(), 'w') as f:
            json.dump(info, f, indent=2, default=str)
        
        print(f"ğŸ’¾ å®éªŒä¿¡æ¯å·²ä¿å­˜åˆ°: {self.get_experiment_info_path()}")


class M2MOEPTrainer:
    """
    MÂ²-MOEPè®­ç»ƒå™¨
    ç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„è¾“å‡ºç®¡ç†ç³»ç»Ÿ
    """
    
    def __init__(self, config: Dict, output_manager: OutputManager = None):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨
        if output_manager is None:
            experiment_name = config.get('experiment_name', self._generate_experiment_name())
            output_manager = OutputManager(experiment_name)
        
        self.output_manager = output_manager
        
        # åŸºç¡€è®¾ç½®
        self.set_seed(config.get('seed', 42))
        self.setup_logging()
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        self.save_config()
        
        # åˆå§‹åŒ–æ•°æ®å’Œæ¨¡å‹
        self.data_module = UniversalDataModule(config)
        
        # æ ¹æ®å®é™…æ•°æ®æ›´æ–°é…ç½®
        actual_features = self.data_module.get_dataset_info()['num_features']
        self.config['model']['input_dim'] = actual_features
        self.config['model']['output_dim'] = actual_features
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = M2_MOEP(config).to(self.device)
        
        # ä¿å­˜å®éªŒä¿¡æ¯
        model_info = {
            'total_params': sum(p.numel() for p in self.model.parameters()),
            'trainable_params': sum(p.numel() for p in self.model.parameters() if p.requires_grad),
            'actual_features': actual_features
        }
        self.output_manager.save_experiment_info(self.config, model_info)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training'].get('weight_decay', 1e-4)
        )
        
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=config['training']['epochs'],
            eta_min=config['training'].get('min_lr', 1e-6)
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'metrics': []
        }
        
        self.logger.info(f" è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        self.logger.info(f" æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        self.logger.info(f" å®éªŒç›®å½•: {self.output_manager.experiment_dir}")
    
    def _generate_experiment_name(self) -> str:
        """ç”Ÿæˆå®éªŒåç§°"""
        dataset_name = self.config.get('data', {}).get('dataset_name', 'unknown')
        pred_len = self.config.get('model', {}).get('pred_len', 96)
        timestamp = datetime.now().strftime('%m%d_%H%M')
        return f"M2MOEP_{dataset_name}_pred{pred_len}_{timestamp}"
    
    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡å¤"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(self.output_manager.get_logs_dir(), f'train_{timestamp}.log')
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info(f" æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    def save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        config_path = os.path.join(self.output_manager.get_configs_dir(), 'training_config.yaml')
        with open(config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False, indent=2)
        
        self.logger.info(f" é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªå®Œæ•´çš„epoch"""
        self.model.train()
        train_loader = self.data_module.get_train_loader()
        
        epoch_losses = {
            'total': 0.0,
            'prediction': 0.0,
            'reconstruction': 0.0,
            'triplet': 0.0,
            'prototype': 0.0,
            'load_balance': 0.0
        }
        
        num_batches = len(train_loader)
        
        with tqdm(train_loader, desc=f'è®­ç»ƒ Epoch {self.current_epoch}') as pbar:
            for batch_idx, (batch_x, batch_y) in enumerate(pbar):
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                output = self.model(batch_x, ground_truth=batch_y, return_details=True)
                
                # è®¡ç®—æŸå¤±
                losses = self.model.compute_loss(output, batch_y, self.current_epoch)
                total_loss = losses['total']
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config['training'].get('gradient_clip', 1.0)
                )
                
                self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±
                for key, loss in losses.items():
                    if key in epoch_losses:
                        epoch_losses[key] += loss.item() if isinstance(loss, torch.Tensor) else loss
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f"{total_loss.item():.4f}",
                    'Pred': f"{losses.get('prediction', 0):.4f}",
                    'Triplet': f"{losses.get('triplet', 0):.4f}",
                    'Temp': f"{self.model.temperature.item():.3f}"
                })
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        return epoch_losses
    
    def validate_epoch(self, current_epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        val_loader = self.data_module.get_val_loader()
        
        epoch_losses = {
            'total': 0.0,
            'prediction': 0.0,
            'reconstruction': 0.0,
            'triplet': 0.0,
            'prototype': 0.0,
            'load_balance': 0.0
        }
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                output = self.model(batch_x, return_details=True)
                losses = self.model.compute_loss(output, batch_y, current_epoch)
                
                # ç´¯ç§¯æŸå¤±
                for key, loss in losses.items():
                    if key in epoch_losses:
                        epoch_losses[key] += loss.item() if isinstance(loss, torch.Tensor) else loss
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                all_predictions.append(output['predictions'].cpu())
                all_targets.append(batch_y.cpu())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        num_batches = len(val_loader)
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        all_predictions = torch.cat(all_predictions, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        metrics = calculate_metrics(all_predictions, all_targets)
        
        return epoch_losses, metrics
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'training_history': self.training_history
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.output_manager.get_checkpoints_dir(), 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
        if is_best:
            best_path = os.path.join(self.output_manager.get_checkpoints_dir(), 'best_checkpoint.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f" ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½è®­ç»ƒæ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            self.logger.warning(f" æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint['best_val_loss']
        self.training_history = checkpoint['training_history']
        
        self.logger.info(f" ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {checkpoint_path}")
    
    def train(self):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        self.logger.info(" å¼€å§‹è®­ç»ƒ...")
        
        epochs = self.config['training']['epochs']
        patience = self.config['training'].get('patience', 20)
        
        for epoch in range(self.current_epoch, epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_losses = self.train_epoch()
            val_losses, val_metrics = self.validate_epoch(self.current_epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['train_loss'].append(train_losses['total'])
            self.training_history['val_loss'].append(val_losses['total'])
            self.training_history['metrics'].append(val_metrics)
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            self.logger.info(
                f"Epoch {epoch:3d} | "
                f"Train Loss: {train_losses['total']:.4f} | "
                f"Val Loss: {val_losses['total']:.4f} | "
                f"RMSE: {val_metrics['RMSE']:.4f} | "
                f"MAE: {val_metrics['MAE']:.4f} | "
                f"RÂ²: {val_metrics['R2']:.4f}"
            )
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_losses['total'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_losses['total']
                self.patience_counter = 0
                self.logger.info(f" å‘ç°æ›´å¥½çš„æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            else:
                self.patience_counter += 1
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= patience:
                self.logger.info(f"â¹ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        self.logger.info(" è®­ç»ƒå®Œæˆï¼")
        self.save_training_results()
    
    def save_training_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦"""
        results = {
            'experiment_name': self.output_manager.experiment_name,
            'config': self.config,
            'training_history': self.training_history,
            'best_val_loss': self.best_val_loss,
            'final_epoch': self.current_epoch,
            'total_params': sum(p.numel() for p in self.model.parameters()),
            'completed_at': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ£€æŸ¥ç‚¹ç›®å½•
        results_path = os.path.join(self.output_manager.get_checkpoints_dir(), 'training_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜æœ€ç»ˆé…ç½®
        final_config_path = os.path.join(self.output_manager.get_configs_dir(), 'final_config.yaml')
        with open(final_config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False, indent=2)
        
        self.logger.info(f" è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        self.logger.info(f" æœ€ç»ˆé…ç½®å·²ä¿å­˜åˆ°: {final_config_path}")
    
    def get_model(self) -> M2_MOEP:
        """è·å–è®­ç»ƒå¥½çš„æ¨¡å‹"""
        return self.model
    
    def get_data_module(self) -> UniversalDataModule:
        """è·å–æ•°æ®æ¨¡å—"""
        return self.data_module
    
    def get_output_manager(self) -> OutputManager:
        """è·å–è¾“å‡ºç®¡ç†å™¨"""
        return self.output_manager


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MÂ²-MOEPè®­ç»ƒè„šæœ¬')
    
    # å®éªŒç®¡ç†
    parser.add_argument('--experiment-name', type=str, help='å®éªŒåç§°')
    parser.add_argument('--output-dir', type=str, default='output', help='è¾“å‡ºæ ¹ç›®å½•')
    
    # é…ç½®ç›¸å…³
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„æˆ–æ•°æ®é›†åç§°')
    parser.add_argument('--dataset', type=str, help='æ•°æ®é›†åç§°')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--seq-len', type=int, help='è¾“å…¥åºåˆ—é•¿åº¦')
    parser.add_argument('--pred-len', type=int, help='é¢„æµ‹é•¿åº¦')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, help='å­¦ä¹ ç‡')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--hidden-dim', type=int, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num-experts', type=int, help='ä¸“å®¶æ•°é‡')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæˆ–åŠ è½½é…ç½®
    config = None
    
    if args.config:
        if args.config.endswith('.yaml'):
            with open(args.config, 'r') as f:
                config = yaml.safe_load(f)
        else:
            config = ConfigGenerator.generate_config(args.config)
    elif args.dataset:
        config = ConfigGenerator.generate_config(args.dataset)
    else:
        print(" è¯·æŒ‡å®š --config æˆ– --dataset å‚æ•°")
        return 1
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.seq_len:
        config['model']['seq_len'] = args.seq_len
        config['data']['seq_len'] = args.seq_len
    if args.pred_len:
        config['model']['pred_len'] = args.pred_len
        config['data']['pred_len'] = args.pred_len
    if args.epochs:
        config['training']['epochs'] = args.epochs
    if args.batch_size:
        config['training']['batch_size'] = args.batch_size
    if args.learning_rate:
        config['training']['learning_rate'] = args.learning_rate
    if args.hidden_dim:
        config['model']['hidden_dim'] = args.hidden_dim
    if args.num_experts:
        config['model']['num_experts'] = args.num_experts
    if args.seed:
        config['seed'] = args.seed
    
    # è®¾ç½®å®éªŒåç§°
    if args.experiment_name:
        config['experiment_name'] = args.experiment_name
    
    try:
        # åˆ›å»ºè¾“å‡ºç®¡ç†å™¨
        output_manager = OutputManager(
            experiment_name=config.get('experiment_name'),
            base_output_dir=args.output_dir
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = M2MOEPTrainer(config, output_manager)
        
        # å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒ
        if args.resume:
            trainer.load_checkpoint(args.resume)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        print(f"\n è®­ç»ƒå®Œæˆï¼")
        print(f" æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: {output_manager.experiment_dir}")
        print(f" æ£€æŸ¥ç‚¹: {output_manager.get_checkpoints_dir()}")
        print(f" æ—¥å¿—: {output_manager.get_logs_dir()}")
        print(f" é…ç½®: {output_manager.get_configs_dir()}")
        
    except Exception as e:
        print(f" è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    main()