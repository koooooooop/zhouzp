#!/usr/bin/env python3
"""
MÂ²-MOEPè®­ç»ƒè„šæœ¬
"""

import os
import sys
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import numpy as np
from tqdm import tqdm
import logging
from datetime import datetime
from typing import Dict, Optional, Tuple
import argparse
import json

from models.m2_moep import M2_MOEP
from data.universal_dataset import UniversalDataModule
from utils.losses import CompositeLoss
from utils.metrics import calculate_metrics
from configs.config_generator import ConfigGenerator


class M2MOEPTrainer:
    """MÂ²-MOEPè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # é…ç½®éªŒè¯
        self._validate_config()
        
        # è®¾ç½®éšæœºç§å­
        self.set_seed(config.get('seed', 42))
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.setup_logging()
        
        # ç¡®ä¿Flowæ¨¡å‹å­˜åœ¨
        self._ensure_flow_model()
        
        # åˆå§‹åŒ–æ•°æ®æ¨¡å—
        self.data_module = UniversalDataModule(config)
        
        # æ›´æ–°é…ç½®ä¸­çš„å®é™…ç‰¹å¾æ•°
        actual_features = self.data_module.get_dataset_info()['num_features']
        self.config['model']['input_dim'] = actual_features
        self.config['model']['output_dim'] = actual_features
        
        # ç¡®ä¿é—¨æ§ç½‘ç»œé…ç½®åŒ…å«å¿…è¦çš„ç»´åº¦ä¿¡æ¯
        if 'embedding_dim' not in self.config['model']:
            self.config['model']['embedding_dim'] = min(128, actual_features)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = M2_MOEP(config).to(self.device)
        
        # ğŸ”§ ä¿®å¤ï¼šä¸å†ä½¿ç”¨ç‹¬ç«‹çš„CompositeLossï¼Œä½¿ç”¨æ¨¡å‹å†…ç½®çš„æŸå¤±è®¡ç®—
        # self.criterion = CompositeLoss(config)  # æ³¨é‡Šæ‰
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training'].get('weight_decay', 1e-4)
        )
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = CosineAnnealingLR(
            self.optimizer,
            T_max=config['training']['epochs'],
            eta_min=config['training'].get('min_lr', 1e-6)
        )
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'metrics': []
        }
        
        # æ¢¯åº¦ç›‘æ§å˜é‡
        self.gradient_stats = {
            'grad_norms': [],
            'grad_clip_count': 0,
            'max_grad_norm': 0.0,
            'adaptive_clip_value': self.config['training'].get('gradient_clip', 1.0)
        }
        
        # æ·»åŠ æ¢¯åº¦è£å‰ªé˜ˆå€¼å±æ€§
        self.grad_clip_threshold = self.config['training'].get('gradient_clip', 1.0)
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = config.get('save_dir', 'checkpoints')
        os.makedirs(self.save_dir, exist_ok=True)
        
        self.logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        self.logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # è®­ç»ƒé…ç½® - æ·»åŠ é»˜è®¤å€¼
        train_config = config.get('training', {})
        self.epochs = train_config.get('epochs', 20)
        self.batch_size = train_config.get('batch_size', 16)
        self.learning_rate = train_config.get('learning_rate', 0.001)
    
    def set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    def _validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
        # éªŒè¯æ¨¡å‹é…ç½®
        model_config = self.config.get('model', {})
        if model_config.get('input_dim', 0) <= 0:
            raise ValueError("æ¨¡å‹è¾“å…¥ç»´åº¦å¿…é¡»å¤§äº0")
        
        if model_config.get('hidden_dim', 0) <= 0:
            raise ValueError("æ¨¡å‹éšè—å±‚ç»´åº¦å¿…é¡»å¤§äº0")
        
        if model_config.get('output_dim', 0) <= 0:
            raise ValueError("æ¨¡å‹è¾“å‡ºç»´åº¦å¿…é¡»å¤§äº0")
        
        if model_config.get('num_experts', 0) <= 0:
            raise ValueError("ä¸“å®¶æ•°é‡å¿…é¡»å¤§äº0")
        
        if model_config.get('seq_len', 0) <= 0:
            raise ValueError("åºåˆ—é•¿åº¦å¿…é¡»å¤§äº0")
        
        if model_config.get('pred_len', 0) <= 0:
            raise ValueError("é¢„æµ‹é•¿åº¦å¿…é¡»å¤§äº0")
        
        # éªŒè¯è®­ç»ƒé…ç½®
        train_config = self.config.get('training', {})
        if train_config.get('batch_size', 0) <= 0:
            raise ValueError("æ‰¹æ¬¡å¤§å°å¿…é¡»å¤§äº0")
        
        if train_config.get('learning_rate', 0) <= 0:
            raise ValueError("å­¦ä¹ ç‡å¿…é¡»å¤§äº0")
        
        if train_config.get('epochs', 0) <= 0:
            raise ValueError("è®­ç»ƒè½®æ•°å¿…é¡»å¤§äº0")
        
        # éªŒè¯æ•°æ®é…ç½®
        data_config = self.config.get('data', {})
        if not data_config.get('dataset_name'):
            raise ValueError("æ•°æ®é›†åç§°ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯æ¢¯åº¦è£å‰ªé…ç½®
        gradient_clip = train_config.get('gradient_clip', 1.0)
        if gradient_clip <= 0:
            raise ValueError("æ¢¯åº¦è£å‰ªé˜ˆå€¼å¿…é¡»å¤§äº0")
        
        # éªŒè¯Flowæ¨¡å‹é…ç½®
        flow_config = model_config.get('flow', {})
        if flow_config.get('latent_dim', 0) <= 0:
            raise ValueError("Flowæ½œåœ¨ç»´åº¦å¿…é¡»å¤§äº0")
        
        # éªŒè¯æ¸©åº¦é…ç½®
        temp_config = model_config.get('temperature', {})
        if temp_config.get('initial', 1.0) <= 0:
            raise ValueError("åˆå§‹æ¸©åº¦å¿…é¡»å¤§äº0")
        
        if temp_config.get('min', 0.1) <= 0:
            raise ValueError("æœ€å°æ¸©åº¦å¿…é¡»å¤§äº0")
        
        if temp_config.get('max', 10.0) <= 0:
            raise ValueError("æœ€å¤§æ¸©åº¦å¿…é¡»å¤§äº0")
        
        # éªŒè¯æŸå¤±æƒé‡é…ç½®
        loss_weights = train_config.get('loss_weights', {})
        for key, value in loss_weights.items():
            if key.startswith('init_sigma_') and value <= 0:
                raise ValueError(f"æŸå¤±æƒé‡åˆå§‹åŒ–å‚æ•°{key}å¿…é¡»å¤§äº0")
        
        print("é…ç½®éªŒè¯é€šè¿‡ âœ“")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = 'logs'
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(log_dir, f'train_{timestamp}.log')
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def _ensure_flow_model(self):
        """ç¡®ä¿Flowæ¨¡å‹å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨åˆ™è‡ªåŠ¨é¢„è®­ç»ƒ"""
        flow_model_path = self.config['training'].get('flow_model_path', 'flow_model_default.pth')
        
        if os.path.exists(flow_model_path):
            print(f"Flowæ¨¡å‹å·²å­˜åœ¨: {flow_model_path}")
            return
        
        print(f"Flowæ¨¡å‹æœªæ‰¾åˆ°ï¼Œå¼€å§‹è‡ªåŠ¨é¢„è®­ç»ƒ: {flow_model_path}")
        
        try:
            # å¯¼å…¥é¢„è®­ç»ƒå‡½æ•°
            from pretrain_flow import pretrain_flow_model
            success = pretrain_flow_model(self.config, flow_model_path)
            
            if success:
                print(f"Flowæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ: {flow_model_path}")
            else:
                print(f"Flowæ¨¡å‹é¢„è®­ç»ƒå¤±è´¥ï¼Œå°†ç»§ç»­è®­ç»ƒä½†ä¸ä½¿ç”¨Flowé‡æ„")
                
        except Exception as e:
            print(f"Flowæ¨¡å‹é¢„è®­ç»ƒå¤±è´¥: {e}")
            print("å°†ç»§ç»­è®­ç»ƒï¼Œä½†ä¸ä½¿ç”¨Flowé‡æ„æŸå¤±")
    
    def _check_layer_gradients(self):
        """æ£€æŸ¥æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°ï¼Œç”¨äºè°ƒè¯•"""
        layer_grad_norms = {}
        problematic_layers = []
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                layer_grad_norms[name] = grad_norm
                
                # æ£€æŸ¥å¼‚å¸¸æ¢¯åº¦
                if torch.isnan(param.grad).any():
                    problematic_layers.append(f"{name}: NaNæ¢¯åº¦")
                elif torch.isinf(param.grad).any():
                    problematic_layers.append(f"{name}: Infæ¢¯åº¦")
                elif grad_norm > 10.0:
                    problematic_layers.append(f"{name}: æ¢¯åº¦è¿‡å¤§({grad_norm:.4f})")
        
        # è¾“å‡ºå¼‚å¸¸ä¿¡æ¯
        if problematic_layers:
            self.logger.warning(f"å‘ç°å¼‚å¸¸æ¢¯åº¦: {'; '.join(problematic_layers)}")
        
        # è¾“å‡ºå‰5ä¸ªæœ€å¤§æ¢¯åº¦èŒƒæ•°çš„å±‚
        top_layers = sorted(layer_grad_norms.items(), key=lambda x: x[1], reverse=True)[:5]
        grad_info = [f"{name}: {norm:.4f}" for name, norm in top_layers]
        self.logger.debug(f"æ¢¯åº¦èŒƒæ•°æœ€å¤§çš„5å±‚: {'; '.join(grad_info)}")
    
    def _adaptive_gradient_clipping(self, losses: Dict[str, torch.Tensor]) -> float:
        """
        è‡ªé€‚åº”æ¢¯åº¦è£å‰ª - ä¿®å¤ç‰ˆæœ¬
        :param losses: æŸå¤±å­—å…¸
        :return: ä½¿ç”¨çš„è£å‰ªé˜ˆå€¼
        """
        # è®¡ç®—æ€»æ¢¯åº¦èŒƒæ•°
        total_norm = 0.0
        param_count = 0
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        total_norm = total_norm ** 0.5
        
        # ä¿®å¤æ¢¯åº¦è£å‰ªç­–ç•¥
        if total_norm > self.grad_clip_threshold:
            # ä¸è¦å°†é˜ˆå€¼é™åˆ°å¤ªä½
            min_threshold = 0.1  # æœ€å°é˜ˆå€¼
            
            # æ ¹æ®æ¢¯åº¦çˆ†ç‚¸ä¸¥é‡ç¨‹åº¦è°ƒæ•´
            if total_norm > 10.0:
                # ä¸¥é‡æ¢¯åº¦çˆ†ç‚¸
                self.grad_clip_threshold = max(min_threshold, self.grad_clip_threshold * 0.5)
            elif total_norm > 5.0:
                # ä¸­ç­‰æ¢¯åº¦çˆ†ç‚¸
                self.grad_clip_threshold = max(min_threshold, self.grad_clip_threshold * 0.8)
            else:
                # è½»å¾®æ¢¯åº¦çˆ†ç‚¸
                self.grad_clip_threshold = max(min_threshold, self.grad_clip_threshold * 0.9)
            
            self.logger.warning(f"æ£€æµ‹åˆ°æ¢¯åº¦çˆ†ç‚¸: {total_norm:.4f}")
            self.logger.info(f"è°ƒæ•´æ¢¯åº¦è£å‰ªé˜ˆå€¼ä¸º: {self.grad_clip_threshold:.4f}")
            
            # åº”ç”¨æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip_threshold)
        
        else:
            # æ¢¯åº¦æ­£å¸¸æ—¶ï¼Œé€æ¸æ¢å¤é˜ˆå€¼
            if self.grad_clip_threshold < 1.0:
                self.grad_clip_threshold = min(1.0, self.grad_clip_threshold * 1.01)
        
        return self.grad_clip_threshold
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        train_loader = self.data_module.get_train_loader()
        
        epoch_losses = {
            'total': 0.0,
            'prediction': 0.0,
            'reconstruction': 0.0,
            'triplet': 0.0,
            'contrastive': 0.0,
            'consistency': 0.0,
            'load_balance': 0.0,
            'prototype': 0.0
        }
        
        num_batches = len(train_loader)
        
        with tqdm(train_loader, desc=f'Epoch {self.current_epoch}') as pbar:
            for batch_idx, batch_data in enumerate(pbar):
                if len(batch_data) == 2:
                    batch_x, batch_y = batch_data
                else:
                    print(f"é…ç½®é”™è¯¯: æœŸæœ›2ä¸ªå€¼ï¼Œä½†å¾—åˆ°{len(batch_data)}ä¸ª")
                    continue
                
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰éƒ¨åˆ†ç»„ä»¶åœ¨CUDAä¸Šï¼ˆå¦‚Mambaä¸“å®¶ï¼‰
                model_has_cuda_components = any(
                    hasattr(expert, 'use_mamba') and expert.use_mamba 
                    for expert in self.model.experts if hasattr(expert, 'use_mamba')
                )
                
                if model_has_cuda_components and torch.cuda.is_available():
                    # å¦‚æœæ¨¡å‹æœ‰CUDAç»„ä»¶ï¼Œç¡®ä¿æ•°æ®ä¹Ÿåœ¨CUDAä¸Š
                    batch_x, batch_y = batch_x.cuda(), batch_y.cuda()
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                
                # æ¨¡å‹é¢„æµ‹ - ä¼ é€’ground_truthç”¨äºä¸‰å…ƒç»„æŒ–æ˜
                output = self.model(batch_x, ground_truth=batch_y, return_aux_info=True)
                
                # ğŸ”§ ä¿®å¤ï¼šç§»é™¤Flowé‡æ„ä»£ç ï¼Œæ¨¡å‹å†…éƒ¨å¤„ç†
                # predictions = output['predictions']
                # aux_info = output['aux_info']
                
                # è·å–Flowé‡æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                # reconstructed = None
                # if hasattr(self.data_module, 'get_flow_reconstruction'):
                #     reconstructed = self.data_module.get_flow_reconstruction(batch_x)
                #     if reconstructed is not None and reconstructed.numel() > 0:
                #         reconstructed = reconstructed.to(self.device)
                #         aux_info['original_input'] = batch_x
                
                # è®¡ç®—æŸå¤±
                losses = self.model.compute_loss(
                    outputs=output,
                    targets=batch_y,
                    epoch=self.current_epoch
                )
                total_loss = losses['total']
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                # å¢å¼ºçš„æ¢¯åº¦è£å‰ªå’Œç›‘æ§
                # 1. å…ˆè®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆä¸è¿›è¡Œè£å‰ªï¼‰
                total_grad_norm = 0.0
                for param in self.model.parameters():
                    if param.grad is not None:
                        param_norm = param.grad.data.norm(2)
                        total_grad_norm += param_norm.item() ** 2
                total_grad_norm = total_grad_norm ** 0.5
                
                # 2. æ¢¯åº¦ç»Ÿè®¡å’Œç›‘æ§
                self.gradient_stats['grad_norms'].append(total_grad_norm)
                self.gradient_stats['max_grad_norm'] = max(
                    self.gradient_stats['max_grad_norm'], 
                    total_grad_norm
                )
                
                # 3. æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
                if torch.isnan(torch.tensor(total_grad_norm)) or torch.isinf(torch.tensor(total_grad_norm)):
                    self.logger.warning(f"æ¢¯åº¦èŒƒæ•°å¼‚å¸¸: {total_grad_norm}")
                    # è·³è¿‡è¿™ä¸ªbatchçš„å‚æ•°æ›´æ–°
                    self.optimizer.zero_grad()
                    continue
                
                # 4. è‡ªé€‚åº”æ¢¯åº¦è£å‰ª - ç®€åŒ–ç‰ˆæœ¬
                clip_value = self.grad_clip_threshold
                if total_grad_norm > clip_value:
                    # åº”ç”¨æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_value)
                    
                    # ç®€åŒ–é˜ˆå€¼è°ƒæ•´
                    if total_grad_norm > 10.0:
                        self.grad_clip_threshold = max(0.1, self.grad_clip_threshold * 0.5)
                    
                    self.logger.warning(f"æ¢¯åº¦è£å‰ª: {total_grad_norm:.4f} -> {clip_value:.4f}")
                    self.gradient_stats['grad_clip_count'] += 1
                else:
                    # æ¢¯åº¦æ­£å¸¸æ—¶ï¼Œé€æ¸æ¢å¤é˜ˆå€¼
                    if self.grad_clip_threshold < 1.0:
                        self.grad_clip_threshold = min(1.0, self.grad_clip_threshold * 1.01)
                
                self.optimizer.step()
                
                # æ›´æ–°æ¸©åº¦è°ƒåº¦
                if hasattr(self.model, 'update_temperature_schedule'):
                    if 'expert_weights' in output and output['expert_weights'] is not None:
                        expert_weights = output['expert_weights']
                        if expert_weights.dim() == 2 and expert_weights.size(0) > 0:
                            expert_usage = expert_weights.mean(dim=0)
                            expert_entropy = -torch.sum(
                                expert_usage * torch.log(expert_usage + 1e-8)
                            )
                            self.model.update_temperature_schedule(self.current_epoch, expert_entropy)
                
                # ç´¯ç§¯æŸå¤± - ç®€åŒ–ç‰ˆæœ¬
                for key, loss in losses.items():
                    if key in epoch_losses:
                        epoch_losses[key] += loss.item() if isinstance(loss, torch.Tensor) else loss
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f"{total_loss.item():.4f}",
                    'Pred': f"{losses.get('prediction', 0):.4f}",
                    'Triplet': f"{losses.get('triplet', 0):.4f}",
                    'Temp': f"{self.model.temperature.item():.3f}",
                    'Grad': f"{total_grad_norm:.3f}",
                    'Clip': f"{clip_value:.3f}"
                })
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        return epoch_losses
    
    def validate_epoch(self, current_epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        éªŒè¯å•ä¸ªepoch
        
        Args:
            current_epoch (int): å½“å‰çš„è®­ç»ƒè½®æ¬¡
            
        Returns:
            éªŒè¯æŸå¤±å’ŒæŒ‡æ ‡
        """
        self.model.eval()
        val_loader = self.data_module.get_val_loader()
        
        epoch_losses = {
            'total': 0.0,
            'prediction': 0.0,
            'reconstruction': 0.0,
            'triplet': 0.0,
            'contrastive': 0.0,
            'consistency': 0.0,
            'load_balance': 0.0,
            'prototype': 0.0
        }
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰éƒ¨åˆ†ç»„ä»¶åœ¨CUDAä¸Šï¼ˆå¦‚Mambaä¸“å®¶ï¼‰
                model_has_cuda_components = any(
                    hasattr(expert, 'use_mamba') and expert.use_mamba 
                    for expert in self.model.experts if hasattr(expert, 'use_mamba')
                )
                
                if model_has_cuda_components and torch.cuda.is_available():
                    # å¦‚æœæ¨¡å‹æœ‰CUDAç»„ä»¶ï¼Œç¡®ä¿æ•°æ®ä¹Ÿåœ¨CUDAä¸Š
                    batch_x, batch_y = batch_x.cuda(), batch_y.cuda()
                
                # éªŒè¯æ—¶ä¸éœ€è¦ä¸‰å…ƒç»„æŒ–æ˜ï¼Œæ‰€ä»¥ä¸ä¼ é€’ground_truth
                output = self.model(batch_x, return_aux_info=True)
                
                # ğŸ”§ ä¿®å¤ï¼šç§»é™¤Flowé‡æ„ä»£ç ï¼Œæ¨¡å‹å†…éƒ¨å¤„ç†
                # predictions = output['predictions']
                # aux_info = output['aux_info']
                
                # è·å–Flowé‡æ„
                # reconstructed = None
                # if hasattr(self.data_module, 'get_flow_reconstruction'):
                #     reconstructed = self.data_module.get_flow_reconstruction(batch_x)
                #     if reconstructed is not None and reconstructed.numel() > 0:
                #         reconstructed = reconstructed.to(self.device)
                #         aux_info['original_input'] = batch_x
                
                # è®¡ç®—æŸå¤±
                losses = self.model.compute_loss(
                    outputs=output,
                    targets=batch_y,
                    epoch=current_epoch
                )
                total_loss = losses['total']
                
                # ç´¯ç§¯æŸå¤± - ç®€åŒ–ç‰ˆæœ¬
                for key, loss in losses.items():
                    if key in epoch_losses:
                        epoch_losses[key] += loss.item() if isinstance(loss, torch.Tensor) else loss
                
                # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ç”¨äºè®¡ç®—æŒ‡æ ‡
                all_predictions.append(output['predictions'].cpu())
                all_targets.append(batch_y.cpu())
        
        # è®¡ç®—å¹³å‡æŸå¤±
        num_batches = len(val_loader)
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        # è®¡ç®—æŒ‡æ ‡
        all_predictions = torch.cat(all_predictions, dim=0)
        all_targets = torch.cat(all_targets, dim=0)
        metrics = calculate_metrics(all_predictions, all_targets)
        
        # æ›´æ–°æ¸©åº¦è°ƒåº¦
        if self.config['model'].get('use_temperature_scheduler', False):
            self.model.update_temperature(total_loss, current_epoch)
        
        return epoch_losses, metrics
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'training_history': self.training_history
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.save_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_checkpoint.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.best_val_loss = checkpoint['best_val_loss']
        self.training_history = checkpoint['training_history']
        
        self.logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹æˆåŠŸ: {checkpoint_path}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        
        epochs = self.config['training']['epochs']
        patience = self.config['training'].get('patience', 20)
        
        for epoch in range(self.current_epoch, epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_losses = self.train_epoch()
            
            # éªŒè¯ä¸€ä¸ªepoch
            val_losses, val_metrics = self.validate_epoch(self.current_epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['train_loss'].append(train_losses['total'])
            self.training_history['val_loss'].append(val_losses['total'])
            self.training_history['metrics'].append(val_metrics)
            
            # æ—¥å¿—è¾“å‡º
            self.logger.info(
                f"Epoch {epoch:3d} | "
                f"Train Loss: {train_losses['total']:.4f} | "
                f"Val Loss: {val_losses['total']:.4f} | "
                f"RMSE: {val_metrics['RMSE']:.4f} | "
                f"MAE: {val_metrics['MAE']:.4f} | "
                f"RÂ²: {val_metrics['R2']:.4f} | "
                f"Temp: {self.model.temperature.item():.3f}"
            )
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_losses['total'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_losses['total']
                self.patience_counter = 0
                self.logger.info(f"æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
            else:
                self.patience_counter += 1
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.config['training'].get('save_interval', 10) == 0 or is_best:
                self.save_checkpoint(is_best)
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= patience:
                self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch}")
                break
        
        self.logger.info("è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_training_results()
    
    def save_training_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        results = {
            'config': self.config,
            'training_history': self.training_history,
            'best_val_loss': self.best_val_loss,
            'final_epoch': self.current_epoch,
            'model_info': {
                'total_params': sum(p.numel() for p in self.model.parameters()),
                'trainable_params': sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            }
        }
        
        # ä¿å­˜ä¸ºJSON
        results_path = os.path.join(self.save_dir, 'training_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.save_dir, 'final_config.yaml')
        with open(config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False, indent=2)
        
        self.logger.info(f"è®­ç»ƒç»“æœä¿å­˜åˆ°: {results_path}")
    
    def get_model(self) -> M2_MOEP:
        """è·å–è®­ç»ƒå¥½çš„æ¨¡å‹"""
        return self.model
    
    def get_data_module(self) -> UniversalDataModule:
        """è·å–æ•°æ®æ¨¡å—"""
        return self.data_module


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MÂ²-MOEPè®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset', type=str, help='æ•°æ®é›†åç§°')
    parser.add_argument('--epochs', type=int, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, help='å­¦ä¹ ç‡')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config.endswith('.yaml') or args.config.endswith('.yml'):
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
    else:
        # å‡è®¾æ˜¯æ•°æ®é›†åç§°ï¼Œç”Ÿæˆé…ç½®
        config = ConfigGenerator.generate_config(args.config)
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.dataset:
        config['data']['dataset_name'] = args.dataset
    if args.epochs:
        config['training']['epochs'] = args.epochs
    if args.batch_size:
        config['data']['batch_size'] = args.batch_size
    if args.learning_rate:
        config['training']['learning_rate'] = args.learning_rate
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = M2MOEPTrainer(config)
        
        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.resume:
            trainer.load_checkpoint(args.resume)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
    except ValueError as e:
        print(f"é…ç½®é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è®¾ç½®")
        return 1
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == '__main__':
    main()