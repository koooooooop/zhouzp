"""
å¤åˆæŸå¤±å‡½æ•°æ¨¡å— - å®Œæ•´å¤åŸç‰ˆæœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Optional, Tuple
import logging


class CompositeLoss(nn.Module):
    """å¤åˆæŸå¤±å‡½æ•° """

    def __init__(self, config):
        super().__init__()
        
        # æ·»åŠ loggeræˆ–ä½¿ç”¨printæ›¿ä»£
        self.logger = logging.getLogger(__name__)
        
        # æŒ‰ç…§æ–‡æ¡£çš„è®¾è®¡ï¼Œåªä¿ç•™ä¸‰ä¸ªæ ¸å¿ƒæŸå¤±
        loss_weights = config['training'].get('loss_weights', {})
        self.prediction_weight = loss_weights.get('prediction', 1.0)
        self.reconstruction_weight = loss_weights.get('reconstruction', 0.1)
        self.triplet_weight = loss_weights.get('triplet', 0.1)
        
        # æ–‡æ¡£è®¾è®¡ï¼šç§»é™¤èšç±»æŸå¤±ï¼
        # ç§»é™¤: self.clustering_weight
        # ç§»é™¤: self.balance_weight
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        
        # ä¸‰å…ƒç»„æŸå¤± - æŒ‰æ–‡æ¡£è®¾è®¡
        triplet_margin = config['training'].get('triplet_margin', 0.5)
        self.triplet_loss = TripletLoss(margin=triplet_margin)
        
        # æ•°å€¼ç¨³å®šæ€§å‚æ•°
        self.loss_clamp_min = 0.0
        self.loss_clamp_max = 100.0
        
        print("æŸå¤±å‡½æ•°:")
        print(f"   - é¢„æµ‹æŸå¤±æƒé‡: {self.prediction_weight}")
        print(f"   - é‡æ„æŸå¤±æƒé‡: {self.reconstruction_weight}")
        print(f"   - ä¸‰å…ƒç»„æŸå¤±æƒé‡: {self.triplet_weight}")

    def forward(self, predictions, targets, expert_weights=None, expert_embeddings=None, 
                flow_embeddings=None, flow_log_det=None, **kwargs):
        """
        å‰å‘ä¼ æ’­
        """
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥å¼ é‡éƒ½éœ€è¦æ¢¯åº¦
        if not predictions.requires_grad:
            predictions = predictions.requires_grad_(True)
        if not targets.requires_grad:
            targets = targets.requires_grad_(True)
        
        # è¾“å…¥æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        predictions = self._stabilize_tensor(predictions, "predictions")
        targets = self._stabilize_tensor(targets, "targets")
        
        # é¢„æµ‹æŸå¤± - ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æŸå¤±è®¡ç®—
        prediction_loss = F.mse_loss(predictions, targets, reduction='mean')
        prediction_loss = torch.clamp(prediction_loss, min=0.0, max=10.0)  # æ›´ä¸¥æ ¼çš„ä¸Šé™
        
        # æŸå¤±æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(prediction_loss) or torch.isinf(prediction_loss):
            print("è­¦å‘Š: é¢„æµ‹æŸå¤±åŒ…å«NaNæˆ–Infï¼Œé‡ç½®ä¸ºå®‰å…¨å€¼")
            prediction_loss = torch.tensor(1.0, device=predictions.device, requires_grad=True)
        
        # é‡æ„æŸå¤±
        reconstruction_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
        if flow_embeddings is not None:
            flow_embeddings = self._stabilize_tensor(flow_embeddings, "flow_embeddings")
            if not flow_embeddings.requires_grad:
                flow_embeddings = flow_embeddings.requires_grad_(True)
            
            if flow_embeddings.shape != targets.shape:
                # å¦‚æœflow_embeddingsçš„æ—¶é—´ç»´åº¦æ¯”targetså¤§ï¼Œæå–æœ€åçš„pred_lenæ­¥
                if (flow_embeddings.dim() == 3 and targets.dim() == 3 and 
                    flow_embeddings.shape[0] == targets.shape[0] and 
                    flow_embeddings.shape[2] == targets.shape[2] and 
                    flow_embeddings.shape[1] > targets.shape[1]):
                    
                    # æå–æœ€åçš„pred_lenä¸ªæ—¶é—´æ­¥
                    pred_len = targets.shape[1]
                    flow_embeddings_matched = flow_embeddings[:, -pred_len:, :]
                    
                    reconstruction_loss = F.mse_loss(flow_embeddings_matched, targets, reduction='mean')
                    reconstruction_loss = torch.clamp(reconstruction_loss, min=0.0, max=10.0)
                    
                    if torch.isnan(reconstruction_loss) or torch.isinf(reconstruction_loss):
                        print("è­¦å‘Š: é‡æ„æŸå¤±åŒ…å«NaNæˆ–Infï¼Œé‡ç½®ä¸ºé›¶")
                        reconstruction_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
                else:
                    # å…¶ä»–ç»´åº¦ä¸åŒ¹é…æƒ…å†µï¼Œè·³è¿‡é‡æ„æŸå¤±
                    print(f"è­¦å‘Š: é‡æ„æŸå¤±ç»´åº¦ä¸åŒ¹é… flow:{flow_embeddings.shape} vs targets:{targets.shape}")
                    reconstruction_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
            else:
                reconstruction_loss = F.mse_loss(flow_embeddings, targets, reduction='mean')
                reconstruction_loss = torch.clamp(reconstruction_loss, min=0.0, max=10.0)
                
                if torch.isnan(reconstruction_loss) or torch.isinf(reconstruction_loss):
                    print("è­¦å‘Š: é‡æ„æŸå¤±åŒ…å«NaNæˆ–Infï¼Œé‡ç½®ä¸ºé›¶")
                    reconstruction_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
        
        # ä¸‰å…ƒç»„æŸå¤± - ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›´ä¿å®ˆçš„è®¡ç®—
        triplet_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
        if expert_weights is not None and expert_embeddings is not None:
            try:
                # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                expert_weights = self._stabilize_tensor(expert_weights, "expert_weights")
                expert_embeddings = self._stabilize_tensor(expert_embeddings, "expert_embeddings")
                
                # ç¡®ä¿éœ€è¦æ¢¯åº¦
                if not expert_weights.requires_grad:
                    expert_weights = expert_weights.requires_grad_(True)
                if not expert_embeddings.requires_grad:
                    expert_embeddings = expert_embeddings.requires_grad_(True)
                
                # å®‰å…¨çš„ä¸‰å…ƒç»„æŸå¤±è®¡ç®—
                if expert_embeddings.numel() > 0 and expert_weights.numel() > 0:
                    triplet_loss = self.triplet_loss(expert_embeddings, expert_weights)
                    triplet_loss = torch.clamp(triplet_loss, min=0.0, max=5.0)  # æ›´ä¸¥æ ¼çš„ä¸Šé™
                    
                    if torch.isnan(triplet_loss) or torch.isinf(triplet_loss):
                        print("è­¦å‘Š: ä¸‰å…ƒç»„æŸå¤±åŒ…å«NaNæˆ–Infï¼Œé‡ç½®ä¸ºé›¶")
                        triplet_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
                        
            except Exception as e:
                print(f"è­¦å‘Š: ä¸‰å…ƒç»„æŸå¤±è®¡ç®—å¤±è´¥: {e}")
                triplet_loss = torch.tensor(0.0, device=predictions.device, requires_grad=True)
        
        # æ€»æŸå¤±è®¡ç®— - ğŸ”§ å…³é”®ä¿®å¤ï¼šæ›´ä¿å®ˆçš„æƒé‡
        total_loss = (
            self.prediction_weight * prediction_loss +
            self.reconstruction_weight * reconstruction_loss +
            self.triplet_weight * triplet_loss
        )
        
        # æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        total_loss = torch.clamp(total_loss, min=0.0, max=20.0)  # æ›´ä¸¥æ ¼çš„æ€»æŸå¤±ä¸Šé™
        
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("é”™è¯¯: æ€»æŸå¤±ä¸ºNaNæˆ–Infï¼Œä½¿ç”¨é¢„æµ‹æŸå¤±å¤‡ä»½")
            total_loss = prediction_loss
        
        # ç¡®ä¿æ€»æŸå¤±éœ€è¦æ¢¯åº¦
        if not total_loss.requires_grad:
            total_loss = total_loss.requires_grad_(True)
        
        return {
            'total': total_loss,
            'prediction': prediction_loss,
            'reconstruction': reconstruction_loss,
            'triplet': triplet_loss
        }

    def _stabilize_tensor(self, tensor, name="tensor"):
        """æ•°å€¼ç¨³å®šåŒ–å¤„ç† - å¢å¼ºç‰ˆ"""
        if tensor is None:
            return tensor
            
        # æ£€æŸ¥å’Œä¿®å¤NaN
        if torch.isnan(tensor).any():
            print(f"è­¦å‘Š: {name}åŒ…å«NaNï¼Œæ›¿æ¢ä¸º0")
            tensor = torch.nan_to_num(tensor, nan=0.0)
        
        # æ£€æŸ¥å’Œä¿®å¤Inf
        if torch.isinf(tensor).any():
            print(f"è­¦å‘Š: {name}åŒ…å«Infï¼Œè¿›è¡Œæˆªæ–­")
            tensor = torch.nan_to_num(tensor, posinf=5.0, neginf=-5.0)
        
        # æ•°å€¼èŒƒå›´è£å‰ª
        tensor = torch.clamp(tensor, min=-10.0, max=10.0)
        
        return tensor

    def compute_reconstruction_loss(self, x_original, x_reconstructed):
        """è®¡ç®—é‡æ„æŸå¤±"""
        if x_reconstructed is None:
            return torch.tensor(0.0, device=x_original.device)
        
        try:
            # ç¡®ä¿è¾“å…¥å½¢çŠ¶ä¸€è‡´
            if x_original.dim() == 3:  # [B, seq_len, input_dim]
                x_flat = x_original.view(x_original.size(0), -1)
            else:
                x_flat = x_original
            
            if x_reconstructed.dim() == 3:  # [B, seq_len, input_dim]
                x_recon_flat = x_reconstructed.view(x_reconstructed.size(0), -1)
            else:
                x_recon_flat = x_reconstructed
            
            # è®¡ç®—é‡æ„æŸå¤±
            recon_loss = F.mse_loss(x_recon_flat, x_flat)
            
            return recon_loss
        except Exception as e:
            # å¦‚æœå½¢çŠ¶ä¸åŒ¹é…æˆ–å…¶ä»–é”™è¯¯ï¼Œè¿”å›0æŸå¤±
            return torch.tensor(0.0, device=x_original.device)

    def compute_load_balancing_loss(self, routing_weights):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨ç‡
        expert_usage = routing_weights.mean(dim=0)
        
        # è®¡ç®—æ–¹å·®ï¼šç†æƒ³æƒ…å†µä¸‹æ‰€æœ‰ä¸“å®¶ä½¿ç”¨ç‡ç›¸ç­‰
        ideal_usage = 1.0 / routing_weights.size(1)
        balance_loss = torch.var(expert_usage) + F.mse_loss(expert_usage, torch.full_like(expert_usage, ideal_usage))
        
        return balance_loss

    def compute_kl_consistency_loss(self, routing_weights, embeddings=None):
        """KL-ä¸€è‡´æ€§æŸå¤±ï¼šå¯¹äºåµŒå…¥ç©ºé—´ç›¸è¿‘çš„æ ·æœ¬ï¼Œå…¶è·¯ç”±åˆ†å¸ƒåº”ç›¸ä¼¼ã€‚"""

        batch_size = routing_weights.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=routing_weights.device)

        # è‹¥æœªæä¾› embeddingsï¼Œåˆ™ç›´æ¥ä½¿ç”¨ routing_weights è®¡ç®—ç›¸ä¼¼åº¦
        if embeddings is None:
            embeddings = routing_weights

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = F.cosine_similarity(
            embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=-1
        )  # [B,B]

        # å®šä¹‰"ç›¸ä¼¼"é˜ˆå€¼
        similar_mask = (sim_matrix > 0.8).float() - torch.eye(batch_size, device=sim_matrix.device)

        if similar_mask.sum() == 0:
            return torch.tensor(0.0, device=routing_weights.device)

        # è®¡ç®— KL(P||Q) + KL(Q||P) å¯¹ç§° KL
        p = routing_weights.unsqueeze(1)  # [B,1,E]
        q = routing_weights.unsqueeze(0)  # [1,B,E]
        kl_pq = (p * (p.clamp(1e-8).log() - q.clamp(1e-8).log())).sum(-1)
        kl_qp = (q * (q.clamp(1e-8).log() - p.clamp(1e-8).log())).sum(-1)
        sym_kl = kl_pq + kl_qp  # [B,B]

        loss = (sym_kl * similar_mask).sum() / similar_mask.sum()
        return loss

    def update_epoch(self, epoch):
        """æ›´æ–°å½“å‰epoch"""
        self.current_epoch = epoch


class TripletLoss(nn.Module):
    """
    æŒ‰æ–‡æ¡£è®¾è®¡çš„ä¸‰å…ƒç»„æŸå¤±ï¼šåŸºäºé¢„æµ‹æ€§èƒ½æ„å»ºä¸‰å…ƒç»„
    """
    def __init__(self, margin=0.5, mining_strategy='batch_hard'):
        super().__init__()
        self.margin = margin
        self.mining_strategy = mining_strategy

    def forward(self, embeddings, expert_weights):
        """
        æŒ‰æ–‡æ¡£è®¾è®¡çš„ä¸‰å…ƒç»„æŸå¤±è®¡ç®—
        æ–‡æ¡£åŸç†ï¼šå¦‚æœä¸“å®¶kå¯¹é”šç‚¹é¢„æµ‹æœ€å‡†ç¡®ï¼Œé‚£ä¹ˆå¦ä¸€ä¸ªåŒæ ·ç”±ä¸“å®¶kä¸»å¯¼çš„å·¥ä½œè´Ÿè½½ä¸ºæ­£æ ·æœ¬
        """
        if embeddings.size(0) < 3:  # è‡³å°‘éœ€è¦3ä¸ªæ ·æœ¬æ„æˆä¸‰å…ƒç»„
            return torch.tensor(0.0, device=embeddings.device)
        
        batch_size = embeddings.size(0)
        
        # === 1. ç¡®å®šæ¯ä¸ªæ ·æœ¬çš„ä¸»å¯¼ä¸“å®¶ ===
        dominant_experts = torch.argmax(expert_weights, dim=1)  # [B]
        
        # === 2. æ„å»ºæ­£è´Ÿæ ·æœ¬mask ===
        # æ­£æ ·æœ¬ï¼šå…·æœ‰ç›¸åŒä¸»å¯¼ä¸“å®¶çš„æ ·æœ¬
        positive_mask = (dominant_experts.unsqueeze(0) == dominant_experts.unsqueeze(1))  # [B, B]
        # ç§»é™¤å¯¹è§’çº¿ (è‡ªå·±ä¸è‡ªå·±)
        positive_mask = positive_mask & ~torch.eye(batch_size, device=embeddings.device, dtype=torch.bool)
        
        # è´Ÿæ ·æœ¬ï¼šå…·æœ‰ä¸åŒä¸»å¯¼ä¸“å®¶çš„æ ·æœ¬
        negative_mask = ~positive_mask & ~torch.eye(batch_size, device=embeddings.device, dtype=torch.bool)
        
        # === 3. è®¡ç®—åµŒå…¥è·ç¦»çŸ©é˜µ ===
        dist_matrix = torch.cdist(embeddings, embeddings, p=2)  # [B, B]
        
        # === 4. Batch Hard Mining (æ–‡æ¡£æ¨èç­–ç•¥) ===
        losses = []
        
        for i in range(batch_size):
            # æ‰¾åˆ°å½“å‰é”šç‚¹çš„æ­£è´Ÿæ ·æœ¬
            pos_indices = positive_mask[i].nonzero(as_tuple=False).squeeze(1)
            neg_indices = negative_mask[i].nonzero(as_tuple=False).squeeze(1)
            
            if len(pos_indices) == 0 or len(neg_indices) == 0:
                continue
            
            if self.mining_strategy == 'batch_hard':
                # æœ€éš¾çš„æ­£æ ·æœ¬ï¼šè·ç¦»æœ€è¿œçš„æ­£æ ·æœ¬
                hardest_positive_dist = dist_matrix[i, pos_indices].max()
                # æœ€éš¾çš„è´Ÿæ ·æœ¬ï¼šè·ç¦»æœ€è¿‘çš„è´Ÿæ ·æœ¬
                hardest_negative_dist = dist_matrix[i, neg_indices].min()
                
                # è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
                loss = F.relu(hardest_positive_dist - hardest_negative_dist + self.margin)
                losses.append(loss)
        
        if not losses:
            return torch.tensor(0.0, device=embeddings.device)
        
        return torch.stack(losses).mean()


class FocalLoss(nn.Module):
    """
    Focal Loss for handling class imbalance
    """
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class TemperatureScaledCrossEntropy(nn.Module):
    """
    Temperature-scaled cross entropy for calibration
    """
    def __init__(self, temperature=1.0):
        super().__init__()
        self.temperature = temperature

    def forward(self, logits, targets):
        scaled_logits = logits / self.temperature
        return F.cross_entropy(scaled_logits, targets)


def compute_expert_usage_entropy(routing_weights):
    """
    è®¡ç®—ä¸“å®¶ä½¿ç”¨ç†µ
    :param routing_weights: è·¯ç”±æƒé‡ [B, E]
    :return: ç†µå€¼
    """
    expert_usage = routing_weights.mean(dim=0)
    entropy = -torch.sum(expert_usage * torch.log(expert_usage + 1e-8))
    return entropy


def compute_expert_diversity_loss(expert_outputs, routing_weights):
    """
    è®¡ç®—ä¸“å®¶å¤šæ ·æ€§æŸå¤±
    :param expert_outputs: ä¸“å®¶è¾“å‡º [B, E, D]
    :param routing_weights: è·¯ç”±æƒé‡ [B, E]
    :return: å¤šæ ·æ€§æŸå¤±
    """
    # åŠ æƒä¸“å®¶è¾“å‡º
    weighted_outputs = expert_outputs * routing_weights.unsqueeze(-1)
    
    # è®¡ç®—ä¸“å®¶é—´çš„ç›¸ä¼¼åº¦
    similarities = []
    num_experts = expert_outputs.size(1)
    
    for i in range(num_experts):
        for j in range(i + 1, num_experts):
            sim = F.cosine_similarity(
                weighted_outputs[:, i, :], 
                weighted_outputs[:, j, :], 
                dim=1
            ).mean()
            similarities.append(sim)
    
    if similarities:
        # å¤šæ ·æ€§æŸå¤±ï¼šæœ€å°åŒ–ä¸“å®¶é—´ç›¸ä¼¼åº¦
        diversity_loss = torch.stack(similarities).mean()
    else:
        diversity_loss = torch.tensor(0.0, device=expert_outputs.device)
    
    return diversity_loss


def compute_routing_consistency_loss(routing_weights, embeddings, temperature=0.1):
    """
    è®¡ç®—è·¯ç”±ä¸€è‡´æ€§æŸå¤±
    :param routing_weights: è·¯ç”±æƒé‡ [B, E]
    :param embeddings: åµŒå…¥å‘é‡ [B, D]
    :param temperature: æ¸©åº¦å‚æ•°
    :return: ä¸€è‡´æ€§æŸå¤±
    """
    # è®¡ç®—åµŒå…¥ç›¸ä¼¼åº¦
    embedding_sim = torch.mm(F.normalize(embeddings), F.normalize(embeddings).t())
    
    # è®¡ç®—è·¯ç”±æƒé‡ç›¸ä¼¼åº¦
    routing_sim = torch.mm(routing_weights, routing_weights.t())
    
    # ä¸€è‡´æ€§æŸå¤±ï¼šåµŒå…¥ç›¸ä¼¼çš„æ ·æœ¬åº”è¯¥æœ‰ç›¸ä¼¼çš„è·¯ç”±æƒé‡
    consistency_loss = F.mse_loss(
        F.softmax(embedding_sim / temperature, dim=1),
        F.softmax(routing_sim / temperature, dim=1)
    )
    
    return consistency_loss

def calculate_enhanced_load_balancing_loss(routing_weights, num_experts, epsilon=1e-8, diversity_weight=1.0, epoch=0):
    """
    ä¼˜åŒ–çš„ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé‡‡ç”¨æ›´æ¸©å’Œçš„å¤šæ ·æ€§ç­–ç•¥
    """
    batch_size = routing_weights.size(0)
    
    # === åŠ¨æ€æƒé‡è°ƒæ•´ç­–ç•¥ ===
    # å‰æœŸ(0-10 epoch)ï¼šå¼ºè°ƒå¤šæ ·æ€§ï¼Œé˜²æ­¢æ—©æœŸåå¡Œ
    # ä¸­æœŸ(10-30 epoch)ï¼šå¹³è¡¡å¤šæ ·æ€§å’Œæ€§èƒ½
    # åæœŸ(30+ epoch)ï¼šä¸»è¦å…³æ³¨æ€§èƒ½ï¼Œå‡å°‘å¤šæ ·æ€§çº¦æŸ
    
    if epoch < 10:
        # æ—©æœŸï¼šé˜²æ­¢ä¸“å®¶åå¡Œ
        diversity_factor = 1.0
        balance_factor = 2.0
    elif epoch < 30:
        # ä¸­æœŸï¼šé€æ¸å‡å°‘å¤šæ ·æ€§çº¦æŸ
        diversity_factor = max(0.3, 1.0 - (epoch - 10) * 0.035)  # 1.0 -> 0.3
        balance_factor = max(1.0, 2.0 - (epoch - 10) * 0.05)     # 2.0 -> 1.0
    else:
        # åæœŸï¼šä¸»è¦å…³æ³¨æ€§èƒ½
        diversity_factor = 0.1
        balance_factor = 0.5
    
    # === ä¸“å®¶ä½¿ç”¨ç‡åˆ†æ ===
    expert_usage = routing_weights.mean(dim=0)  # [num_experts]
    max_usage = expert_usage.max()
    min_usage = expert_usage.min()
    usage_ratio = max_usage / (min_usage + epsilon)
    
    # åªæœ‰åœ¨ä¸¥é‡ä¸å‡è¡¡æ—¶æ‰å¢åŠ æƒ©ç½š
    collapse_penalty = 1.0
    if usage_ratio > 20.0:  # æé«˜é˜ˆå€¼ï¼Œæ›´å®½æ¾
        collapse_penalty = 1.5 + min(1.0, usage_ratio / 50.0)  # æ›´æ¸©å’Œçš„æƒ©ç½š
    
    # === ç­–ç•¥1: æ¸©å’Œçš„ä½¿ç”¨ç‡å‡è¡¡æŸå¤± ===
    expert_usage = expert_usage.clamp_min(epsilon)
    target_usage = torch.full_like(expert_usage, 1.0 / num_experts)
    
    # ä½¿ç”¨æ›´æ¸©å’Œçš„L2æŸå¤±æ›¿ä»£KLæ•£åº¦
    balance_loss = F.mse_loss(expert_usage, target_usage) * balance_factor * collapse_penalty
    
    # === ç­–ç•¥2: è½¯å¤šæ ·æ€§çº¦æŸ ===
    # åªæœ‰åœ¨å¤šæ ·æ€§å› å­è¾ƒé«˜æ—¶æ‰è®¡ç®—
    diversity_loss = torch.tensor(0.0, device=routing_weights.device)
    if diversity_factor > 0.2:
        routing_norm = F.normalize(routing_weights.t(), p=2, dim=1)  # [num_experts, batch_size]
        similarity_matrix = torch.mm(routing_norm, routing_norm.t())  # [num_experts, num_experts]
        
        # ç§»é™¤å¯¹è§’çº¿
        eye_mask = torch.eye(num_experts, device=routing_weights.device)
        off_diagonal_sim = similarity_matrix * (1 - eye_mask)
        
        # æ›´æ¸©å’Œçš„å¤šæ ·æ€§æŸå¤±
        diversity_loss = off_diagonal_sim.abs().mean() * diversity_factor
    
    # === ç­–ç•¥3: è‡ªé€‚åº”ç†µæ­£åˆ™åŒ– ===
    # ç›®æ ‡ç†µæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´
    routing_entropy = -torch.sum(routing_weights * torch.log(routing_weights.clamp_min(epsilon)), dim=1)
    
    if epoch < 20:
        # æ—©æœŸï¼šé¼“åŠ±æ›´é«˜çš„ç†µï¼ˆæ›´åˆ†æ•£çš„è·¯ç”±ï¼‰
        target_entropy_ratio = 0.8
    else:
        # åæœŸï¼šå…è®¸æ›´é›†ä¸­çš„è·¯ç”±
        target_entropy_ratio = 0.5
    
    target_entropy = torch.log(torch.tensor(float(num_experts), device=routing_weights.device)) * target_entropy_ratio
    entropy_loss = F.mse_loss(routing_entropy, target_entropy.expand_as(routing_entropy)) * balance_factor * 0.3
    
    # === ç»„åˆæŸå¤± ===
    total_balance_loss = balance_loss + diversity_loss + entropy_loss
    
    return total_balance_loss