"""
è¶…å…¨é¢æµ‹è¯•å¥—ä»¶ - è¦†ç›–è®­ç»ƒå…¨æµç¨‹
ç»†è‡´æµ‹è¯•æ¯ä¸ªå…³é”®å‡½æ•°ã€å…³é”®å‚æ•°ã€å…³é”®æ¨¡å—
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import traceback
from typing import Dict, Any, List, Tuple
import logging
import warnings
import time
import psutil
import gc

# è®¾ç½®è­¦å‘Šçº§åˆ«
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

class UltraComprehensiveTestSuite:
    """è¶…å…¨é¢æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(level=logging.INFO)
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.test_results = {}
        self.critical_errors = []
        
        # æµ‹è¯•é…ç½®
        self.small_batch_size = 2
        self.medium_batch_size = 4
        self.large_batch_size = 8
        
        print(f"ğŸ”¥ è¶…å…¨é¢æµ‹è¯•å¼€å§‹ï¼è®¾å¤‡: {self.device}")
        print("=" * 100)
    
    def log_test(self, test_name: str, success: bool, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ…" if success else "âŒ"
        self.logger.info(f"{status} {test_name}: {message}")
        self.test_results[test_name] = {"success": success, "message": message}
        if not success:
            self.critical_errors.append(f"{test_name}: {message}")
    
    def test_config_generation_detailed(self) -> Dict[str, Any]:
        """æ·±åº¦æµ‹è¯•é…ç½®ç”Ÿæˆ"""
        from configs.config_generator import ConfigGenerator
        
        # æµ‹è¯•ä¸åŒæ•°æ®é›†çš„é…ç½®ç”Ÿæˆ
        test_configs = {}
        datasets = ['weather', 'etth1', 'etth2', 'ettm1', 'ettm2']
        
        for dataset in datasets:
            try:
                config = ConfigGenerator.generate_config(
                    dataset, batch_size=self.small_batch_size, epochs=2
                )
                test_configs[dataset] = config
                
                # éªŒè¯é…ç½®å®Œæ•´æ€§
                self._validate_config_completeness(config, dataset)
                
                self.log_test(f"é…ç½®ç”Ÿæˆ-{dataset}", True, f"é…ç½®ç”ŸæˆæˆåŠŸ")
                
            except Exception as e:
                self.log_test(f"é…ç½®ç”Ÿæˆ-{dataset}", False, f"é…ç½®ç”Ÿæˆå¤±è´¥: {e}")
        
        return test_configs['weather'] if 'weather' in test_configs else {}
    
    def _validate_config_completeness(self, config: Dict, dataset: str):
        """éªŒè¯é…ç½®å®Œæ•´æ€§"""
        required_sections = ['model', 'data', 'training', 'evaluation']
        for section in required_sections:
            if section not in config:
                raise ValueError(f"ç¼ºå¤±é…ç½®æ®µ: {section}")
        
        # æ¨¡å‹é…ç½®éªŒè¯
        model_config = config['model']
        required_model_keys = [
            'input_dim', 'output_dim', 'hidden_dim', 'num_experts',
            'expert_params', 'flow', 'triplet', 'diversity', 'temperature'
        ]
        
        for key in required_model_keys:
            if key not in model_config:
                raise ValueError(f"ç¼ºå¤±æ¨¡å‹é…ç½®: {key}")
        
        # ä¸“å®¶å‚æ•°éªŒè¯
        expert_params = model_config['expert_params']
        if 'mamba_d_model' not in expert_params:
            raise ValueError("ç¼ºå¤±mamba_d_model")
        if 'mamba_scales' not in expert_params:
            raise ValueError("ç¼ºå¤±mamba_scales")
        
        # éªŒè¯æ•°å€¼èŒƒå›´
        if model_config['num_experts'] < 1 or model_config['num_experts'] > 10:
            raise ValueError(f"ä¸“å®¶æ•°é‡å¼‚å¸¸: {model_config['num_experts']}")
        
        if model_config['hidden_dim'] < 32 or model_config['hidden_dim'] > 1024:
            raise ValueError(f"éšè—ç»´åº¦å¼‚å¸¸: {model_config['hidden_dim']}")
    
    def test_data_loading_detailed(self, config: Dict[str, Any]) -> bool:
        """æ·±åº¦æµ‹è¯•æ•°æ®åŠ è½½"""
        from data.universal_dataset import UniversalDataModule
        
        try:
            # åˆå§‹åŒ–æ•°æ®æ¨¡å—
            data_module = UniversalDataModule(config)
            
            # è·å–æ•°æ®åŠ è½½å™¨
            train_loader = data_module.get_train_loader()
            val_loader = data_module.get_val_loader()
            test_loader = data_module.get_test_loader()
            
            # æµ‹è¯•æ‰¹æ¬¡ä¸€è‡´æ€§
            self._test_batch_consistency(train_loader, "è®­ç»ƒé›†")
            self._test_batch_consistency(val_loader, "éªŒè¯é›†")
            self._test_batch_consistency(test_loader, "æµ‹è¯•é›†")
            
            # æµ‹è¯•æ•°æ®èŒƒå›´å’Œåˆ†å¸ƒ
            self._test_data_distribution(train_loader, "è®­ç»ƒé›†")
            
            # æµ‹è¯•æ•°æ®å¢å¼ºï¼ˆå¦‚æœæœ‰ï¼‰
            self._test_data_augmentation(data_module, config)
            
            self.log_test("æ•°æ®åŠ è½½è¯¦ç»†æµ‹è¯•", True, "æ•°æ®åŠ è½½è¯¦ç»†æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.log_test("æ•°æ®åŠ è½½è¯¦ç»†æµ‹è¯•", False, f"æ•°æ®åŠ è½½è¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _test_batch_consistency(self, loader, loader_name: str):
        """æµ‹è¯•æ‰¹æ¬¡ä¸€è‡´æ€§"""
        batch_shapes = []
        batch_dtypes = []
        
        for i, (batch_x, batch_y) in enumerate(loader):
            batch_shapes.append((batch_x.shape, batch_y.shape))
            batch_dtypes.append((batch_x.dtype, batch_y.dtype))
            
            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():
                raise ValueError(f"{loader_name}æ‰¹æ¬¡{i}è¾“å…¥åŒ…å«NaNæˆ–Inf")
            
            if torch.isnan(batch_y).any() or torch.isinf(batch_y).any():
                raise ValueError(f"{loader_name}æ‰¹æ¬¡{i}ç›®æ ‡åŒ…å«NaNæˆ–Inf")
            
            if i >= 3:  # åªæ£€æŸ¥å‰å‡ ä¸ªæ‰¹æ¬¡
                break
        
        # æ£€æŸ¥å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(batch_shapes)) > 1:
            raise ValueError(f"{loader_name}æ‰¹æ¬¡å½¢çŠ¶ä¸ä¸€è‡´: {batch_shapes}")
        
        # æ£€æŸ¥æ•°æ®ç±»å‹ä¸€è‡´æ€§
        if len(set(batch_dtypes)) > 1:
            raise ValueError(f"{loader_name}æ‰¹æ¬¡æ•°æ®ç±»å‹ä¸ä¸€è‡´: {batch_dtypes}")
    
    def _test_data_distribution(self, loader, loader_name: str):
        """æµ‹è¯•æ•°æ®åˆ†å¸ƒ"""
        all_x = []
        all_y = []
        
        for i, (batch_x, batch_y) in enumerate(loader):
            all_x.append(batch_x)
            all_y.append(batch_y)
            if i >= 5:  # åªæ£€æŸ¥å‰å‡ ä¸ªæ‰¹æ¬¡
                break
        
        if all_x:
            x_tensor = torch.cat(all_x, dim=0)
            y_tensor = torch.cat(all_y, dim=0)
            
            # æ£€æŸ¥ç»Ÿè®¡ç‰¹æ€§
            x_mean = x_tensor.mean().item()
            x_std = x_tensor.std().item()
            y_mean = y_tensor.mean().item()
            y_std = y_tensor.std().item()
            
            print(f"{loader_name}ç»Ÿè®¡: Xå‡å€¼={x_mean:.3f}, Xæ ‡å‡†å·®={x_std:.3f}, Yå‡å€¼={y_mean:.3f}, Yæ ‡å‡†å·®={y_std:.3f}")
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            if abs(x_mean) > 100 or x_std > 1000:
                raise ValueError(f"{loader_name}è¾“å…¥æ•°æ®åˆ†å¸ƒå¼‚å¸¸")
    
    def _test_data_augmentation(self, data_module, config):
        """æµ‹è¯•æ•°æ®å¢å¼º"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å¢å¼ºé…ç½®
        if 'augmentation' in config.get('data', {}):
            print("æ£€æµ‹åˆ°æ•°æ®å¢å¼ºé…ç½®")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®å¢å¼ºçš„å…·ä½“æµ‹è¯•
    
    def test_expert_network_detailed(self, config: Dict[str, Any]) -> bool:
        """è¯¦ç»†æµ‹è¯•ä¸“å®¶ç½‘ç»œ"""
        from models.expert import FFTmsMambaExpert
        
        try:
            # åˆ›å»ºä¸“å®¶ç½‘ç»œ
            expert = FFTmsMambaExpert(config).to(self.device)
            
            # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
            batch_sizes = [self.small_batch_size, self.medium_batch_size, self.large_batch_size]
            
            for batch_size in batch_sizes:
                self._test_expert_single_batch(expert, config, batch_size)
            
            # æµ‹è¯•ä¸“å®¶ç½‘ç»œçš„ç‰¹å®šåŠŸèƒ½
            self._test_expert_fft_fusion(expert, config)
            self._test_expert_multiscale_processing(expert, config)
            self._test_expert_personalization(expert, config)
            
            # æµ‹è¯•ä¸“å®¶ç½‘ç»œçš„æ•°å€¼ç¨³å®šæ€§
            self._test_expert_numerical_stability(expert, config)
            
            # æµ‹è¯•ä¸“å®¶ç½‘ç»œçš„æ¢¯åº¦æµ
            self._test_expert_gradient_flow(expert, config)
            
            self.log_test("ä¸“å®¶ç½‘ç»œè¯¦ç»†æµ‹è¯•", True, "ä¸“å®¶ç½‘ç»œè¯¦ç»†æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.log_test("ä¸“å®¶ç½‘ç»œè¯¦ç»†æµ‹è¯•", False, f"ä¸“å®¶ç½‘ç»œè¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _test_expert_single_batch(self, expert, config, batch_size):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œå•æ‰¹æ¬¡"""
        seq_len = config['data']['seq_len']
        input_dim = config['model']['input_dim']
        pred_len = config['data']['pred_len']
        output_dim = config['model']['output_dim']
        
        test_input = torch.randn(batch_size, seq_len, input_dim).to(self.device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = expert(test_input)
        
        # éªŒè¯è¾“å‡º
        expected_shape = (batch_size, pred_len, output_dim)
        if output.shape != expected_shape:
            raise ValueError(f"æ‰¹æ¬¡{batch_size}è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape} vs {expected_shape}")
        
        # æ•°å€¼æ£€æŸ¥
        if torch.isnan(output).any() or torch.isinf(output).any():
            raise ValueError(f"æ‰¹æ¬¡{batch_size}è¾“å‡ºåŒ…å«NaNæˆ–Inf")
        
        print(f"âœ“ ä¸“å®¶ç½‘ç»œæ‰¹æ¬¡{batch_size}æµ‹è¯•é€šè¿‡")
    
    def _test_expert_fft_fusion(self, expert, config):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œFFTèåˆ"""
        if hasattr(expert, 'fft_fusion') and expert.fft_fusion is not None:
            print("âœ“ ä¸“å®¶ç½‘ç»œåŒ…å«FFTèåˆå±‚")
            
            # æµ‹è¯•FFTèåˆçš„æ•°å€¼ç¨³å®šæ€§
            batch_size = self.small_batch_size
            seq_len = config['data']['seq_len']
            input_dim = config['model']['input_dim']
            
            # æç«¯è¾“å…¥æµ‹è¯•
            extreme_inputs = [
                torch.zeros(batch_size, seq_len, input_dim).to(self.device),
                torch.ones(batch_size, seq_len, input_dim).to(self.device),
                torch.randn(batch_size, seq_len, input_dim).to(self.device) * 100
            ]
            
            for i, extreme_input in enumerate(extreme_inputs):
                fused = expert._stable_fft_fusion(extreme_input)
                if torch.isnan(fused).any() or torch.isinf(fused).any():
                    raise ValueError(f"FFTèåˆæç«¯è¾“å…¥{i}å¤±è´¥")
        else:
            print("âš  ä¸“å®¶ç½‘ç»œä¸åŒ…å«FFTèåˆå±‚")
    
    def _test_expert_multiscale_processing(self, expert, config):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œå¤šå°ºåº¦å¤„ç†"""
        print("âœ“ ä¸“å®¶ç½‘ç»œå¤šå°ºåº¦å¤„ç†æµ‹è¯•")
        
        # æ£€æŸ¥learnable_deltas
        if hasattr(expert, 'learnable_deltas'):
            deltas = expert.learnable_deltas
            print(f"å¯å­¦ä¹ å°ºåº¦å‚æ•°: {deltas}")
            
            # æ£€æŸ¥å°ºåº¦å‚æ•°æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            if torch.any(deltas < 0.5) or torch.any(deltas > 10):
                raise ValueError(f"å°ºåº¦å‚æ•°å¼‚å¸¸: {deltas}")
        
        # æ£€æŸ¥å¤šå°ºåº¦å±‚æ•°é‡
        if hasattr(expert, 'multi_scale_mamba'):
            num_scales = len(expert.multi_scale_mamba)
            print(f"å¤šå°ºåº¦å±‚æ•°é‡: {num_scales}")
    
    def _test_expert_personalization(self, expert, config):
        """æµ‹è¯•ä¸“å®¶ä¸ªæ€§åŒ–"""
        if hasattr(expert, 'expert_personalization'):
            print("âœ“ ä¸“å®¶ç½‘ç»œåŒ…å«ä¸ªæ€§åŒ–å±‚")
            
            # æµ‹è¯•ä¸ªæ€§åŒ–å±‚çš„å‚æ•°
            for name, param in expert.expert_personalization.named_parameters():
                if torch.isnan(param).any() or torch.isinf(param).any():
                    raise ValueError(f"ä¸ªæ€§åŒ–å±‚å‚æ•°{name}å¼‚å¸¸")
        else:
            print("âš  ä¸“å®¶ç½‘ç»œä¸åŒ…å«ä¸ªæ€§åŒ–å±‚")
    
    def _test_expert_numerical_stability(self, expert, config):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œæ•°å€¼ç¨³å®šæ€§"""
        batch_size = self.small_batch_size
        seq_len = config['data']['seq_len']
        input_dim = config['model']['input_dim']
        
        # æç«¯è¾“å…¥æµ‹è¯•
        extreme_cases = [
            torch.zeros(batch_size, seq_len, input_dim).to(self.device),
            torch.ones(batch_size, seq_len, input_dim).to(self.device) * 1000,
            torch.randn(batch_size, seq_len, input_dim).to(self.device) * 0.001
        ]
        
        for i, extreme_input in enumerate(extreme_cases):
            try:
                with torch.no_grad():
                    output = expert(extreme_input)
                
                if torch.isnan(output).any() or torch.isinf(output).any():
                    raise ValueError(f"æç«¯è¾“å…¥{i}å¯¼è‡´æ•°å€¼å¼‚å¸¸")
                
                print(f"âœ“ æç«¯è¾“å…¥{i}æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âš  æç«¯è¾“å…¥{i}æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_expert_gradient_flow(self, expert, config):
        """æµ‹è¯•ä¸“å®¶ç½‘ç»œæ¢¯åº¦æµ"""
        batch_size = self.small_batch_size
        seq_len = config['data']['seq_len']
        input_dim = config['model']['input_dim']
        
        expert.train()
        test_input = torch.randn(batch_size, seq_len, input_dim, requires_grad=True).to(self.device)
        
        output = expert(test_input)
        loss = output.sum()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        gradient_norms = []
        for name, param in expert.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                gradient_norms.append(grad_norm)
                
                if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                    raise ValueError(f"å‚æ•°{name}æ¢¯åº¦å¼‚å¸¸")
        
        if not gradient_norms:
            raise ValueError("æ²¡æœ‰è®¡ç®—åˆ°æ¢¯åº¦")
        
        avg_grad_norm = np.mean(gradient_norms)
        print(f"âœ“ æ¢¯åº¦æµæµ‹è¯•é€šè¿‡ï¼Œå¹³å‡æ¢¯åº¦èŒƒæ•°: {avg_grad_norm:.6f}")
    
    def test_full_model_detailed(self, config: Dict[str, Any]) -> bool:
        """è¯¦ç»†æµ‹è¯•å®Œæ•´æ¨¡å‹"""
        from models.m2_moep import M2_MOEP
        
        try:
            # åˆ›å»ºæ¨¡å‹
            model = M2_MOEP(config).to(self.device)
            
            # æµ‹è¯•æ¨¡å‹ç»„ä»¶
            self._test_model_components(model, config)
            
            # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
            self._test_model_forward_pass(model, config)
            
            # æµ‹è¯•æ¨¡å‹çš„ç‰¹æ®ŠåŠŸèƒ½
            self._test_model_temperature_scheduling(model, config)
            self._test_model_expert_routing(model, config)
            self._test_model_triplet_mining(model, config)
            
            # æµ‹è¯•æ¨¡å‹çš„æ•°å€¼ç¨³å®šæ€§
            self._test_model_numerical_stability(model, config)
            
            self.log_test("å®Œæ•´æ¨¡å‹è¯¦ç»†æµ‹è¯•", True, "å®Œæ•´æ¨¡å‹è¯¦ç»†æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.log_test("å®Œæ•´æ¨¡å‹è¯¦ç»†æµ‹è¯•", False, f"å®Œæ•´æ¨¡å‹è¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _test_model_components(self, model, config):
        """æµ‹è¯•æ¨¡å‹ç»„ä»¶"""
        # æ£€æŸ¥flowæ¨¡å‹
        if hasattr(model, 'flow_model'):
            print("âœ“ æ¨¡å‹åŒ…å«Flowæ¨¡å‹")
            
            # æµ‹è¯•Flowæ¨¡å‹çš„ç¼–ç è§£ç 
            test_input = torch.randn(2, model.flow_model.input_dim).to(self.device)
            try:
                latent = model.flow_model.encode(test_input)
                reconstructed = model.flow_model.decode(latent)
                print(f"âœ“ Flowæ¨¡å‹ç¼–ç è§£ç æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âš  Flowæ¨¡å‹ç¼–ç è§£ç æµ‹è¯•å¤±è´¥: {e}")
        
        # æ£€æŸ¥é—¨æ§ç½‘ç»œ
        if hasattr(model, 'gating'):
            print("âœ“ æ¨¡å‹åŒ…å«é—¨æ§ç½‘ç»œ")
            
            # æµ‹è¯•é—¨æ§ç½‘ç»œ
            test_latent = torch.randn(2, model.flow_latent_dim).to(self.device)
            try:
                gating_output = model.gating(test_latent)
                expert_embeddings = model.gating.get_embeddings(test_latent)
                print(f"âœ“ é—¨æ§ç½‘ç»œæµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âš  é—¨æ§ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        
        # æ£€æŸ¥ä¸“å®¶ç½‘ç»œ
        if hasattr(model, 'experts'):
            print(f"âœ“ æ¨¡å‹åŒ…å«{len(model.experts)}ä¸ªä¸“å®¶ç½‘ç»œ")
            
            # æµ‹è¯•æ¯ä¸ªä¸“å®¶
            for i, expert in enumerate(model.experts):
                try:
                    test_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
                    expert_output = expert(test_input)
                    print(f"âœ“ ä¸“å®¶{i}æµ‹è¯•é€šè¿‡")
                except Exception as e:
                    print(f"âš  ä¸“å®¶{i}æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_model_forward_pass(self, model, config):
        """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
        batch_sizes = [self.small_batch_size, self.medium_batch_size]
        
        for batch_size in batch_sizes:
            test_input = torch.randn(batch_size, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
            
            # æ¨ç†æ¨¡å¼
            model.eval()
            with torch.no_grad():
                output = model(test_input, return_aux_info=True)
            
            # éªŒè¯è¾“å‡º
            if 'predictions' not in output:
                raise ValueError("æ¨¡å‹è¾“å‡ºç¼ºå°‘predictions")
            
            predictions = output['predictions']
            expected_shape = (batch_size, config['data']['pred_len'], config['model']['output_dim'])
            if predictions.shape != expected_shape:
                raise ValueError(f"é¢„æµ‹å½¢çŠ¶é”™è¯¯: {predictions.shape} vs {expected_shape}")
            
            # éªŒè¯è¾…åŠ©ä¿¡æ¯
            aux_info = output['aux_info']
            required_aux_keys = ['expert_weights', 'expert_embeddings', 'temperature']
            for key in required_aux_keys:
                if key not in aux_info:
                    raise ValueError(f"è¾…åŠ©ä¿¡æ¯ç¼ºå°‘{key}")
            
            print(f"âœ“ æ‰¹æ¬¡{batch_size}å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡")
    
    def _test_model_temperature_scheduling(self, model, config):
        """æµ‹è¯•æ¨¡å‹æ¸©åº¦è°ƒåº¦"""
        if hasattr(model, 'temperature'):
            initial_temp = model.temperature.item()
            print(f"âœ“ åˆå§‹æ¸©åº¦: {initial_temp}")
            
            # æµ‹è¯•æ¸©åº¦æ›´æ–°
            if hasattr(model, 'update_temperature_schedule'):
                try:
                    expert_entropy = torch.log(torch.tensor(float(model.num_experts))) * 0.5
                    model.update_temperature_schedule(epoch=5, expert_entropy=expert_entropy)
                    new_temp = model.temperature.item()
                    print(f"âœ“ æ¸©åº¦è°ƒåº¦æµ‹è¯•é€šè¿‡ï¼Œæ–°æ¸©åº¦: {new_temp}")
                except Exception as e:
                    print(f"âš  æ¸©åº¦è°ƒåº¦æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_model_expert_routing(self, model, config):
        """æµ‹è¯•æ¨¡å‹ä¸“å®¶è·¯ç”±"""
        batch_size = self.small_batch_size
        test_input = torch.randn(batch_size, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
        
        model.eval()
        with torch.no_grad():
            output = model(test_input, return_aux_info=True)
            expert_weights = output['aux_info']['expert_weights']
            
            # éªŒè¯ä¸“å®¶æƒé‡
            if expert_weights.shape != (batch_size, model.num_experts):
                raise ValueError(f"ä¸“å®¶æƒé‡å½¢çŠ¶é”™è¯¯: {expert_weights.shape}")
            
            # éªŒè¯æƒé‡å’Œä¸º1
            weight_sums = expert_weights.sum(dim=-1)
            if not torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5):
                raise ValueError("ä¸“å®¶æƒé‡å’Œä¸ä¸º1")
            
            # è®¡ç®—è·¯ç”±å¤šæ ·æ€§
            expert_usage = expert_weights.mean(dim=0)
            usage_entropy = -torch.sum(expert_usage * torch.log(expert_usage + 1e-8))
            print(f"âœ“ ä¸“å®¶è·¯ç”±æµ‹è¯•é€šè¿‡ï¼Œä½¿ç”¨ç†µ: {usage_entropy:.3f}")
    
    def _test_model_triplet_mining(self, model, config):
        """æµ‹è¯•æ¨¡å‹ä¸‰å…ƒç»„æŒ–æ˜"""
        batch_size = 6  # éœ€è¦è¶³å¤Ÿçš„æ ·æœ¬æ„æˆä¸‰å…ƒç»„
        
        test_input = torch.randn(batch_size, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
        ground_truth = torch.randn(batch_size, config['data']['pred_len'], config['model']['output_dim']).to(self.device)
        
        model.eval()
        with torch.no_grad():
            output = model(test_input, return_aux_info=True)
            
            # æ¨¡æ‹Ÿä¸‰å…ƒç»„æŒ–æ˜
            if hasattr(model, 'mine_triplets_based_on_prediction_performance'):
                try:
                    expert_weights = output['aux_info']['expert_weights']
                    expert_predictions = torch.stack([expert(test_input) for expert in model.experts], dim=1)
                    
                    triplets = model.mine_triplets_based_on_prediction_performance(
                        test_input, expert_weights, expert_predictions, ground_truth
                    )
                    
                    print(f"âœ“ ä¸‰å…ƒç»„æŒ–æ˜æµ‹è¯•é€šè¿‡ï¼Œå‘ç°{len(triplets)}ä¸ªä¸‰å…ƒç»„")
                except Exception as e:
                    print(f"âš  ä¸‰å…ƒç»„æŒ–æ˜æµ‹è¯•å¤±è´¥: {e}")
    
    def _test_model_numerical_stability(self, model, config):
        """æµ‹è¯•æ¨¡å‹æ•°å€¼ç¨³å®šæ€§"""
        # æç«¯è¾“å…¥æµ‹è¯•
        batch_size = self.small_batch_size
        seq_len = config['data']['seq_len']
        input_dim = config['model']['input_dim']
        
        extreme_cases = [
            torch.zeros(batch_size, seq_len, input_dim).to(self.device),
            torch.ones(batch_size, seq_len, input_dim).to(self.device) * 1000,
            torch.randn(batch_size, seq_len, input_dim).to(self.device) * 0.001
        ]
        
        model.eval()
        for i, extreme_input in enumerate(extreme_cases):
            try:
                with torch.no_grad():
                    output = model(extreme_input)
                
                predictions = output['predictions']
                if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                    raise ValueError(f"æç«¯è¾“å…¥{i}å¯¼è‡´é¢„æµ‹å¼‚å¸¸")
                
                print(f"âœ“ æ¨¡å‹æç«¯è¾“å…¥{i}æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âš  æ¨¡å‹æç«¯è¾“å…¥{i}æµ‹è¯•å¤±è´¥: {e}")
    
    def test_training_process_detailed(self, config: Dict[str, Any]) -> bool:
        """è¯¦ç»†æµ‹è¯•è®­ç»ƒè¿‡ç¨‹"""
        from train import M2MOEPTrainer
        
        try:
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = M2MOEPTrainer(config)
            
            # æµ‹è¯•è®­ç»ƒå™¨ç»„ä»¶
            self._test_trainer_components(trainer, config)
            
            # æµ‹è¯•å•æ­¥è®­ç»ƒ
            self._test_single_training_step(trainer, config)
            
            # æµ‹è¯•å¤šæ­¥è®­ç»ƒ
            self._test_multiple_training_steps(trainer, config)
            
            # æµ‹è¯•éªŒè¯æ­¥éª¤
            self._test_validation_step(trainer, config)
            
            # æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
            self._test_model_save_load(trainer, config)
            
            self.log_test("è®­ç»ƒè¿‡ç¨‹è¯¦ç»†æµ‹è¯•", True, "è®­ç»ƒè¿‡ç¨‹è¯¦ç»†æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.log_test("è®­ç»ƒè¿‡ç¨‹è¯¦ç»†æµ‹è¯•", False, f"è®­ç»ƒè¿‡ç¨‹è¯¦ç»†æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            return False
    
    def _test_trainer_components(self, trainer, config):
        """æµ‹è¯•è®­ç»ƒå™¨ç»„ä»¶"""
        # æ£€æŸ¥è®­ç»ƒå™¨å±æ€§
        required_attrs = ['model', 'data_module', 'criterion', 'optimizer', 'scheduler', 'device']
        for attr in required_attrs:
            if not hasattr(trainer, attr):
                raise ValueError(f"è®­ç»ƒå™¨ç¼ºå°‘å±æ€§: {attr}")
        
        print(f"âœ“ è®­ç»ƒå™¨ç»„ä»¶å®Œæ•´ï¼Œè®¾å¤‡: {trainer.device}")
        
        # æ£€æŸ¥ä¼˜åŒ–å™¨
        if hasattr(trainer, 'optimizer'):
            print(f"âœ“ ä¼˜åŒ–å™¨ç±»å‹: {type(trainer.optimizer).__name__}")
            
            # æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°
            param_groups = trainer.optimizer.param_groups
            print(f"âœ“ ä¼˜åŒ–å™¨å‚æ•°ç»„: {len(param_groups)}")
            
            for i, group in enumerate(param_groups):
                print(f"  ç»„{i}: lr={group['lr']}, weight_decay={group.get('weight_decay', 0)}")
        
        # æ£€æŸ¥å­¦ä¹ ç‡è°ƒåº¦å™¨
        if hasattr(trainer, 'scheduler'):
            print(f"âœ“ å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹: {type(trainer.scheduler).__name__}")
    
    def _test_single_training_step(self, trainer, config):
        """æµ‹è¯•å•æ­¥è®­ç»ƒ"""
        # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        train_loader = trainer.data_module.get_train_loader()
        batch_x, batch_y = next(iter(train_loader))
        batch_x = batch_x.to(trainer.device)
        batch_y = batch_y.to(trainer.device)
        
        # è®°å½•åˆå§‹æŸå¤±
        trainer.model.eval()
        with torch.no_grad():
            initial_output = trainer.model(batch_x, ground_truth=batch_y, return_aux_info=True)
            initial_loss = trainer.criterion(
                predictions=initial_output['predictions'],
                targets=batch_y,
                expert_weights=initial_output['aux_info'].get('expert_weights'),
                expert_embeddings=initial_output['aux_info'].get('expert_embeddings')
            )['total']
        
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        trainer.model.train()
        trainer.optimizer.zero_grad()
        
        output = trainer.model(batch_x, ground_truth=batch_y, return_aux_info=True)
        losses = trainer.criterion(
            predictions=output['predictions'],
            targets=batch_y,
            expert_weights=output['aux_info'].get('expert_weights'),
            expert_embeddings=output['aux_info'].get('expert_embeddings')
        )
        
        total_loss = losses['total']
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        grad_norm = torch.nn.utils.clip_grad_norm_(trainer.model.parameters(), 1.0)
        
        trainer.optimizer.step()
        
        # è®°å½•è®­ç»ƒåæŸå¤±
        trainer.model.eval()
        with torch.no_grad():
            final_output = trainer.model(batch_x, ground_truth=batch_y, return_aux_info=True)
            final_loss = trainer.criterion(
                predictions=final_output['predictions'],
                targets=batch_y,
                expert_weights=final_output['aux_info'].get('expert_weights'),
                expert_embeddings=final_output['aux_info'].get('expert_embeddings')
            )['total']
        
        print(f"âœ“ å•æ­¥è®­ç»ƒ: åˆå§‹æŸå¤±={initial_loss.item():.4f}, æœ€ç»ˆæŸå¤±={final_loss.item():.4f}, æ¢¯åº¦èŒƒæ•°={grad_norm.item():.4f}")
    
    def _test_multiple_training_steps(self, trainer, config):
        """æµ‹è¯•å¤šæ­¥è®­ç»ƒ"""
        train_loader = trainer.data_module.get_train_loader()
        
        losses = []
        for i, (batch_x, batch_y) in enumerate(train_loader):
            if i >= 3:  # åªæµ‹è¯•å‰3æ­¥
                break
            
            batch_x = batch_x.to(trainer.device)
            batch_y = batch_y.to(trainer.device)
            
            trainer.model.train()
            trainer.optimizer.zero_grad()
            
            output = trainer.model(batch_x, ground_truth=batch_y, return_aux_info=True)
            loss_dict = trainer.criterion(
                predictions=output['predictions'],
                targets=batch_y,
                expert_weights=output['aux_info'].get('expert_weights'),
                expert_embeddings=output['aux_info'].get('expert_embeddings')
            )
            
            total_loss = loss_dict['total']
            total_loss.backward()
            
            grad_norm = torch.nn.utils.clip_grad_norm_(trainer.model.parameters(), 1.0)
            trainer.optimizer.step()
            
            losses.append(total_loss.item())
            print(f"  æ­¥éª¤{i}: æŸå¤±={total_loss.item():.4f}, æ¢¯åº¦èŒƒæ•°={grad_norm.item():.4f}")
        
        print(f"âœ“ å¤šæ­¥è®­ç»ƒå®Œæˆï¼Œå¹³å‡æŸå¤±: {np.mean(losses):.4f}")
    
    def _test_validation_step(self, trainer, config):
        """æµ‹è¯•éªŒè¯æ­¥éª¤"""
        val_loader = trainer.data_module.get_val_loader()
        
        trainer.model.eval()
        val_losses = []
        
        with torch.no_grad():
            for i, (batch_x, batch_y) in enumerate(val_loader):
                if i >= 3:  # åªæµ‹è¯•å‰3æ­¥
                    break
                
                batch_x = batch_x.to(trainer.device)
                batch_y = batch_y.to(trainer.device)
                
                output = trainer.model(batch_x, return_aux_info=True)
                loss_dict = trainer.criterion(
                    predictions=output['predictions'],
                    targets=batch_y,
                    expert_weights=output['aux_info'].get('expert_weights'),
                    expert_embeddings=output['aux_info'].get('expert_embeddings')
                )
                
                val_losses.append(loss_dict['total'].item())
        
        avg_val_loss = np.mean(val_losses)
        print(f"âœ“ éªŒè¯æ­¥éª¤å®Œæˆï¼Œå¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")
    
    def _test_model_save_load(self, trainer, config):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        # ä¿å­˜æ¨¡å‹
        save_path = "test_model_checkpoint.pth"
        
        try:
            # ä¿å­˜æ¨¡å‹çŠ¶æ€
            torch.save({
                'model_state_dict': trainer.model.state_dict(),
                'optimizer_state_dict': trainer.optimizer.state_dict(),
                'config': config
            }, save_path)
            
            # åŠ è½½æ¨¡å‹
            checkpoint = torch.load(save_path, map_location=trainer.device)
            
            # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
            from models.m2_moep import M2_MOEP
            new_model = M2_MOEP(checkpoint['config']).to(trainer.device)
            new_model.load_state_dict(checkpoint['model_state_dict'])
            
            # æµ‹è¯•åŠ è½½åçš„æ¨¡å‹
            test_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(trainer.device)
            
            new_model.eval()
            with torch.no_grad():
                output = new_model(test_input)
            
            print("âœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡")
            
        except Exception as e:
            print(f"âš  æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(save_path):
                os.remove(save_path)
    
    def run_ultra_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œè¶…å…¨é¢æµ‹è¯•"""
        print("ğŸ”¬ å¼€å§‹è¶…å…¨é¢æ·±åº¦æµ‹è¯•...")
        
        # é˜¶æ®µ1: åŸºç¡€æµ‹è¯•
        print("\n" + "="*50 + " é˜¶æ®µ1: åŸºç¡€é…ç½®æµ‹è¯• " + "="*50)
        config = self.test_config_generation_detailed()
        if not config:
            print("âŒ åŸºç¡€é…ç½®æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return self._generate_test_report()
        
        # é˜¶æ®µ2: æ•°æ®æµ‹è¯•
        print("\n" + "="*50 + " é˜¶æ®µ2: æ•°æ®åŠ è½½æµ‹è¯• " + "="*50)
        self.test_data_loading_detailed(config)
        
        # é˜¶æ®µ3: ä¸“å®¶ç½‘ç»œæµ‹è¯•
        print("\n" + "="*50 + " é˜¶æ®µ3: ä¸“å®¶ç½‘ç»œæµ‹è¯• " + "="*50)
        self.test_expert_network_detailed(config)
        
        # é˜¶æ®µ4: å®Œæ•´æ¨¡å‹æµ‹è¯•
        print("\n" + "="*50 + " é˜¶æ®µ4: å®Œæ•´æ¨¡å‹æµ‹è¯• " + "="*50)
        self.test_full_model_detailed(config)
        
        # é˜¶æ®µ5: è®­ç»ƒè¿‡ç¨‹æµ‹è¯•
        print("\n" + "="*50 + " é˜¶æ®µ5: è®­ç»ƒè¿‡ç¨‹æµ‹è¯• " + "="*50)
        self.test_training_process_detailed(config)
        
        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_test_report()
    
    def _generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*100)
        print("ğŸ“Š è¶…å…¨é¢æµ‹è¯•æŠ¥å‘Š")
        print("="*100)
        
        passed_tests = sum(1 for result in self.test_results.values() if result['success'])
        total_tests = len(self.test_results)
        
        print(f"ğŸ“ˆ æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {passed_tests/total_tests*100:.1f}%")
        
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå®Œå…¨å°±ç»ªã€‚")
        else:
            print("âš ï¸  å­˜åœ¨å¤±è´¥çš„æµ‹è¯•:")
            for error in self.critical_errors:
                print(f"   - {error}")
        
        # æŒ‰ç±»åˆ«æ±‡æ€»
        categories = {}
        for test_name, result in self.test_results.items():
            category = test_name.split('-')[0] if '-' in test_name else test_name
            if category not in categories:
                categories[category] = {'pass': 0, 'fail': 0}
            
            if result['success']:
                categories[category]['pass'] += 1
            else:
                categories[category]['fail'] += 1
        
        print("\nğŸ“‹ åˆ†ç±»æµ‹è¯•ç»“æœ:")
        for category, stats in categories.items():
            total = stats['pass'] + stats['fail']
            rate = stats['pass'] / total * 100 if total > 0 else 0
            print(f"   {category}: {stats['pass']}/{total} ({rate:.1f}%)")
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'all_passed': passed_tests == total_tests,
            'test_results': self.test_results,
            'critical_errors': self.critical_errors,
            'categories': categories
        }

def main():
    """ä¸»å‡½æ•°"""
    test_suite = UltraComprehensiveTestSuite()
    report = test_suite.run_ultra_comprehensive_test()
    
    # è¿”å›çŠ¶æ€ç 
    return 0 if report['all_passed'] else 1

if __name__ == "__main__":
    sys.exit(main()) 