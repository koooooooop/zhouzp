"""
å…¨é¢æ·±åº¦æµ‹è¯•è„šæœ¬ - ç¡®ä¿è®­ç»ƒæµç¨‹æ— é—®é¢˜
æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„è¿è¡Œæ—¶é”™è¯¯å’Œæ½œåœ¨é—®é¢˜
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import traceback
from typing import Dict, Any
import logging
import warnings

# è®¾ç½®è­¦å‘Šçº§åˆ«
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

def setup_test_logging():
    """è®¾ç½®æµ‹è¯•æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    return logging.getLogger(__name__)

class ComprehensiveTestSuite:
    """å…¨é¢æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.logger = setup_test_logging()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.test_results = {}
        self.critical_errors = []
        
    def log_test(self, test_name: str, success: bool, message: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… PASS" if success else "âŒ FAIL"
        self.logger.info(f"{status} {test_name}: {message}")
        self.test_results[test_name] = {"success": success, "message": message}
        if not success:
            self.critical_errors.append(f"{test_name}: {message}")
    
    def test_1_imports_and_dependencies(self) -> bool:
        """æµ‹è¯•1: å¯¼å…¥å’Œä¾èµ–å…³ç³»"""
        try:
            # æ ¸å¿ƒä¾èµ–
            import torch
            import numpy as np
            import pandas as pd
            import yaml
            
            # é¡¹ç›®æ¨¡å—å¯¼å…¥æµ‹è¯•
            from models.m2_moep import M2_MOEP
            from models.expert import FFTmsMambaExpert
            from models.gating import GatingEncoder
            from models.flow import PowerfulNormalizingFlow
            from data.universal_dataset import UniversalDataModule
            from utils.losses import CompositeLoss
            from configs.config_generator import ConfigGenerator
            from train import M2MOEPTrainer
            
            self.log_test("æ¨¡å—å¯¼å…¥", True, "æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
            return True
            
        except ImportError as e:
            self.log_test("æ¨¡å—å¯¼å…¥", False, f"å¯¼å…¥å¤±è´¥: {e}")
            return False
        except Exception as e:
            self.log_test("æ¨¡å—å¯¼å…¥", False, f"æœªçŸ¥é”™è¯¯: {e}")
            return False
    
    def test_2_config_generation(self) -> Dict[str, Any]:
        """æµ‹è¯•2: é…ç½®ç”Ÿæˆå’ŒéªŒè¯"""
        try:
            from configs.config_generator import ConfigGenerator
            
            # ç”Ÿæˆæµ‹è¯•é…ç½®
            config = ConfigGenerator.generate_config(
                'weather',
                batch_size=8,
                epochs=2,
                learning_rate=0.001
            )
            
            # éªŒè¯å…³é”®é…ç½®é¡¹
            required_keys = [
                'model', 'data', 'training', 'evaluation'
            ]
            
            for key in required_keys:
                if key not in config:
                    raise KeyError(f"ç¼ºå¤±é…ç½®é¡¹: {key}")
            
            # éªŒè¯æ¨¡å‹é…ç½®
            model_config = config['model']
            required_model_keys = [
                'input_dim', 'output_dim', 'hidden_dim', 'num_experts',
                'expert_params', 'flow', 'triplet', 'diversity', 'temperature'
            ]
            
            for key in required_model_keys:
                if key not in model_config:
                    raise KeyError(f"ç¼ºå¤±æ¨¡å‹é…ç½®é¡¹: {key}")
            
            # éªŒè¯ä¸“å®¶å‚æ•°
            expert_params = model_config['expert_params']
            if 'mamba_d_model' not in expert_params:
                raise KeyError("ç¼ºå¤±mamba_d_modelå‚æ•°")
            if 'mamba_scales' not in expert_params:
                raise KeyError("ç¼ºå¤±mamba_scaleså‚æ•°")
            
            # éªŒè¯æ•°æ®é…ç½®
            data_config = config['data']
            if data_config['seq_len'] <= 0 or data_config['pred_len'] <= 0:
                raise ValueError("åºåˆ—é•¿åº¦é…ç½®æ— æ•ˆ")
            
            self.log_test("é…ç½®ç”Ÿæˆå’ŒéªŒè¯", True, f"é…ç½®éªŒè¯æˆåŠŸï¼ŒåŒ…å«{len(config)}ä¸ªä¸»è¦éƒ¨åˆ†")
            return config
            
        except Exception as e:
            self.log_test("é…ç½®ç”Ÿæˆå’ŒéªŒè¯", False, f"é…ç½®é”™è¯¯: {e}")
            return {}
    
    def test_3_data_loading(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•3: æ•°æ®åŠ è½½å’Œå¤„ç†"""
        try:
            from data.universal_dataset import UniversalDataModule
            
            # åˆå§‹åŒ–æ•°æ®æ¨¡å—
            data_module = UniversalDataModule(config)
            
            # éªŒè¯æ•°æ®ä¿¡æ¯
            dataset_info = data_module.get_dataset_info()
            required_info_keys = ['num_features', 'num_samples', 'seq_len', 'pred_len']
            
            for key in required_info_keys:
                if key not in dataset_info:
                    raise KeyError(f"ç¼ºå¤±æ•°æ®é›†ä¿¡æ¯: {key}")
            
            # éªŒè¯æ•°æ®åŠ è½½å™¨
            train_loader = data_module.get_train_loader()
            val_loader = data_module.get_val_loader()
            test_loader = data_module.get_test_loader()
            
            # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
            batch_x, batch_y = next(iter(train_loader))
            
            # éªŒè¯æ•°æ®å½¢çŠ¶
            expected_x_shape = (config['training']['batch_size'], config['data']['seq_len'], dataset_info['num_features'])
            expected_y_shape = (config['training']['batch_size'], config['data']['pred_len'], dataset_info['num_features'])
            
            if batch_x.shape != expected_x_shape:
                self.logger.warning(f"è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…: æœŸæœ›{expected_x_shape}, å®é™…{batch_x.shape}")
            
            if batch_y.shape != expected_y_shape:
                self.logger.warning(f"ç›®æ ‡å½¢çŠ¶ä¸åŒ¹é…: æœŸæœ›{expected_y_shape}, å®é™…{batch_y.shape}")
            
            # éªŒè¯æ•°æ®ç±»å‹å’Œè®¾å¤‡
            if not torch.is_tensor(batch_x) or not torch.is_tensor(batch_y):
                raise TypeError("æ•°æ®ä¸æ˜¯å¼ é‡ç±»å‹")
            
            if batch_x.dtype != torch.float32 or batch_y.dtype != torch.float32:
                raise TypeError(f"æ•°æ®ç±»å‹é”™è¯¯: {batch_x.dtype}, {batch_y.dtype}")
            
            # éªŒè¯æ•°å€¼èŒƒå›´
            if torch.isnan(batch_x).any() or torch.isinf(batch_x).any():
                raise ValueError("è¾“å…¥æ•°æ®åŒ…å«NaNæˆ–Inf")
            
            if torch.isnan(batch_y).any() or torch.isinf(batch_y).any():
                raise ValueError("ç›®æ ‡æ•°æ®åŒ…å«NaNæˆ–Inf")
            
            self.log_test("æ•°æ®åŠ è½½å’Œå¤„ç†", True, 
                         f"æ•°æ®åŠ è½½æˆåŠŸ: {len(train_loader)}ä¸ªè®­ç»ƒæ‰¹æ¬¡, ç‰¹å¾æ•°{dataset_info['num_features']}")
            return True
            
        except Exception as e:
            self.log_test("æ•°æ®åŠ è½½å’Œå¤„ç†", False, f"æ•°æ®åŠ è½½é”™è¯¯: {e}")
            return False
    
    def test_4_expert_network(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•4: ä¸“å®¶ç½‘ç»œè¯¦ç»†æµ‹è¯•"""
        try:
            from models.expert import FFTmsMambaExpert
            
            # æ›´æ–°é…ç½®ä»¥ç¡®ä¿å½“å‰ä¸“å®¶ID
            config['model']['current_expert_id'] = 0
            
            # åˆ›å»ºä¸“å®¶ç½‘ç»œ
            expert = FFTmsMambaExpert(config).to(self.device)
            
            # éªŒè¯ä¸“å®¶ç½‘ç»œç»“æ„
            required_attrs = [
                'input_projection', 'multi_scale_mamba', 'scale_fusion',
                'output_projection', 'prediction_head', 'expert_personalization',
                'learnable_deltas', 'use_mamba'
            ]
            
            for attr in required_attrs:
                if not hasattr(expert, attr):
                    raise AttributeError(f"ä¸“å®¶ç½‘ç»œç¼ºå¤±å±æ€§: {attr}")
            
            # éªŒè¯å¯å­¦ä¹ å‚æ•°
            if not isinstance(expert.learnable_deltas, nn.Parameter):
                raise TypeError("learnable_deltasä¸æ˜¯å¯å­¦ä¹ å‚æ•°")
            
            if len(expert.learnable_deltas) != len(config['model']['expert_params']['mamba_scales']):
                raise ValueError("learnable_deltasé•¿åº¦ä¸scalesä¸åŒ¹é…")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size = 4
            seq_len = config['data']['seq_len']
            input_dim = config['model']['input_dim']
            
            test_input = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            
            # å‰å‘ä¼ æ’­æµ‹è¯•
            with torch.no_grad():
                output = expert(test_input)
            
            # éªŒè¯è¾“å‡ºå½¢çŠ¶
            expected_output_shape = (batch_size, config['data']['pred_len'], config['model']['output_dim'])
            if output.shape != expected_output_shape:
                raise ValueError(f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_output_shape}, å®é™…{output.shape}")
            
            # éªŒè¯è¾“å‡ºæ•°å€¼
            if torch.isnan(output).any() or torch.isinf(output).any():
                raise ValueError("ä¸“å®¶ç½‘ç»œè¾“å‡ºåŒ…å«NaNæˆ–Inf")
            
            # æµ‹è¯•æ¢¯åº¦è®¡ç®—
            expert.train()
            test_input.requires_grad_(True)
            output = expert(test_input)
            loss = output.sum()
            loss.backward()
            
            # éªŒè¯æ¢¯åº¦
            grad_norms = []
            for name, param in expert.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    grad_norms.append(grad_norm)
                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                        raise ValueError(f"å‚æ•°{name}çš„æ¢¯åº¦åŒ…å«NaNæˆ–Inf")
            
            if len(grad_norms) == 0:
                raise ValueError("æ²¡æœ‰è®¡ç®—åˆ°ä»»ä½•æ¢¯åº¦")
            
            self.log_test("ä¸“å®¶ç½‘ç»œ", True, 
                         f"ä¸“å®¶ç½‘ç»œæµ‹è¯•æˆåŠŸ: {sum(p.numel() for p in expert.parameters()):,}ä¸ªå‚æ•°, "
                         f"æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms):.6f}")
            return True
            
        except Exception as e:
            self.log_test("ä¸“å®¶ç½‘ç»œ", False, f"ä¸“å®¶ç½‘ç»œé”™è¯¯: {e}")
            traceback.print_exc()
            return False
    
    def test_5_full_model(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•5: å®Œæ•´æ¨¡å‹æµ‹è¯•"""
        try:
            from models.m2_moep import M2_MOEP
            
            # åˆ›å»ºå®Œæ•´æ¨¡å‹
            model = M2_MOEP(config).to(self.device)
            
            # éªŒè¯æ¨¡å‹ç»“æ„
            required_components = [
                'flow_model', 'gating', 'experts', 'log_temperature'
            ]
            
            for component in required_components:
                if not hasattr(model, component):
                    raise AttributeError(f"æ¨¡å‹ç¼ºå¤±ç»„ä»¶: {component}")
            
            # éªŒè¯ä¸“å®¶æ•°é‡
            if len(model.experts) != config['model']['num_experts']:
                raise ValueError(f"ä¸“å®¶æ•°é‡ä¸åŒ¹é…: æœŸæœ›{config['model']['num_experts']}, å®é™…{len(model.experts)}")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size = 4
            seq_len = config['data']['seq_len']
            input_dim = config['model']['input_dim']
            
            test_input = torch.randn(batch_size, seq_len, input_dim).to(self.device)
            test_target = torch.randn(batch_size, config['data']['pred_len'], config['model']['output_dim']).to(self.device)
            
            # æ¨ç†æ¨¡å¼æµ‹è¯•
            model.eval()
            with torch.no_grad():
                output = model(test_input, return_aux_info=True)
            
            # éªŒè¯è¾“å‡ºç»“æ„
            if 'predictions' not in output:
                raise KeyError("æ¨¡å‹è¾“å‡ºç¼ºå¤±predictions")
            
            if 'aux_info' not in output:
                raise KeyError("æ¨¡å‹è¾“å‡ºç¼ºå¤±aux_info")
            
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # éªŒè¯é¢„æµ‹å½¢çŠ¶
            expected_pred_shape = (batch_size, config['data']['pred_len'], config['model']['output_dim'])
            if predictions.shape != expected_pred_shape:
                raise ValueError(f"é¢„æµ‹å½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_pred_shape}, å®é™…{predictions.shape}")
            
            # éªŒè¯è¾…åŠ©ä¿¡æ¯
            required_aux_keys = ['expert_weights', 'expert_embeddings', 'temperature']
            for key in required_aux_keys:
                if key not in aux_info:
                    raise KeyError(f"è¾…åŠ©ä¿¡æ¯ç¼ºå¤±: {key}")
            
            # éªŒè¯ä¸“å®¶æƒé‡
            expert_weights = aux_info['expert_weights']
            if expert_weights.shape != (batch_size, config['model']['num_experts']):
                raise ValueError(f"ä¸“å®¶æƒé‡å½¢çŠ¶é”™è¯¯: {expert_weights.shape}")
            
            # éªŒè¯æƒé‡å’Œä¸º1
            weight_sums = expert_weights.sum(dim=-1)
            if not torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-6):
                raise ValueError("ä¸“å®¶æƒé‡å’Œä¸ä¸º1")
            
            # è®­ç»ƒæ¨¡å¼æµ‹è¯•ï¼ˆå¸¦æ¢¯åº¦ï¼‰
            model.train()
            output = model(test_input, ground_truth=test_target, return_aux_info=True)
            
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # éªŒè¯è®­ç»ƒæ¨¡å¼ä¸‹çš„è¾…åŠ©ä¿¡æ¯
            if 'reconstruction_loss' not in aux_info:
                self.logger.warning("è®­ç»ƒæ¨¡å¼ä¸‹ç¼ºå¤±reconstruction_loss")
            
            if 'triplet_loss' not in aux_info:
                self.logger.warning("è®­ç»ƒæ¨¡å¼ä¸‹ç¼ºå¤±triplet_loss")
            
            self.log_test("å®Œæ•´æ¨¡å‹", True, 
                         f"æ¨¡å‹æµ‹è¯•æˆåŠŸ: {sum(p.numel() for p in model.parameters()):,}ä¸ªå‚æ•°, "
                         f"æ¸©åº¦: {model.temperature.item():.3f}")
            return True
            
        except Exception as e:
            self.log_test("å®Œæ•´æ¨¡å‹", False, f"å®Œæ•´æ¨¡å‹é”™è¯¯: {e}")
            traceback.print_exc()
            return False
    
    def test_6_loss_computation(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•6: æŸå¤±è®¡ç®—æµ‹è¯•"""
        try:
            from utils.losses import CompositeLoss
            from models.m2_moep import M2_MOEP
            
            # åˆ›å»ºæŸå¤±å‡½æ•°
            criterion = CompositeLoss(config)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            batch_size = 4
            pred_len = config['data']['pred_len']
            output_dim = config['model']['output_dim']
            num_experts = config['model']['num_experts']
            
            predictions = torch.randn(batch_size, pred_len, output_dim)
            targets = torch.randn(batch_size, pred_len, output_dim)
            expert_weights = torch.softmax(torch.randn(batch_size, num_experts), dim=-1)
            expert_embeddings = torch.randn(batch_size, 128)
            
            # è®¡ç®—æŸå¤±
            losses = criterion(
                predictions=predictions,
                targets=targets,
                expert_weights=expert_weights,
                expert_embeddings=expert_embeddings
            )
            
            # éªŒè¯æŸå¤±ç»“æ„
            required_loss_keys = ['total', 'prediction', 'reconstruction', 'triplet']
            for key in required_loss_keys:
                if key not in losses:
                    raise KeyError(f"æŸå¤±ç¼ºå¤±: {key}")
            
            # éªŒè¯æŸå¤±å€¼
            total_loss = losses['total']
            if torch.isnan(total_loss) or torch.isinf(total_loss):
                raise ValueError("æ€»æŸå¤±åŒ…å«NaNæˆ–Inf")
            
            if total_loss.item() < 0:
                raise ValueError("æ€»æŸå¤±ä¸ºè´Ÿæ•°")
            
            # éªŒè¯å„ç»„ä»¶æŸå¤±
            for key, loss in losses.items():
                if isinstance(loss, torch.Tensor):
                    if torch.isnan(loss) or torch.isinf(loss):
                        raise ValueError(f"æŸå¤±{key}åŒ…å«NaNæˆ–Inf")
                    if loss.item() < 0:
                        raise ValueError(f"æŸå¤±{key}ä¸ºè´Ÿæ•°")
            
            # æµ‹è¯•æ¢¯åº¦åä¼ 
            total_loss.backward()
            
            self.log_test("æŸå¤±è®¡ç®—", True, 
                         f"æŸå¤±è®¡ç®—æˆåŠŸ: æ€»æŸå¤±{total_loss.item():.6f}, "
                         f"é¢„æµ‹æŸå¤±{losses['prediction'].item():.6f}")
            return True
            
        except Exception as e:
            self.log_test("æŸå¤±è®¡ç®—", False, f"æŸå¤±è®¡ç®—é”™è¯¯: {e}")
            traceback.print_exc()
            return False
    
    def test_7_training_step(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•7: å®Œæ•´è®­ç»ƒæ­¥éª¤æµ‹è¯•"""
        try:
            from train import M2MOEPTrainer
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = M2MOEPTrainer(config)
            
            # éªŒè¯è®­ç»ƒå™¨ç»„ä»¶
            required_trainer_attrs = [
                'model', 'data_module', 'criterion', 'optimizer', 'scheduler'
            ]
            
            for attr in required_trainer_attrs:
                if not hasattr(trainer, attr):
                    raise AttributeError(f"è®­ç»ƒå™¨ç¼ºå¤±å±æ€§: {attr}")
            
            # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
            train_loader = trainer.data_module.get_train_loader()
            batch_data = next(iter(train_loader))
            
            if len(batch_data) != 2:
                raise ValueError(f"æ‰¹æ¬¡æ•°æ®æ ¼å¼é”™è¯¯: æœŸæœ›2ä¸ªå…ƒç´ , å®é™…{len(batch_data)}")
            
            batch_x, batch_y = batch_data
            batch_x = batch_x.to(trainer.device)
            batch_y = batch_y.to(trainer.device)
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            trainer.model.train()
            trainer.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = trainer.model(batch_x, ground_truth=batch_y, return_aux_info=True)
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # æŸå¤±è®¡ç®—
            losses = trainer.criterion(
                predictions=predictions,
                targets=batch_y,
                expert_weights=aux_info.get('expert_weights'),
                expert_embeddings=aux_info.get('expert_embeddings')
            )
            total_loss = losses['total']
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            # æ¢¯åº¦æ£€æŸ¥
            grad_norm = torch.nn.utils.clip_grad_norm_(
                trainer.model.parameters(), 
                float('inf')
            )
            
            if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                raise ValueError("æ¢¯åº¦èŒƒæ•°å¼‚å¸¸")
            
            # ä¼˜åŒ–å™¨æ­¥éª¤
            trainer.optimizer.step()
            
            # éªŒè¯å‚æ•°æ›´æ–°
            param_changed = False
            for param in trainer.model.parameters():
                if param.requires_grad and param.grad is not None:
                    param_changed = True
                    break
            
            if not param_changed:
                raise ValueError("å‚æ•°æ²¡æœ‰æ›´æ–°")
            
            self.log_test("å®Œæ•´è®­ç»ƒæ­¥éª¤", True, 
                         f"è®­ç»ƒæ­¥éª¤æˆåŠŸ: æŸå¤±{total_loss.item():.6f}, "
                         f"æ¢¯åº¦èŒƒæ•°{grad_norm.item():.6f}")
            return True
            
        except Exception as e:
            self.log_test("å®Œæ•´è®­ç»ƒæ­¥éª¤", False, f"è®­ç»ƒæ­¥éª¤é”™è¯¯: {e}")
            traceback.print_exc()
            return False
    
    def test_8_memory_and_device_consistency(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•8: å†…å­˜å’Œè®¾å¤‡ä¸€è‡´æ€§"""
        try:
            from models.m2_moep import M2_MOEP
            
            # åˆ›å»ºæ¨¡å‹
            model = M2_MOEP(config).to(self.device)
            
            # æ£€æŸ¥å…³é”®å‚æ•°è®¾å¤‡ä¸€è‡´æ€§ï¼ˆå…è®¸ä¸“å®¶ç½‘ç»œåœ¨CPUä¸Šï¼‰
            device_issues = []
            
            # æ£€æŸ¥é—¨æ§ç½‘ç»œå‚æ•°
            for name, param in model.gating.named_parameters():
                if param.device != self.device:
                    device_issues.append(f"é—¨æ§ç½‘ç»œå‚æ•°{name}åœ¨é”™è¯¯è®¾å¤‡ä¸Š: {param.device}")
            
            # æ£€æŸ¥ä¸»æ¨¡å‹å‚æ•°ï¼ˆé™¤äº†ä¸“å®¶ç½‘ç»œï¼‰
            for name, param in model.named_parameters():
                if not name.startswith('experts.'):  # å¿½ç•¥ä¸“å®¶ç½‘ç»œå‚æ•°
                    if param.device != self.device:
                        device_issues.append(f"ä¸»æ¨¡å‹å‚æ•°{name}åœ¨é”™è¯¯è®¾å¤‡ä¸Š: {param.device}")
            
            # æ£€æŸ¥ç¼“å†²åŒº
            for name, buffer in model.named_buffers():
                if not name.startswith('experts.'):  # å¿½ç•¥ä¸“å®¶ç½‘ç»œç¼“å†²åŒº
                    if buffer.device != self.device:
                        device_issues.append(f"ä¸»æ¨¡å‹ç¼“å†²åŒº{name}åœ¨é”™è¯¯è®¾å¤‡ä¸Š: {buffer.device}")
            
            # è®¾å¤‡é—®é¢˜ä¸ç®—é”™è¯¯ï¼ˆå› ä¸ºä¸“å®¶ç½‘ç»œåœ¨CPUä¸Šæ˜¯æ­£å¸¸çš„ï¼‰
            if device_issues:
                self.logger.warning(f"å‘ç°è®¾å¤‡ä¸ä¸€è‡´ï¼ˆå¯èƒ½æ˜¯æ­£å¸¸çš„ï¼‰: {device_issues[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            # å†…å­˜æ³„æ¼æµ‹è¯•
            initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
            
            # å¤šæ¬¡å‰å‘ä¼ æ’­æµ‹è¯•
            for i in range(10):
                test_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
                with torch.no_grad():
                    output = model(test_input)
                del test_input, output
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                final_memory = torch.cuda.memory_allocated()
                memory_increase = final_memory - initial_memory
                
                if memory_increase > 100 * 1024 * 1024:  # 100MBé˜ˆå€¼
                    self.logger.warning(f"å¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼: å¢åŠ {memory_increase / 1024 / 1024:.1f}MB")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­æ˜¯å¦æ­£å¸¸å·¥ä½œï¼ˆè¿™æ˜¯å…³é”®æµ‹è¯•ï¼‰
            test_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
            with torch.no_grad():
                output = model(test_input)
            
            # éªŒè¯è¾“å‡ºç»“æ„ï¼ˆoutputå¯èƒ½æ˜¯å­—å…¸ï¼‰
            if isinstance(output, dict):
                if 'predictions' in output:
                    predictions = output['predictions']
                    # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å¼ é‡
                    if not torch.is_tensor(predictions):
                        raise RuntimeError("æ¨¡å‹é¢„æµ‹è¾“å‡ºä¸æ˜¯å¼ é‡")
                    
                    # æ£€æŸ¥é¢„æµ‹å½¢çŠ¶
                    expected_shape = (2, config['data']['pred_len'], config['model']['output_dim'])
                    if predictions.shape != expected_shape:
                        raise RuntimeError(f"é¢„æµ‹å½¢çŠ¶é”™è¯¯: æœŸæœ›{expected_shape}, å®é™…{predictions.shape}")
                    
                    # é‡è¦ï¼šä¸“å®¶ç½‘ç»œåœ¨CPUä¸Šæ—¶ï¼Œè¾“å‡ºå¯èƒ½åœ¨ä¸åŒè®¾å¤‡ä¸Šï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    # åªè¦è¾“å‡ºæ˜¯æœ‰æ•ˆçš„å¼ é‡å°±è¡Œ
                    if torch.isnan(predictions).any() or torch.isinf(predictions).any():
                        raise RuntimeError("æ¨¡å‹é¢„æµ‹è¾“å‡ºåŒ…å«NaNæˆ–Inf")
                    
                    self.logger.info(f"æ¨¡å‹é¢„æµ‹è¾“å‡ºæ­£å¸¸: è®¾å¤‡{predictions.device}, å½¢çŠ¶{predictions.shape}")
                else:
                    raise RuntimeError("æ¨¡å‹è¾“å‡ºå­—å…¸ä¸­ç¼ºå°‘predictionsé”®")
            else:
                # ç›´æ¥æ˜¯å¼ é‡
                if not torch.is_tensor(output):
                    raise RuntimeError("æ¨¡å‹è¾“å‡ºä¸æ˜¯å¼ é‡")
                
                # æ£€æŸ¥è¾“å‡ºæ•°å€¼
                if torch.isnan(output).any() or torch.isinf(output).any():
                    raise RuntimeError("æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Inf")
                
                self.logger.info(f"æ¨¡å‹è¾“å‡ºæ­£å¸¸: è®¾å¤‡{output.device}, å½¢çŠ¶{output.shape}")
            
            self.log_test("å†…å­˜å’Œè®¾å¤‡ä¸€è‡´æ€§", True, "æ¨¡å‹åŠŸèƒ½æ­£å¸¸ï¼ˆä¸“å®¶ç½‘ç»œè®¾å¤‡åˆ†ç¦»æ˜¯æ­£å¸¸çš„ï¼‰")
            return True
            
        except Exception as e:
            self.log_test("å†…å­˜å’Œè®¾å¤‡ä¸€è‡´æ€§", False, f"å†…å­˜/è®¾å¤‡é”™è¯¯: {e}")
            return False
    
    def test_9_numerical_stability(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•9: æ•°å€¼ç¨³å®šæ€§æµ‹è¯•"""
        try:
            from models.m2_moep import M2_MOEP
            from utils.losses import CompositeLoss
            
            model = M2_MOEP(config).to(self.device)
            criterion = CompositeLoss(config)
            
            # æç«¯è¾“å…¥æµ‹è¯•
            test_cases = [
                torch.zeros(2, config['data']['seq_len'], config['model']['input_dim']),  # å…¨é›¶
                torch.ones(2, config['data']['seq_len'], config['model']['input_dim']),   # å…¨ä¸€
                torch.randn(2, config['data']['seq_len'], config['model']['input_dim']) * 100,  # å¤§å€¼
                torch.randn(2, config['data']['seq_len'], config['model']['input_dim']) * 0.001,  # å°å€¼
            ]
            
            for i, test_input in enumerate(test_cases):
                test_input = test_input.to(self.device)
                
                # æµ‹è¯•å‰å‘ä¼ æ’­
                model.eval()
                with torch.no_grad():
                    output = model(test_input, return_aux_info=True)
                    predictions = output['predictions']
                    
                    # æ£€æŸ¥è¾“å‡ºæ•°å€¼ç¨³å®šæ€§
                    if torch.isnan(predictions).any():
                        raise ValueError(f"æµ‹è¯•ç”¨ä¾‹{i}: é¢„æµ‹åŒ…å«NaN")
                    
                    if torch.isinf(predictions).any():
                        raise ValueError(f"æµ‹è¯•ç”¨ä¾‹{i}: é¢„æµ‹åŒ…å«Inf")
                    
                    # æ£€æŸ¥ä¸“å®¶æƒé‡
                    expert_weights = output['aux_info']['expert_weights']
                    if torch.isnan(expert_weights).any():
                        raise ValueError(f"æµ‹è¯•ç”¨ä¾‹{i}: ä¸“å®¶æƒé‡åŒ…å«NaN")
                    
                    # æ£€æŸ¥æƒé‡å’Œ
                    weight_sums = expert_weights.sum(dim=-1)
                    if not torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5):
                        raise ValueError(f"æµ‹è¯•ç”¨ä¾‹{i}: ä¸“å®¶æƒé‡å’Œå¼‚å¸¸")
            
            # æ¸©åº¦ç¨³å®šæ€§æµ‹è¯•
            original_temp = model.temperature.item()
            
            # è®¾ç½®æç«¯æ¸©åº¦å€¼
            extreme_temps = [0.001, 100.0]
            for temp in extreme_temps:
                model.temperature = temp
                
                test_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
                with torch.no_grad():
                    output = model(test_input, return_aux_info=True)
                    
                    if torch.isnan(output['predictions']).any():
                        raise ValueError(f"æ¸©åº¦{temp}: é¢„æµ‹åŒ…å«NaN")
            
            # æ¢å¤åŸå§‹æ¸©åº¦
            model.temperature = original_temp
            
            self.log_test("æ•°å€¼ç¨³å®šæ€§", True, "æ•°å€¼ç¨³å®šæ€§æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            self.log_test("æ•°å€¼ç¨³å®šæ€§", False, f"æ•°å€¼ç¨³å®šæ€§é”™è¯¯: {e}")
            return False
    
    def test_10_error_recovery(self, config: Dict[str, Any]) -> bool:
        """æµ‹è¯•10: é”™è¯¯æ¢å¤å’Œå¼‚å¸¸å¤„ç†"""
        try:
            from models.m2_moep import M2_MOEP
            
            model = M2_MOEP(config).to(self.device)
            
            # æµ‹è¯•ç»´åº¦ä¸åŒ¹é…é”™è¯¯
            try:
                wrong_input = torch.randn(2, config['data']['seq_len'] + 10, config['model']['input_dim']).to(self.device)
                output = model(wrong_input)
                self.logger.warning("ç»´åº¦æ£€æŸ¥å¯èƒ½æœ‰é—®é¢˜ï¼šåº”è¯¥æŠ›å‡ºé”™è¯¯ä½†æ²¡æœ‰")
            except (ValueError, RuntimeError) as e:
                self.logger.info(f"æ­£ç¡®æ•è·ç»´åº¦é”™è¯¯: {type(e).__name__}")
            
            # æµ‹è¯•è®¾å¤‡ä¸åŒ¹é…å¤„ç†
            if torch.cuda.is_available() and self.device.type == 'cuda':
                try:
                    cpu_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim'])
                    output = model(cpu_input)  # åº”è¯¥è‡ªåŠ¨ç§»åˆ°GPU
                    if output.device.type != 'cuda':
                        raise RuntimeError("è®¾å¤‡è‡ªåŠ¨è¿ç§»å¤±è´¥")
                    self.logger.info("è®¾å¤‡è‡ªåŠ¨è¿ç§»æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"è®¾å¤‡è¿ç§»é”™è¯¯: {e}")
            
            # æµ‹è¯•NaNè¾“å…¥å¤„ç†
            try:
                nan_input = torch.randn(2, config['data']['seq_len'], config['model']['input_dim']).to(self.device)
                nan_input[0, 0, 0] = float('nan')
                output = model(nan_input)
                self.logger.warning("NaNè¾“å…¥å¤„ç†å¯èƒ½æœ‰é—®é¢˜ï¼šåº”è¯¥æŠ›å‡ºé”™è¯¯æˆ–ä¿®å¤")
            except (ValueError, RuntimeError) as e:
                self.logger.info(f"æ­£ç¡®å¤„ç†NaNè¾“å…¥: {type(e).__name__}")
            
            self.log_test("é”™è¯¯æ¢å¤å’Œå¼‚å¸¸å¤„ç†", True, "å¼‚å¸¸å¤„ç†æµ‹è¯•å®Œæˆ")
            return True
            
        except Exception as e:
            self.log_test("é”™è¯¯æ¢å¤å’Œå¼‚å¸¸å¤„ç†", False, f"å¼‚å¸¸å¤„ç†æµ‹è¯•é”™è¯¯: {e}")
            return False
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        print("ğŸ”¬ å¼€å§‹å…¨é¢æ·±åº¦æµ‹è¯•...")
        print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {self.device}")
        print("=" * 80)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            ("å¯¼å…¥å’Œä¾èµ–", self.test_1_imports_and_dependencies),
            ("é…ç½®ç”Ÿæˆ", self.test_2_config_generation),
        ]
        
        config = None
        all_passed = True
        
        # æ‰§è¡ŒåŸºç¡€æµ‹è¯•
        for test_name, test_func in tests:
            try:
                if test_name == "é…ç½®ç”Ÿæˆ":
                    config = test_func()
                    success = bool(config)
                else:
                    success = test_func()
                
                if not success:
                    all_passed = False
                    
            except Exception as e:
                self.log_test(test_name, False, f"æµ‹è¯•å¼‚å¸¸: {e}")
                all_passed = False
        
        # å¦‚æœåŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•
        if not config:
            print("\nâŒ åŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œåœæ­¢åç»­æµ‹è¯•")
            return self._generate_test_report()
        
        # é«˜çº§æµ‹è¯•åˆ—è¡¨
        advanced_tests = [
            ("æ•°æ®åŠ è½½", lambda: self.test_3_data_loading(config)),
            ("ä¸“å®¶ç½‘ç»œ", lambda: self.test_4_expert_network(config)),
            ("å®Œæ•´æ¨¡å‹", lambda: self.test_5_full_model(config)),
            ("æŸå¤±è®¡ç®—", lambda: self.test_6_loss_computation(config)),
            ("è®­ç»ƒæ­¥éª¤", lambda: self.test_7_training_step(config)),
            ("å†…å­˜è®¾å¤‡", lambda: self.test_8_memory_and_device_consistency(config)),
            ("æ•°å€¼ç¨³å®šæ€§", lambda: self.test_9_numerical_stability(config)),
            ("å¼‚å¸¸å¤„ç†", lambda: self.test_10_error_recovery(config)),
        ]
        
        # æ‰§è¡Œé«˜çº§æµ‹è¯•
        for test_name, test_func in advanced_tests:
            try:
                success = test_func()
                if not success:
                    all_passed = False
            except Exception as e:
                self.log_test(test_name, False, f"æµ‹è¯•å¼‚å¸¸: {e}")
                all_passed = False
                traceback.print_exc()
        
        return self._generate_test_report()
    
    def _generate_test_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å…¨é¢æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)
        
        passed_tests = sum(1 for result in self.test_results.values() if result['success'])
        total_tests = len(self.test_results)
        
        print(f"ğŸ“ˆ æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
        
        if passed_tests == total_tests:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒæµç¨‹å‡†å¤‡å°±ç»ªã€‚")
        else:
            print("âš ï¸  å­˜åœ¨å¤±è´¥çš„æµ‹è¯•ï¼Œéœ€è¦ä¿®å¤ï¼š")
            for error in self.critical_errors:
                print(f"   - {error}")
        
        print("\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in self.test_results.items():
            status = "âœ…" if result['success'] else "âŒ"
            print(f"   {status} {test_name}: {result['message']}")
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
            'all_passed': passed_tests == total_tests,
            'test_results': self.test_results,
            'critical_errors': self.critical_errors
        }

def main():
    """ä¸»å‡½æ•°"""
    test_suite = ComprehensiveTestSuite()
    report = test_suite.run_comprehensive_test()
    
    # è¿”å›çŠ¶æ€ç 
    return 0 if report['all_passed'] else 1

if __name__ == "__main__":
    sys.exit(main()) 