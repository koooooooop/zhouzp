#!/usr/bin/env python3
"""
MÂ²-MOEP æ¼”ç¤ºå®éªŒè„šæœ¬
å¿«é€Ÿå±•ç¤ºç³»ç»ŸåŠŸèƒ½å’Œå®éªŒæµç¨‹
"""

import os
import sys
import torch
import numpy as np
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.m2_moep import M2_MOEP
from utils.losses import CompositeLoss
from utils.metrics import calculate_metrics
from data.universal_dataset import UniversalDataModule
from configs.config_generator import ConfigGenerator


def demo_experiment():
    """æ¼”ç¤ºå®éªŒ"""
    
    print("ğŸš€ MÂ²-MOEP æ¼”ç¤ºå®éªŒå¼€å§‹")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    if torch.cuda.is_available():
        print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    # 1. åˆ›å»ºåˆæˆæ•°æ®å®éªŒ
    print("\nğŸ”¬ æ­¥éª¤1: åˆæˆæ•°æ®å®éªŒ")
    print("-" * 30)
    
    # åˆæˆæ•°æ®é…ç½®
    synthetic_config = {
        'model': {
            'input_dim': 10,
            'output_dim': 10,
            'hidden_dim': 128,
            'num_experts': 4,
            'seq_len': 24,
            'pred_len': 12,
            'top_k': 3,
            'embedding_dim': 64,
            'expert_params': {
                'mamba_d_model': 128,
                'mamba_scales': [1, 2, 4]
            },
            'flow': {
                'latent_dim': 64,
                'hidden_dim': 128,
                'num_coupling_layers': 4
            }
        },
        'data': {
            'dataset_type': 'synthetic',
            'data_path': 'synthetic',
            'seq_len': 24,
            'pred_len': 12,
            'batch_size': 16,
            'num_workers': 2,
            'synthetic_samples': 1000,
            'noise_level': 0.1
        },
        'training': {
            'epochs': 3,
            'learning_rate': 1e-3,
            'weight_decay': 1e-4,
            'loss_weights': {
                'init_sigma_rc': 1.0,
                'init_sigma_cl': 1.0,
                'init_sigma_pr': 1.0,
                'init_sigma_consistency': 1.0,
                'init_sigma_balance': 1.0
            }
        }
    }
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = M2_MOEP(synthetic_config).to(device)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # åˆ›å»ºæ•°æ®
        data_module = UniversalDataModule(synthetic_config, for_pretraining=True)
        train_loader = data_module.get_train_loader()
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸï¼Œæ‰¹æ¬¡æ•°: {len(train_loader)}")
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        criterion = CompositeLoss(synthetic_config).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=synthetic_config['training']['learning_rate'])
        
        # è®­ç»ƒå¾ªç¯
        model.train()
        print("\nğŸƒ å¼€å§‹è®­ç»ƒ...")
        
        for epoch in range(synthetic_config['training']['epochs']):
            epoch_losses = []
            epoch_start = time.time()
            
            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):
                if batch_idx >= 5:  # é™åˆ¶æ‰¹æ¬¡æ•°é‡
                    break
                
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                output = model(batch_x, ground_truth=batch_y, return_aux_info=True)
                predictions = output['predictions']
                aux_info = output['aux_info']
                
                # è®¡ç®—æŸå¤±
                total_loss, losses = criterion(predictions, batch_y, aux_info)
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                epoch_losses.append(total_loss.item())
            
            epoch_time = time.time() - epoch_start
            avg_loss = np.mean(epoch_losses)
            print(f"   Epoch {epoch+1}/{synthetic_config['training']['epochs']}: "
                  f"æŸå¤±={avg_loss:.4f}, æ—¶é—´={epoch_time:.2f}s")
        
        # è¯„ä¼°
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
        model.eval()
        with torch.no_grad():
            test_batch_x, test_batch_y = next(iter(train_loader))
            test_batch_x = test_batch_x.to(device)
            test_batch_y = test_batch_y.to(device)
            
            output = model(test_batch_x, return_aux_info=True)
            predictions = output['predictions']
            aux_info = output['aux_info']
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = calculate_metrics(predictions.cpu().numpy(), test_batch_y.cpu().numpy())
            
            print(f"   RMSE: {metrics['rmse']:.4f}")
            print(f"   MAE: {metrics['mae']:.4f}")
            print(f"   MAPE: {metrics['mape']:.4f}")
            
            # ä¸“å®¶åˆ†æ
            expert_weights = aux_info['expert_weights']
            expert_usage = expert_weights.mean(dim=0)
            print(f"   ä¸“å®¶ä½¿ç”¨ç‡: {expert_usage.cpu().numpy()}")
        
        print("âœ… åˆæˆæ•°æ®å®éªŒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åˆæˆæ•°æ®å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
    
    # 2. çœŸå®æ•°æ®é›†å®éªŒé¢„è§ˆ
    print("\nğŸ¢ æ­¥éª¤2: çœŸå®æ•°æ®é›†å®éªŒé¢„è§ˆ")
    print("-" * 30)
    
    # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
    datasets = ConfigGenerator.get_supported_datasets()
    print("ğŸ“‹ å¯ç”¨æ•°æ®é›†:")
    for i, dataset in enumerate(datasets, 1):
        info = ConfigGenerator.get_dataset_info(dataset)
        print(f"   {i}. {dataset}: {info['description']}")
    
    print("\nğŸ’¡ è¿è¡ŒçœŸå®æ•°æ®é›†å®éªŒçš„å‘½ä»¤:")
    print("   python train.py --config configs/electricity_quick.yaml")
    print("   python universal_experiment.py --dataset electricity --epochs 10")
    print("   python run_all_experiments.py  # è¿è¡Œæ‰€æœ‰æ•°æ®é›†")
    
    # 3. ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
    print("\nâš¡ æ­¥éª¤3: ç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("-" * 30)
    
    try:
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½
        batch_sizes = [8, 16, 32, 64]
        print("ğŸ” æ‰¹æ¬¡å¤§å°æ€§èƒ½æµ‹è¯•:")
        
        for batch_size in batch_sizes:
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                test_input = torch.randn(batch_size, 24, 10).to(device)
                
                # è®¡æ—¶
                start_time = time.time()
                
                with torch.no_grad():
                    output = model(test_input, return_aux_info=True)
                
                end_time = time.time()
                inference_time = end_time - start_time
                throughput = batch_size / inference_time
                
                print(f"   æ‰¹æ¬¡å¤§å° {batch_size:2d}: {inference_time*1000:.2f}ms, "
                      f"ååé‡: {throughput:.1f} samples/s")
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"   æ‰¹æ¬¡å¤§å° {batch_size:2d}: GPUå†…å­˜ä¸è¶³")
                    break
                else:
                    raise e
        
        print("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
    
    print("\nğŸ‰ æ¼”ç¤ºå®éªŒå®Œæˆï¼")
    print("=" * 60)
    
    # 4. ä¸‹ä¸€æ­¥å»ºè®®
    print("\nğŸ“ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. è¿è¡Œå®Œæ•´çš„çœŸå®æ•°æ®é›†å®éªŒ")
    print("2. è°ƒæ•´è¶…å‚æ•°è¿›è¡Œä¼˜åŒ–")
    print("3. åˆ†æä¸“å®¶ç½‘ç»œçš„ç‰¹åŒ–æƒ…å†µ")
    print("4. æ¯”è¾ƒä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½")
    print("5. å¯è§†åŒ–é¢„æµ‹ç»“æœå’Œä¸“å®¶æƒé‡")


if __name__ == "__main__":
    demo_experiment() 