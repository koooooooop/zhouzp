import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

try:
    from mamba_ssm import Mamba
    MAMBA_AVAILABLE = True
except ImportError:
    MAMBA_AVAILABLE = False
    print("Warning: mamba_ssm not available, using LSTM as fallback")

class FrequencyMambaFFTExtractor(nn.Module):
    """
    å€Ÿé‰´FrequencyMambaçš„FFTç‰¹å¾æå–å™¨
    æå–å¹…åº¦å’Œç›¸ä½ä¿¡æ¯ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„é¢‘ç‡é€‰æ‹©
    """
    def __init__(self, seq_len, input_dim, fft_bins=16):
        super().__init__()
        self.seq_len = seq_len
        self.input_dim = input_dim
        self.fft_bins = min(fft_bins, seq_len // 2 + 1)  # ç¡®ä¿ä¸è¶…è¿‡FFTè¾“å‡ºé•¿åº¦
        
        # å¯å­¦ä¹ çš„é¢‘ç‡é€‰æ‹©æƒé‡
        self.freq_attention = nn.Parameter(torch.ones(self.fft_bins) / self.fft_bins)
        
        # æ•°å€¼ç¨³å®šæ€§å‚æ•°
        self.magnitude_scale = nn.Parameter(torch.tensor(1.0))
        self.phase_scale = nn.Parameter(torch.tensor(0.1))
        
    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, input_dim]
        Returns:
            fft_features: [batch_size, input_dim * fft_bins * 2]
        """
        try:
            batch_size = x.size(0)
            fft_features = []
            
            # å¯¹æ¯ä¸ªå˜é‡åˆ†åˆ«è¿›è¡ŒFFT
            for i in range(self.input_dim):
                # æå–å•ä¸ªå˜é‡çš„æ—¶é—´åºåˆ—
                feature = x[:, :, i]  # [batch_size, seq_len]
                
                # è®¡ç®—FFT
                fft_result = torch.fft.rfft(feature, dim=1)  # [batch_size, seq_len//2 + 1]
                
                # æå–å¹…åº¦å’Œç›¸ä½
                magnitudes = torch.abs(fft_result)  # [batch_size, seq_len//2 + 1]
                phases = torch.angle(fft_result)  # [batch_size, seq_len//2 + 1]
                
                # æ•°å€¼ç¨³å®šæ€§å¤„ç†
                magnitudes = torch.clamp(magnitudes, min=1e-8, max=100.0)
                phases = torch.clamp(phases, min=-np.pi, max=np.pi)
                
                # é€‰æ‹©å‰Nä¸ªé¢‘ç‡åˆ†é‡
                if magnitudes.size(1) >= self.fft_bins:
                    selected_magnitudes = magnitudes[:, :self.fft_bins]
                    selected_phases = phases[:, :self.fft_bins]
                else:
                    # å¦‚æœåºåˆ—å¤ªçŸ­ï¼Œè¿›è¡Œé›¶å¡«å……
                    selected_magnitudes = F.pad(magnitudes, (0, self.fft_bins - magnitudes.size(1)))
                    selected_phases = F.pad(phases, (0, self.fft_bins - phases.size(1)))
                
                # åº”ç”¨å¯å­¦ä¹ çš„é¢‘ç‡æƒé‡
                freq_weights = F.softmax(self.freq_attention, dim=0)
                weighted_magnitudes = selected_magnitudes * freq_weights
                weighted_phases = selected_phases * self.phase_scale
                
                # ç»„åˆå¹…åº¦å’Œç›¸ä½ä¿¡æ¯
                combined_features = torch.cat([
                    weighted_magnitudes * self.magnitude_scale,
                    weighted_phases
                ], dim=1)  # [batch_size, fft_bins * 2]
                
                fft_features.append(combined_features)
            
            # æ‹¼æ¥æ‰€æœ‰å˜é‡çš„FFTç‰¹å¾
            all_fft_features = torch.cat(fft_features, dim=1)  # [batch_size, input_dim * fft_bins * 2]
            
            # æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(all_fft_features).any() or torch.isinf(all_fft_features).any():
                print("è­¦å‘Š: FFTç‰¹å¾åŒ…å«NaN/Infï¼Œä½¿ç”¨é›¶å‘é‡æ›¿ä»£")
                all_fft_features = torch.zeros_like(all_fft_features)
            
            return all_fft_features
            
        except Exception as e:
            print(f"FFTç‰¹å¾æå–å¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡ä½œä¸ºå¤‡ç”¨
            return torch.zeros(batch_size, self.input_dim * self.fft_bins * 2, device=x.device)

class FrequencyMambaGate(nn.Module):
    """
    å€Ÿé‰´FrequencyMambaçš„é—¨æ§æœºåˆ¶
    æ§åˆ¶æ—¶åŸŸå’Œé¢‘åŸŸç‰¹å¾çš„èåˆ
    """
    def __init__(self, d_model):
        super().__init__()
        self.gate = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.Sigmoid()
        )
        
    def forward(self, time_features, freq_features):
        """
        Args:
            time_features: [batch_size, seq_len, d_model]
            freq_features: [batch_size, seq_len, d_model]
        Returns:
            gated_features: [batch_size, seq_len, d_model]
        """
        # è®¡ç®—é—¨æ§æƒé‡
        gate_weights = self.gate(time_features + freq_features)
        
        # é—¨æ§èåˆ
        gated_features = gate_weights * time_features + (1 - gate_weights) * freq_features
        
        return gated_features

class FFTmsMambaExpert(nn.Module):
    """
    FFT + ms-Mamba ä¸“å®¶ç½‘ç»œ
    å€Ÿé‰´FrequencyMambaçš„é¢‘åŸŸå¤„ç†ç­–ç•¥
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        self.d_model = config['model']['expert_params'].get('mamba_d_model', 256)
        self.mamba_d_model = self.d_model  # ä¿æŒä¸€è‡´æ€§
        
        # è¾“å…¥å’Œè¾“å‡ºç»´åº¦
        self.input_dim = config['model']['input_dim']
        self.output_dim = config['model']['output_dim']
        self.seq_len = config['model']['seq_len']
        self.pred_len = config['model']['pred_len']
        
        # åˆå§‹å°ºåº¦åˆ—è¡¨ï¼Œå¯è®­ç»ƒ Î”
        init_scales = config['model']['expert_params'].get('mamba_scales', [1, 2, 4])
        self.learnable_deltas = nn.Parameter(torch.tensor(init_scales, dtype=torch.float))
        
        # ä¸“å®¶IDï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰
        self.expert_id = config['model'].get('current_expert_id', 0)
        
        # === FrequencyMambaé£æ ¼çš„é¢‘åŸŸå¤„ç† ===
        # 1. FFTç‰¹å¾æå–å™¨
        fft_bins = config['model']['expert_params'].get('fft_bins', 16)
        self.fft_extractor = FrequencyMambaFFTExtractor(self.seq_len, self.input_dim, fft_bins)
        
        # 2. æ—¶åŸŸæŠ•å½±
        self.time_projection = nn.Linear(self.input_dim, self.d_model)
        
        # 3. é¢‘åŸŸæŠ•å½±
        fft_feature_size = self.input_dim * fft_bins * 2
        self.freq_projection = nn.Linear(fft_feature_size, self.d_model)
        
        # 4. æ—¶é¢‘èåˆå±‚
        self.time_freq_fusion = nn.Linear(self.d_model * 2, self.d_model)
        
        # 5. é¢‘åŸŸé—¨æ§æœºåˆ¶
        self.freq_gate = FrequencyMambaGate(self.d_model)
        
        # === å¤šå°ºåº¦Mambaå±‚ ===
        self.multi_scale_mamba = nn.ModuleList()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰CUDAå’Œmamba-ssm
        self.use_mamba = self._check_mamba_availability()
        
        for scale in init_scales:
            if self.use_mamba:
                try:
                    from mamba_ssm import Mamba
                    mamba_layer = Mamba(
                        d_model=self.d_model,
                        d_state=16,
                        d_conv=4,
                        expand=2,
                    )
                    self.multi_scale_mamba.append(mamba_layer)
                    print(f"âœ… æˆåŠŸåˆå§‹åŒ–Mambaå±‚ (scale={scale})")
                except Exception as e:
                    print(f"âŒ Mambaå±‚åˆå§‹åŒ–å¤±è´¥ (scale={scale}): {e}")
                    self.use_mamba = False
                    print("åˆ‡æ¢åˆ°LSTMæ›¿ä»£æ–¹æ¡ˆ")
                    # é‡æ–°åˆå§‹åŒ–æ‰€æœ‰å±‚ä¸ºLSTM
                    self.multi_scale_mamba = nn.ModuleList()
                    for _ in init_scales:
                        self.multi_scale_mamba.append(
                            nn.LSTM(self.d_model, self.d_model, batch_first=True)
                        )
                    break
            else:
                # ä½¿ç”¨LSTMä½œä¸ºæ›¿ä»£
                self.multi_scale_mamba.append(
                    nn.LSTM(self.d_model, self.d_model, batch_first=True)
                )
                print(f"ğŸ”„ ä½¿ç”¨LSTMæ›¿ä»£Mamba (scale={scale})")
        
        # === è¾“å‡ºå±‚ ===
        # å°ºåº¦èåˆå±‚
        self.scale_fusion = nn.Linear(self.d_model * len(init_scales), self.d_model)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(self.d_model, self.output_dim)
        
        # é¢„æµ‹å¤´
        self.prediction_head = nn.Linear(self.seq_len, self.pred_len)
        
        # ä¸“å®¶ä¸ªæ€§åŒ–å±‚
        self.expert_personalization = nn.Linear(self.d_model, self.d_model)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # æŠ¥å‘ŠMambaä½¿ç”¨çŠ¶æ€
        self._report_mamba_status()

    def _report_mamba_status(self):
        """æŠ¥å‘ŠMambaçš„ä½¿ç”¨çŠ¶æ€"""
        try:
            device = next(self.parameters()).device
        except StopIteration:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if self.use_mamba:
            print(f"ğŸš€ ä¸“å®¶{self.expert_id}æˆåŠŸä½¿ç”¨Mambaæ¶æ„ (FrequencyMambaé£æ ¼)")
            print(f"   - å°ºåº¦æ•°é‡: {len(self.multi_scale_mamba)}")
            print(f"   - d_model: {self.d_model}")
            print(f"   - è®¾å¤‡: {device}")
        else:
            print(f"âš ï¸  ä¸“å®¶{self.expert_id}ä½¿ç”¨LSTMæ›¿ä»£Mamba (FrequencyMambaé£æ ¼)")
            print(f"   - å°ºåº¦æ•°é‡: {len(self.multi_scale_mamba)}")
            print(f"   - d_model: {self.d_model}")
            print(f"   - è®¾å¤‡: {device}")

    def _initialize_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼Œä¸“é—¨é’ˆå¯¹FrequencyMambaé£æ ¼çš„æ¶æ„"""
        for name, m in self.named_modules():
            if isinstance(m, nn.Linear):
                if 'freq_projection' in name:
                    # é¢‘åŸŸæŠ•å½±å±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.1)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'time_projection' in name:
                    # æ—¶åŸŸæŠ•å½±å±‚ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.5)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'time_freq_fusion' in name:
                    # èåˆå±‚ä½¿ç”¨ä¿å®ˆåˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.2)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'gate' in name:
                    # é—¨æ§å±‚ä½¿ç”¨ç‰¹æ®Šåˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.1)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0.1)  # è½»å¾®çš„æ­£åç½®
                else:
                    # å…¶ä»–å±‚ä½¿ç”¨æ ‡å‡†åˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.5)
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            
            elif isinstance(m, nn.LSTM):
                # LSTMæƒé‡åˆå§‹åŒ–
                for param_name, param in m.named_parameters():
                    if 'weight_ih' in param_name:
                        nn.init.xavier_uniform_(param, gain=0.5)
                    elif 'weight_hh' in param_name:
                        nn.init.orthogonal_(param, gain=0.5)
                    elif 'bias' in param_name:
                        nn.init.zeros_(param)
                        # LSTMé—å¿˜é—¨åç½®è®¾ä¸º1
                        n = param.size(0)
                        start, end = n // 4, n // 2
                        param.data[start:end].fill_(1.0)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¯å­¦ä¹ çš„å°ºåº¦å‚æ•°
        if hasattr(self, 'learnable_deltas'):
            with torch.no_grad():
                self.learnable_deltas.data = torch.clamp(self.learnable_deltas.data, 1.0, 4.0)
        
        print(f"âœ… ä¸“å®¶{self.expert_id}æƒé‡åˆå§‹åŒ–å®Œæˆ (FrequencyMambaé£æ ¼)")

    def forward(self, x, return_features=True):
        """
        FrequencyMambaé£æ ¼çš„å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, input_dim]
            return_features: æ˜¯å¦è¿”å›ç‰¹å¾ï¼ˆTrueï¼‰è¿˜æ˜¯é¢„æµ‹ç»“æœï¼ˆFalseï¼‰
        Returns:
            å¦‚æœreturn_features=True: ç‰¹å¾å¼ é‡ [batch_size, seq_len, hidden_dim]
            å¦‚æœreturn_features=False: é¢„æµ‹å¼ é‡ [batch_size, pred_len, output_dim]
        """
        # è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥
        model_device = next(self.parameters()).device
        if x.device != model_device:
            x = x.to(model_device)
        
        batch_size, seq_len, input_dim = x.shape
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        if input_dim != self.input_dim:
            raise ValueError(f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.input_dim}, å®é™…{input_dim}")
        
        if seq_len != self.seq_len:
            raise ValueError(f"åºåˆ—é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{self.seq_len}, å®é™…{seq_len}")
        
        # === 1. FrequencyMambaé£æ ¼çš„æ—¶é¢‘åŸŸå¤„ç† ===
        # æ—¶åŸŸç‰¹å¾æå–
        time_features = self.time_projection(x)  # [B, T, d_model]
        
        # é¢‘åŸŸç‰¹å¾æå–
        fft_features = self.fft_extractor(x)  # [B, input_dim * fft_bins * 2]
        freq_features = self.freq_projection(fft_features)  # [B, d_model]
        
        # å°†é¢‘åŸŸç‰¹å¾æ‰©å±•åˆ°åºåˆ—é•¿åº¦
        freq_features_expanded = freq_features.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, d_model]
        
        # æ—¶é¢‘èåˆ
        fused_input = torch.cat([time_features, freq_features_expanded], dim=-1)  # [B, T, d_model*2]
        fused_features = self.time_freq_fusion(fused_input)  # [B, T, d_model]
        
        # åº”ç”¨é—¨æ§æœºåˆ¶
        gated_features = self.freq_gate(time_features, freq_features_expanded)  # [B, T, d_model]
        
        # æœ€ç»ˆèåˆ
        final_input = fused_features + gated_features  # [B, T, d_model]
        
        # === 2. å¤šå°ºåº¦Mambaå¤„ç† ===
        scale_outputs = []
        
        for i, mamba_layer in enumerate(self.multi_scale_mamba):
            # ä½¿ç”¨å¯å­¦ä¹ çš„deltaå‚æ•°
            delta = torch.clamp(self.learnable_deltas[i], min=1.0, max=8.0)
            scale = delta.item()
            
            # è·å–å½“å‰å°ºåº¦çš„è¾“å…¥
            if scale == 1:
                scaled_input = final_input
            elif scale > 1:
                scaled_input = self._downsample(final_input, scale)
            else:
                target_len = int(seq_len / scale)
                scaled_input = self._upsample(final_input, target_len)
            
            # é€šè¿‡Mambaå±‚æˆ–LSTMå±‚
            if self.use_mamba:
                scaled_output = mamba_layer(scaled_input)
            else:
                scaled_output, _ = mamba_layer(scaled_input)
            
            # æ¢å¤åˆ°åŸå§‹åºåˆ—é•¿åº¦
            if scaled_output.size(1) != seq_len:
                if scaled_output.size(1) < seq_len:
                    scaled_output = self._upsample(scaled_output, seq_len)
                else:
                    scale_factor = scaled_output.size(1) / seq_len
                    scaled_output = self._downsample(scaled_output, scale_factor)
            
            scale_outputs.append(scaled_output)
        
        # === 3. å°ºåº¦èåˆ ===
        if len(scale_outputs) > 1:
            multi_scale_features = torch.cat(scale_outputs, dim=-1)  # [B, T, d_model * num_scales]
            fused_output = self.scale_fusion(multi_scale_features)   # [B, T, d_model]
        else:
            fused_output = scale_outputs[0]
        
        # === 4. ä¸“å®¶ä¸ªæ€§åŒ– ===
        personalized_output = self.expert_personalization(fused_output)
        personalized_output = self.dropout(personalized_output)
        
        # å¦‚æœåªéœ€è¦ç‰¹å¾ï¼Œç›´æ¥è¿”å›
        if return_features:
            return personalized_output  # [B, T, d_model]
        
        # === 5. é¢„æµ‹è¾“å‡º ===
        output_features = self.output_projection(personalized_output)  # [B, T, output_dim]
        
        # æ—¶åºé¢„æµ‹
        output_transposed = output_features.transpose(1, 2)  # [B, output_dim, T]
        predictions_transposed = self.prediction_head(output_transposed)  # [B, output_dim, pred_len]
        predictions = predictions_transposed.transpose(1, 2)  # [B, pred_len, output_dim]
        
        return predictions

    def _downsample(self, x, scale):
        """ä¸‹é‡‡æ ·åˆ°æŒ‡å®šå°ºåº¦"""
        if scale == 1:
            return x
        
        scale = int(scale)
        x_permuted = x.permute(0, 2, 1)  # [B, d_model, seq_len]
        downsampled = F.avg_pool1d(x_permuted, kernel_size=scale, stride=scale)
        return downsampled.permute(0, 2, 1)  # [B, new_seq_len, d_model]

    def _upsample(self, x, target_len):
        """ä¸Šé‡‡æ ·åˆ°ç›®æ ‡é•¿åº¦"""
        if x.size(1) == target_len:
            return x
        
        x_permuted = x.permute(0, 2, 1)  # [B, d_model, seq_len]
        
        try:
            upsampled = F.interpolate(x_permuted, size=target_len, mode='linear', align_corners=False)
        except (RuntimeError, ValueError):
            upsampled = F.interpolate(x_permuted, size=target_len, mode='nearest')
        
        return upsampled.permute(0, 2, 1)  # [B, target_len, d_model]

    def _check_mamba_availability(self):
        """æ£€æŸ¥Mambaå¯ç”¨æ€§"""
        if not MAMBA_AVAILABLE:
            return False
        
        try:
            # æµ‹è¯•åˆ›å»ºä¸€ä¸ªå°çš„Mambaå±‚
            test_mamba = Mamba(d_model=16, d_state=8)
            test_input = torch.randn(1, 10, 16)
            _ = test_mamba(test_input)
            return True
        except Exception as e:
            print(f"Mambaå¯ç”¨æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def to(self, device):
        """è®¾å¤‡è½¬ç§»"""
        result = super().to(device)
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        for param in result.parameters():
            if param.device != device:
                param.data = param.data.to(device)
        
        return result