import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

try:
    from mamba_ssm import Mamba
    MAMBA_AVAILABLE = True
except ImportError:
    MAMBA_AVAILABLE = False
    print("Warning: mamba_ssm not available, using LSTM as fallback")

class FFTmsMambaExpert(nn.Module):
    """
    FFT + ms-Mamba ä¸“å®¶ç½‘ç»œ
    æ—©æœŸèåˆç­–ç•¥ï¼šåŸå§‹æ—¶é—´åºåˆ—ä¸FFTé¢‘åŸŸè¡¨ç¤ºæ‹¼æ¥
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        self.d_model = config['model']['expert_params'].get('mamba_d_model', 256)
        self.mamba_d_model = self.d_model  # ä¿æŒä¸€è‡´æ€§
        
        # è¾“å…¥å’Œè¾“å‡ºç»´åº¦
        self.input_dim = config['model']['input_dim']
        self.output_dim = config['model']['output_dim']
        self.seq_len = config['model']['seq_len']
        self.pred_len = config['model']['pred_len']
        
        # åˆå§‹å°ºåº¦åˆ—è¡¨ï¼Œå¯è®­ç»ƒ Î”
        init_scales = config['model']['expert_params'].get('mamba_scales', [1, 2, 4])
        self.learnable_deltas = nn.Parameter(torch.tensor(init_scales, dtype=torch.float))
        
        # ä¸“å®¶IDï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰
        self.expert_id = config['model'].get('current_expert_id', 0)
        
        # 1. è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(self.input_dim, self.d_model)
        
        # 2. FFTèåˆå±‚
        self.fft_fusion = nn.Linear(self.d_model, self.d_model)  # ä¿®å¤ï¼šD->Dè€ŒéD*3->D
        
        # 3. å¤šå°ºåº¦Mambaå±‚
        self.multi_scale_mamba = nn.ModuleList()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰CUDAå’Œmamba-ssm
        self.use_mamba = self._check_mamba_availability()
        
        for scale in init_scales:
            if self.use_mamba:
                try:
                    from mamba_ssm import Mamba
                    mamba_layer = Mamba(
                        d_model=self.d_model,
                        d_state=16,
                        d_conv=4,
                        expand=2,
                    )
                    self.multi_scale_mamba.append(mamba_layer)
                    print(f"âœ… æˆåŠŸåˆå§‹åŒ–Mambaå±‚ (scale={scale})")
                except Exception as e:
                    print(f"âŒ Mambaå±‚åˆå§‹åŒ–å¤±è´¥ (scale={scale}): {e}")
                    self.use_mamba = False
                    print("åˆ‡æ¢åˆ°LSTMæ›¿ä»£æ–¹æ¡ˆ")
                    # é‡æ–°åˆå§‹åŒ–æ‰€æœ‰å±‚ä¸ºLSTM
                    self.multi_scale_mamba = nn.ModuleList()
                    for _ in init_scales:
                        self.multi_scale_mamba.append(
                            nn.LSTM(self.d_model, self.d_model, batch_first=True)
                        )
                    break
            else:
                # ä½¿ç”¨LSTMä½œä¸ºæ›¿ä»£
                self.multi_scale_mamba.append(
                    nn.LSTM(self.d_model, self.d_model, batch_first=True)
                )
                print(f"ğŸ”„ ä½¿ç”¨LSTMæ›¿ä»£Mamba (scale={scale})")
        
        # 4. å°ºåº¦èåˆå±‚
        self.scale_fusion = nn.Linear(self.d_model * len(init_scales), self.d_model)
        
        # 5. è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(self.d_model, self.output_dim)
        
        # 6. é¢„æµ‹å¤´
        self.prediction_head = nn.Linear(self.seq_len, self.pred_len)
        
        # 7. ä¸“å®¶ä¸ªæ€§åŒ–å±‚
        self.expert_personalization = nn.Linear(self.d_model, self.d_model)
        
        # 8. Dropout
        self.dropout = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # æŠ¥å‘ŠMambaä½¿ç”¨çŠ¶æ€
        self._report_mamba_status()

    def _report_mamba_status(self):
        """æŠ¥å‘ŠMambaçš„ä½¿ç”¨çŠ¶æ€"""
        try:
            device = next(self.parameters()).device
        except StopIteration:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if self.use_mamba:
            print(f"ğŸš€ ä¸“å®¶{self.expert_id}æˆåŠŸä½¿ç”¨Mambaæ¶æ„")
            print(f"   - å°ºåº¦æ•°é‡: {len(self.multi_scale_mamba)}")
            print(f"   - d_model: {self.d_model}")
            print(f"   - è®¾å¤‡: {device}")
        else:
            print(f"âš ï¸  ä¸“å®¶{self.expert_id}ä½¿ç”¨LSTMæ›¿ä»£Mamba")
            print(f"   - å°ºåº¦æ•°é‡: {len(self.multi_scale_mamba)}")
            print(f"   - d_model: {self.d_model}")
            print(f"   - è®¾å¤‡: {device}")

    def _initialize_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼Œä¸“é—¨é’ˆå¯¹æ—¶é—´åºåˆ—é¢„æµ‹ä¼˜åŒ– - ğŸ”§ è¶…ä¿å®ˆç‰ˆæœ¬"""
        for name, m in self.named_modules():
            if isinstance(m, nn.Linear):
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–ç­–ç•¥
                if 'fft_fusion' in name:
                    # FFTèåˆå±‚ä½¿ç”¨æå°çš„æ­£äº¤åˆå§‹åŒ–
                    nn.init.orthogonal_(m.weight, gain=0.01)  # ä»0.1å‡å°‘åˆ°0.01
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'input_projection' in name:
                    # è¾“å…¥æŠ•å½±å±‚ä½¿ç”¨æ›´ä¿å®ˆçš„åˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.1)  # ä»é»˜è®¤1.0å‡å°‘åˆ°0.1
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'output_projection' in name or 'prediction_head' in name:
                    # è¾“å‡ºå±‚ä½¿ç”¨æå°æ–¹å·®çš„æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                    nn.init.normal_(m.weight, mean=0.0, std=0.001)  # ä»0.01å‡å°‘åˆ°0.001
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                elif 'scale_fusion' in name:
                    # å°ºåº¦èåˆå±‚ä½¿ç”¨æä¿å®ˆçš„Xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.1)  # ä»0.5å‡å°‘åˆ°0.1
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
                else:
                    # å…¶ä»–çº¿æ€§å±‚ä½¿ç”¨æä¿å®ˆçš„Xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(m.weight, gain=0.01)  # ä»0.5å‡å°‘åˆ°0.01
                    if m.bias is not None:
                        nn.init.zeros_(m.bias)
            
            elif isinstance(m, nn.LSTM):
                # LSTMæƒé‡åˆå§‹åŒ– - æ›´ä¿å®ˆ
                for param_name, param in m.named_parameters():
                    if 'weight_ih' in param_name:
                        nn.init.xavier_uniform_(param, gain=0.1)  # ä»0.5å‡å°‘åˆ°0.1
                    elif 'weight_hh' in param_name:
                        nn.init.orthogonal_(param, gain=0.1)  # ä»0.5å‡å°‘åˆ°0.1
                    elif 'bias' in param_name:
                        nn.init.zeros_(param)
                        # LSTMé—å¿˜é—¨åç½®è®¾ä¸ºè¾ƒå°å€¼
                        n = param.size(0)
                        start, end = n // 4, n // 2
                        param.data[start:end].fill_(0.5)  # ä»1.0å‡å°‘åˆ°0.5
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¯å­¦ä¹ çš„å°ºåº¦å‚æ•° - æ›´ä¿å®ˆ
        if hasattr(self, 'learnable_deltas'):
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆå§‹åŒ–ä¸ºæ›´å°çš„å€¼ï¼Œé¿å…æç«¯çš„ä¸Šä¸‹é‡‡æ ·
            with torch.no_grad():
                self.learnable_deltas.data = torch.clamp(self.learnable_deltas.data, 1.0, 2.0)  # ä»3.0å‡å°‘åˆ°2.0
        
        print(f"âœ… ä¸“å®¶{self.expert_id}æƒé‡åˆå§‹åŒ–å®Œæˆ (è¶…ä¿å®ˆç­–ç•¥)")

    def _stable_fft_fusion(self, x: torch.Tensor) -> torch.Tensor:
        """
        æ•°å€¼ç¨³å®šçš„FFTèåˆ - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
        :param x: è¾“å…¥å¼ é‡ [B, T, D]
        :return: èåˆåçš„å¼ é‡ [B, T, D]
        """
        try:
            # è¾“å…¥æ£€æŸ¥å’Œä¿®å¤
            if torch.isnan(x).any() or torch.isinf(x).any():
                print("è­¦å‘Š: FFTèåˆè¾“å…¥åŒ…å«NaNæˆ–Infï¼Œæ‰§è¡Œä¿®å¤")
                x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)
            
            # è¾“å…¥å½’ä¸€åŒ– - ä½¿ç”¨æ›´ä¿å®ˆçš„å½’ä¸€åŒ–
            x_normalized = F.layer_norm(x, x.shape[-1:])
            
            # è®¡ç®—FFT - ä»…åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡Œ
            x_fft = torch.fft.fft(x_normalized, dim=-2)
            
            # è·å–å¹…åº¦è°±å’Œç›¸ä½è°±
            magnitude = torch.abs(x_fft)
            phase = torch.angle(x_fft)
            
            # å¹…åº¦è°±ç¨³å®šåŒ–å¤„ç† - æ›´ä¿å®ˆçš„èŒƒå›´
            magnitude = torch.clamp(magnitude, min=1e-6, max=5.0)
            
            # å¯¹æ•°å½’ä¸€åŒ–é˜²æ­¢æ•°å€¼è¿‡å¤§ - æ›´ä¿å®ˆçš„èŒƒå›´
            log_magnitude = torch.log(magnitude + 1e-6)
            log_magnitude = torch.clamp(log_magnitude, min=-3.0, max=3.0)
            
            # ç›¸ä½å½’ä¸€åŒ– - é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
            phase = torch.clamp(phase, min=-np.pi, max=np.pi)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸å†æ‹¼æ¥ç‰¹å¾ï¼Œè€Œæ˜¯ä½¿ç”¨åŠ æƒèåˆ
            if hasattr(self, 'fft_fusion') and self.fft_fusion is not None:
                # åˆ›å»ºåŠ æƒç‰¹å¾è€Œéæ‹¼æ¥ç‰¹å¾
                # ä½¿ç”¨é—¨æ§æœºåˆ¶æ§åˆ¶æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®
                alpha = 0.3  # å¹…åº¦ç‰¹å¾æƒé‡
                beta = 0.2   # ç›¸ä½ç‰¹å¾æƒé‡
                gamma = 0.5  # åŸå§‹ç‰¹å¾æƒé‡
                
                # ç¡®ä¿æƒé‡å’Œä¸º1
                total_weight = alpha + beta + gamma
                alpha, beta, gamma = alpha/total_weight, beta/total_weight, gamma/total_weight
                
                # å°†é¢‘åŸŸç‰¹å¾è½¬æ¢å›æ—¶åŸŸå¹¶èåˆ
                magnitude_features = F.layer_norm(log_magnitude, log_magnitude.shape[-1:])
                phase_features = F.layer_norm(phase, phase.shape[-1:])
                
                # åŠ æƒèåˆè€Œéæ‹¼æ¥
                fused_features = (
                    gamma * x_normalized + 
                    alpha * magnitude_features + 
                    beta * phase_features
                )
                
                # é€šè¿‡fusion layerè¿›è¡Œé¢å¤–çš„ç‰¹å¾å˜æ¢
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šè¾“å…¥ç»´åº¦ä¿æŒDè€ŒéD*3
                fusion_input = fused_features  # [B, T, D]
                fused_x = self.fft_fusion(fusion_input)  # [B, T, D]
            else:
                # ç®€å•å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾
                fused_x = x_normalized
            
            # è¾“å‡ºæ£€æŸ¥å’Œä¿®å¤
            if torch.isnan(fused_x).any() or torch.isinf(fused_x).any():
                print("è­¦å‘Š: FFTèåˆè¾“å‡ºåŒ…å«å¼‚å¸¸å€¼ï¼Œä½¿ç”¨åŸå§‹è¾“å…¥")
                return x
            
            # æ®‹å·®è¿æ¥ï¼ˆæ›´ä¿å®ˆçš„æƒé‡ï¼‰
            residual_weight = 0.1  # æ›´å°çš„èåˆæƒé‡
            output = (1 - residual_weight) * x + residual_weight * fused_x
            
            # æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§ä¿è¯
            output = torch.clamp(output, min=-5.0, max=5.0)
            
            return output
            
        except Exception as e:
            print(f"é”™è¯¯: FFTèåˆå¤±è´¥: {e}")
            return x  # å‡ºé”™æ—¶è¿”å›åŸå§‹è¾“å…¥

    def forward(self, x, return_features=True):
        """
        FFT+ms-Mambaä¸“å®¶å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, input_dim]
            return_features: æ˜¯å¦è¿”å›ç‰¹å¾ï¼ˆTrueï¼‰è¿˜æ˜¯é¢„æµ‹ç»“æœï¼ˆFalseï¼‰
        Returns:
            å¦‚æœreturn_features=True: ç‰¹å¾å¼ é‡ [batch_size, seq_len, hidden_dim]
            å¦‚æœreturn_features=False: é¢„æµ‹å¼ é‡ [batch_size, pred_len, output_dim]
        """
        # è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥
        model_device = next(self.parameters()).device
        if x.device != model_device:
            print(f"âš ï¸  è®¾å¤‡ä¸åŒ¹é…: è¾“å…¥åœ¨{x.device}, æ¨¡å‹åœ¨{model_device}")
            x = x.to(model_device)
            print(f"âœ… è¾“å…¥å·²ç§»åŠ¨åˆ°{model_device}")
        
        batch_size, seq_len, input_dim = x.shape
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        if input_dim != self.input_dim:
            raise ValueError(f"è¾“å…¥ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.input_dim}, å®é™…{input_dim}")
        
        if seq_len != self.seq_len:
            raise ValueError(f"åºåˆ—é•¿åº¦ä¸åŒ¹é…: æœŸæœ›{self.seq_len}, å®é™…{seq_len}")
        
        # === 1. è¾“å…¥æŠ•å½± ===
        x_proj = self.input_projection(x)  # [B, T, d_model]
        
        # === 2. FFTèåˆï¼ˆå¯é€‰ï¼Œæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰ ===
        if hasattr(self, 'fft_fusion') and self.fft_fusion is not None:
            x_proj = self._stable_fft_fusion(x_proj)
        
        # === 3. å¤šå°ºåº¦å¤„ç† ===
        scale_outputs = []
        
        for i, mamba_layer in enumerate(self.multi_scale_mamba):
            # ä½¿ç”¨å¯å­¦ä¹ çš„deltaå‚æ•°å¹¶æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            delta = torch.clamp(self.learnable_deltas[i], min=1.0, max=8.0)
            scale = delta.item()  # è½¬æ¢ä¸ºæ ‡é‡
            
            # è·å–å½“å‰å°ºåº¦çš„è¾“å…¥
            if scale == 1:
                scaled_input = x_proj
            elif scale > 1:
                # ä¸‹é‡‡æ ·
                scaled_input = self._downsample(x_proj, scale)
            else:
                # ä¸Šé‡‡æ ·
                target_len = int(seq_len / scale)
                scaled_input = self._upsample(x_proj, target_len)
            
            # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
            if scaled_input.device != model_device:
                scaled_input = scaled_input.to(model_device)
            
            # é€šè¿‡Mambaå±‚æˆ–LSTMå±‚
            if self.use_mamba:
                scaled_output = mamba_layer(scaled_input)
            else:
                # LSTMéœ€è¦ç‰¹æ®Šå¤„ç†
                scaled_output, _ = mamba_layer(scaled_input)
            
            # æ¢å¤åˆ°åŸå§‹åºåˆ—é•¿åº¦
            if scaled_output.size(1) != seq_len:
                if scaled_output.size(1) < seq_len:
                    # éœ€è¦ä¸Šé‡‡æ ·
                    scaled_output = self._upsample(scaled_output, seq_len)
                else:
                    # éœ€è¦ä¸‹é‡‡æ ·
                    scale_factor = scaled_output.size(1) / seq_len
                    scaled_output = self._downsample(scaled_output, scale_factor)
            
            scale_outputs.append(scaled_output)
        
        # === 4. å°ºåº¦èåˆ ===
        if len(scale_outputs) > 1:
            fused_features = torch.cat(scale_outputs, dim=-1)  # [B, T, d_model * num_scales]
            fused_output = self.scale_fusion(fused_features)   # [B, T, d_model]
        else:
            fused_output = scale_outputs[0]
        
        # === 5. ä¸“å®¶ä¸ªæ€§åŒ– ===
        personalized_output = self.expert_personalization(fused_output)
        personalized_output = self.dropout(personalized_output)
        
        # å¦‚æœåªéœ€è¦ç‰¹å¾ï¼Œç›´æ¥è¿”å›
        if return_features:
            return personalized_output  # [B, T, d_model] æˆ– [B, T, hidden_dim]
        
        # === 6. è¾“å‡ºæŠ•å½± ===
        output_features = self.output_projection(personalized_output)  # [B, T, output_dim]
        
        # === 7. æ—¶åºé¢„æµ‹ ===
        # è½¬ç½®ä»¥è¿›è¡Œæ—¶åºå˜æ¢: [B, output_dim, T]
        output_transposed = output_features.transpose(1, 2)
        
        # é¢„æµ‹å¤´: [B, output_dim, T] -> [B, output_dim, pred_len]
        predictions_transposed = self.prediction_head(output_transposed)
        
        # è½¬å›: [B, pred_len, output_dim]
        predictions = predictions_transposed.transpose(1, 2)
        
        # æœ€ç»ˆè®¾å¤‡æ£€æŸ¥
        if predictions.device != model_device:
            predictions = predictions.to(model_device)
        
        return predictions

    def _downsample(self, x, scale):
        """
        ä¸‹é‡‡æ ·åˆ°æŒ‡å®šå°ºåº¦
        :param x: è¾“å…¥åºåˆ— [B, seq_len, d_model]
        :param scale: ä¸‹é‡‡æ ·å°ºåº¦
        :return: ä¸‹é‡‡æ ·åçš„åºåˆ—
        """
        if scale == 1:
            return x
        
        # ç¡®ä¿scaleæ˜¯æ•´æ•°
        scale = int(scale)
        
        # ä½¿ç”¨å¹³å‡æ± åŒ–ä¸‹é‡‡æ ·
        x_permuted = x.permute(0, 2, 1)  # [B, d_model, seq_len]
        downsampled = F.avg_pool1d(x_permuted, kernel_size=scale, stride=scale)
        return downsampled.permute(0, 2, 1)  # [B, new_seq_len, d_model]

    def _upsample(self, x, target_len):
        """
        ä¸Šé‡‡æ ·åˆ°ç›®æ ‡é•¿åº¦
        :param x: è¾“å…¥åºåˆ— [B, seq_len, d_model]
        :param target_len: ç›®æ ‡é•¿åº¦
        :return: ä¸Šé‡‡æ ·åçš„åºåˆ—
        """
        if x.size(1) == target_len:
            return x
        
        # ä½¿ç”¨çº¿æ€§æ’å€¼ä¸Šé‡‡æ ·ï¼Œå…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬
        x_permuted = x.permute(0, 2, 1)  # [B, d_model, seq_len]
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬å…¼å®¹æ€§
        try:
            # å°è¯•ä½¿ç”¨linearæ¨¡å¼ï¼ˆPyTorch >= 1.9æ¨èï¼‰
            upsampled = F.interpolate(x_permuted, size=target_len, mode='linear', align_corners=False)
        except (RuntimeError, ValueError):
            # å›é€€åˆ°nearestæ¨¡å¼ï¼ˆå…¼å®¹æ‰€æœ‰ç‰ˆæœ¬ï¼‰
            print("è­¦å‘Š: linearæ’å€¼ä¸æ”¯æŒï¼Œä½¿ç”¨nearestæ¨¡å¼")
            upsampled = F.interpolate(x_permuted, size=target_len, mode='nearest')
        
        return upsampled.permute(0, 2, 1)  # [B, target_len, d_model]

    def _check_mamba_availability(self):
        """æ£€æŸ¥Mambaæ˜¯å¦å¯ç”¨ï¼Œå¹¶è¿›è¡Œè®¾å¤‡å…¼å®¹æ€§æµ‹è¯•"""
        # è·å–å½“å‰è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        try:
            import mamba_ssm
            
            # å¦‚æœæ²¡æœ‰CUDAï¼Œç›´æ¥è¿”å›False
            if not torch.cuda.is_available():
                print("âš ï¸  Mambaéœ€è¦CUDAï¼Œä½†CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨LSTMæ›¿ä»£")
                return False
            
            print(f"åœ¨è®¾å¤‡ {device} ä¸Šæµ‹è¯•Mamba...")
            
            # è®¾å¤‡å…¼å®¹æ€§æµ‹è¯•
            test_tensor = torch.randn(1, 10, 64, device=device)
            from mamba_ssm import Mamba
            test_layer = Mamba(d_model=64, d_state=16, d_conv=4, expand=2).to(device)
            _ = test_layer(test_tensor)
            
            print(f"âœ… Mambaåœ¨è®¾å¤‡ {device} ä¸Šæµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âš ï¸  Mambaåœ¨è®¾å¤‡ {device} ä¸Šä¸å¯ç”¨: {e}")
            return False

    def to(self, device):
        """é‡å†™toæ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡"""
        # å¦‚æœä½¿ç”¨mambaä¸”è®¾å¤‡ä¸æ˜¯CUDAï¼Œå¼ºåˆ¶ä½¿ç”¨CUDA
        if self.use_mamba and device.type != 'cuda':
            if torch.cuda.is_available():
                device = torch.device('cuda')
                print(f"ä½¿ç”¨mambaæ—¶å¼ºåˆ¶åˆ‡æ¢åˆ°CUDAè®¾å¤‡")
            else:
                print(f"âš ï¸mambaéœ€è¦CUDAä½†CUDAä¸å¯ç”¨ï¼Œå°†åˆ‡æ¢åˆ°LSTM")
                self.use_mamba = False
                # é‡æ–°åˆå§‹åŒ–ä¸ºLSTM
                self.multi_scale_mamba = nn.ModuleList()
                for _ in range(len(self.learnable_deltas)):
                    self.multi_scale_mamba.append(
                        nn.LSTM(self.d_model, self.d_model, batch_first=True)
                    )
        
        super().to(device)
        
        # ç¡®ä¿å¯å­¦ä¹ å‚æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        if hasattr(self, 'learnable_deltas'):
            self.learnable_deltas.data = self.learnable_deltas.data.to(device)
        
        # ç¡®ä¿æ‰€æœ‰å±‚éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.input_projection.to(device)
        if hasattr(self, 'fft_fusion') and self.fft_fusion is not None:
            self.fft_fusion.to(device)
        
        # ç¡®ä¿å¤šå°ºåº¦mambaå±‚åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        for layer in self.multi_scale_mamba:
            layer.to(device)
        
        self.scale_fusion.to(device)
        self.output_projection.to(device)
        self.prediction_head.to(device)
        self.expert_personalization.to(device)
        
        return self