#!/usr/bin/env python3
"""
MÂ²-MOEP: Multi-scale Multi-expert Orthogonal Embedding Predictor
åŸºäºFFT+ms-Mambaçš„å¤šå°ºåº¦å¤šä¸“å®¶æ—¶åºé¢„æµ‹æ¨¡å‹
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.init import xavier_uniform_, constant_
from typing import Dict, List, Optional, Tuple, Union
import numpy as np

from .expert import FFTmsMambaExpert
from .flow import SimpleStableFlow as FlowModel


class M2_MOEP(nn.Module):
    """
    MÂ²-MOEPä¸»æ¨¡å‹
    
    æ¶æ„ç»„æˆï¼š
    1. è¾“å…¥åµŒå…¥å±‚ (Input Embedding)
    2. ä½ç½®ç¼–ç  (Positional Encoding)
    3. å¤šä¸“å®¶ç½‘ç»œ (Multi-Expert Network with FFT+ms-Mamba)
    4. ä¸“å®¶è·¯ç”±å™¨ (Expert Router)
    5. æ¸©åº¦è°ƒåº¦å™¨ (Temperature Scheduler)
    6. å¤šå°ºåº¦ç‰¹å¾èåˆ (Multi-scale Feature Fusion)
    7. é¢„æµ‹å¤´ (Prediction Head)
    """
    
    def __init__(self, config: Dict):
        super(M2_MOEP, self).__init__()
        
        self.config = config
        self.model_config = config['model']
        self.training_config = config.get('training', {})
        
        # åŸºç¡€å‚æ•°
        self.input_dim = self.model_config['input_dim']
        self.hidden_dim = self.model_config['hidden_dim']
        self.output_dim = self.model_config['output_dim']
        self.num_experts = self.model_config['num_experts']
        self.seq_len = self.model_config['seq_len']
        self.pred_len = self.model_config['pred_len']
        self.embedding_dim = self.model_config.get('embedding_dim', self.hidden_dim)
        
        # è®¾å¤‡ç®¡ç†
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._device_initialized = False  # æ·»åŠ è®¾å¤‡åˆå§‹åŒ–æ ‡å¿—
        
        # æ„å»ºæ¨¡å‹ç»„ä»¶
        self._build_model()
        
        # è‡ªåŠ¨å¤„ç†Mambaä¸“å®¶çš„è®¾å¤‡
        self._handle_mamba_devices()
        
        # åˆå§‹åŒ–æ¸©åº¦è°ƒåº¦å™¨
        self._init_temperature_scheduler()
        
        # åˆå§‹åŒ–æŸå¤±ç»Ÿè®¡
        self._init_loss_stats()
        
        # æ¨¡å‹å‚æ•°åˆå§‹åŒ–
        self._init_weights()
        
        print(f"âœ… MÂ²-MOEPæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è¾“å…¥ç»´åº¦: {self.input_dim}")
        print(f"   - éšè—ç»´åº¦: {self.hidden_dim}")
        print(f"   - ä¸“å®¶æ•°é‡: {self.num_experts}")
        print(f"   - æ€»å‚æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def _build_model(self):
        """æ„å»ºæ¨¡å‹æ¶æ„ - ğŸ”§ æŒ‰ç…§MÂ²-MOEPæ–‡æ¡£é‡æ–°è®¾è®¡"""
        
        # 1. é¢„è®­ç»ƒçš„æµå¼æ¨¡å‹ï¼ˆæ ¸å¿ƒç»„ä»¶ï¼‰
        flow_config = self.model_config.get('flow', {})
        self.flow_model = FlowModel(
            input_dim=self.input_dim * self.seq_len,  # æ‰å¹³åŒ–è¾“å…¥
            flow_layers=flow_config.get('num_layers', 2)
        )
        
        # 2. åº¦é‡å­¦ä¹ é—¨æ§ç½‘ç»œï¼ˆå­ªç”Ÿç¼–ç å™¨ï¼‰
        self.gating_encoder = nn.Sequential(
            nn.Linear(self.input_dim * self.seq_len, self.embedding_dim),
            nn.LayerNorm(self.embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.embedding_dim, self.embedding_dim),
            nn.LayerNorm(self.embedding_dim),
            nn.ReLU(),
            nn.Linear(self.embedding_dim, 128)  # åµŒå…¥å‘é‡ç»´åº¦
        )
        
        # 3. å¯å­¦ä¹ çš„ä¸“å®¶åŸå‹ï¼ˆå…³é”®åˆ›æ–°ï¼‰
        self.expert_prototypes = nn.Parameter(
            torch.randn(self.num_experts, 128) * 0.01  # å°åˆå§‹åŒ–
        )
        
        # 4. ä¸“å®¶ç½‘ç»œï¼ˆFFT+ms-Mambaï¼‰
        expert_configs = []
        for i in range(self.num_experts):
            expert_config = self.config.copy()
            expert_config['model'] = self.config['model'].copy()
            expert_config['model']['current_expert_id'] = i
            # ä¸“å®¶ç½‘ç»œç›´æ¥å¤„ç†åŸå§‹æ»‘åŠ¨çª—å£
            expert_config['model']['input_dim'] = self.input_dim
            expert_config['model']['output_dim'] = self.hidden_dim
            expert_configs.append(expert_config)
        
        self.experts = nn.ModuleList([
            FFTmsMambaExpert(expert_config) for expert_config in expert_configs
        ])
        
        # 5. ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim, self.hidden_dim)
        )
        
        # 6. é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.LayerNorm(self.hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_dim // 2, self.pred_len * self.output_dim)
        )
        
        # 7. æ¸©åº¦å‚æ•°ï¼ˆç”¨äºsoftmaxè·¯ç”±ï¼‰
        self.temperature = nn.Parameter(torch.tensor(1.0))
        
        # 8. å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(self.hidden_dim)
        self.output_norm = nn.LayerNorm(self.output_dim)
        
        # ç§»é™¤ä¸å¿…è¦çš„ç»„ä»¶
        # - åˆ é™¤ä½ç½®ç¼–ç ï¼ˆä¸“å®¶ç½‘ç»œå†…éƒ¨å¤„ç†ï¼‰
        # - åˆ é™¤å¤æ‚çš„è·¯ç”±å™¨ï¼ˆä½¿ç”¨ç®€å•çš„åŸå‹è·ç¦»ï¼‰
        # - åˆ é™¤å¤šæ ·æ€§åŸå‹ï¼ˆä¸“å®¶åŸå‹å·²è¶³å¤Ÿï¼‰
    
    def _handle_mamba_devices(self):
        """è‡ªåŠ¨å¤„ç†Mambaä¸“å®¶çš„è®¾å¤‡åˆ‡æ¢"""
        cuda_available = torch.cuda.is_available()
        
        for i, expert in enumerate(self.experts):
            if hasattr(expert, 'use_mamba') and expert.use_mamba:
                if cuda_available:
                    print(f"ä¸“å®¶{i}ä½¿ç”¨Mambaï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CUDA")
                    expert = expert.cuda()
                    # æ›´æ–°ä¸“å®¶åˆ—è¡¨ä¸­çš„å¼•ç”¨
                    self.experts[i] = expert
                else:
                    print(f"ä¸“å®¶{i}: CUDAä¸å¯ç”¨ï¼ŒMambaä¸“å®¶å°†åˆ‡æ¢åˆ°LSTMæ¨¡å¼")
                    expert.use_mamba = False
                    if hasattr(expert, '_init_lstm_fallback'):
                        expert._init_lstm_fallback()
    
    def _init_temperature_scheduler(self):
        """åˆå§‹åŒ–æ¸©åº¦è°ƒåº¦å™¨"""
        temp_config = self.model_config.get('temperature', {})
        initial_temp = temp_config.get('initial', 1.0)
        self.temperature.data = torch.tensor(initial_temp, dtype=torch.float32)
        
        self.temp_min = temp_config.get('min', 0.1)
        self.temp_max = temp_config.get('max', 5.0)
        self.temp_decay = temp_config.get('decay', 0.95)
        self.temp_schedule = temp_config.get('schedule', 'fixed')
        
        # æ¸©åº¦è°ƒåº¦ç»Ÿè®¡
        self.temp_stats = {
            'current': self.temperature.item(),
            'adjustments': 0,
            'performance_history': []
        }
    
    def _init_loss_stats(self):
        """åˆå§‹åŒ–æŸå¤±ç»Ÿè®¡"""
        self.loss_stats = {
            'total_loss': 0.0,
            'prediction_loss': 0.0,
            'triplet_loss': 0.0,
            'diversity_loss': 0.0,
            'consistency_loss': 0.0,
            'reconstruction_loss': 0.0,
            'load_balance_loss': 0.0,
            'step_count': 0
        }
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for name, module in self.named_modules():
            if isinstance(module, nn.Linear):
                xavier_uniform_(module.weight)
                if module.bias is not None:
                    constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                constant_(module.bias, 0)
                constant_(module.weight, 1.0)
    
    def forward(self, x: torch.Tensor, ground_truth: torch.Tensor = None, 
                return_details: bool = False, return_aux_info: bool = False) -> Union[torch.Tensor, Dict]:
        """
        å‰å‘ä¼ æ’­ - ğŸ”§ ä¸¥æ ¼æŒ‰ç…§MÂ²-MOEPæ–‡æ¡£å·¥ä½œæµç¨‹
        
        å·¥ä½œæµç¨‹ï¼š
        1. æ½œåœ¨ç©ºé—´æ˜ å°„ï¼šWi â†’ zi (é€šè¿‡Flowæ¨¡å‹)
        2. åº¦é‡å­¦ä¹ é—¨æ§ï¼šzi â†’ embi (é€šè¿‡å­ªç”Ÿç¼–ç å™¨)
        3. ä¸“å®¶è·¯ç”±ï¼šembiä¸ä¸“å®¶åŸå‹è®¡ç®—è·ç¦» â†’ Î±k
        4. ä¸“å®¶é¢„æµ‹ï¼šåŸå§‹è¾“å…¥gni â†’ ä¸“å®¶è¾“å‡º
        5. ç»“æœèšåˆï¼šåŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, input_dim]
            ground_truth: çœŸå®å€¼å¼ é‡ï¼Œå¯é€‰
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯
            return_aux_info: æ˜¯å¦è¿”å›è¾…åŠ©ä¿¡æ¯ï¼ˆå…¼å®¹è®­ç»ƒå™¨ï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœæˆ–è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        # return_aux_info ä¸ return_details ç­‰ä»·
        if return_aux_info:
            return_details = True
        
        batch_size, seq_len, input_dim = x.size()
        
        # ç¡®ä¿è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        x = x.to(self.device)
        
        # ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºé‡æ„æŸå¤±
        original_input = x.clone()
        
        # === æ­¥éª¤1ï¼šæ½œåœ¨ç©ºé—´æ˜ å°„ Wi â†’ zi ===
        # æ‰å¹³åŒ–è¾“å…¥ç”¨äºFlowæ¨¡å‹
        x_flat = x.view(batch_size, -1)  # [batch_size, seq_len * input_dim]
        
        try:
            # é€šè¿‡Flowæ¨¡å‹æ˜ å°„åˆ°æ½œåœ¨ç©ºé—´
            z_latent, flow_log_det = self.flow_model(x_flat)
            
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(z_latent).any() or torch.isinf(z_latent).any():
                z_latent = x_flat
                flow_log_det = torch.zeros(batch_size, device=self.device)
                
        except Exception:
            # ç®€åŒ–å¼‚å¸¸å¤„ç†
            z_latent = x_flat
            flow_log_det = torch.zeros(batch_size, device=self.device)
        
        # === æ­¥éª¤2ï¼šåº¦é‡å­¦ä¹ é—¨æ§ zi â†’ embi ===
        # é€šè¿‡å­ªç”Ÿç¼–ç å™¨ç”ŸæˆåµŒå…¥å‘é‡
        embedding_vector = self.gating_encoder(z_latent)  # [batch_size, 128]
        
        # === æ­¥éª¤3ï¼šä¸“å®¶è·¯ç”± embiä¸ä¸“å®¶åŸå‹è®¡ç®—è·ç¦» â†’ Î±k ===
        # è®¡ç®—åµŒå…¥å‘é‡ä¸ä¸“å®¶åŸå‹çš„è·ç¦»
        distances = torch.cdist(
            embedding_vector.unsqueeze(1),  # [batch_size, 1, 128]
            self.expert_prototypes.unsqueeze(0)  # [1, num_experts, 128]
        ).squeeze(1)  # [batch_size, num_experts]
        
        # ä½¿ç”¨è´Ÿè·ç¦»å’Œæ¸©åº¦å‚æ•°è®¡ç®—è·¯ç”±æƒé‡
        routing_logits = -distances / torch.clamp(self.temperature, min=0.1, max=5.0)
        expert_weights = F.softmax(routing_logits, dim=-1)  # [batch_size, num_experts]
        
        # === æ­¥éª¤4ï¼šä¸“å®¶é¢„æµ‹ åŸå§‹è¾“å…¥gni â†’ ä¸“å®¶è¾“å‡º ===
        expert_outputs = []
        expert_details = []
        
        # ä¼˜åŒ–ï¼šç¡®ä¿æ‰€æœ‰ä¸“å®¶ç½‘ç»œåœ¨åŒä¸€è®¾å¤‡ä¸Šï¼ˆé¿å…é‡å¤æ£€æŸ¥ï¼‰
        if not self._device_initialized:
            self._ensure_experts_on_device()
            self._device_initialized = True
        
        for i, expert in enumerate(self.experts):
            try:
                # ä¸“å®¶ç½‘ç»œå¤„ç†åŸå§‹æ»‘åŠ¨çª—å£
                expert_output = expert(x, return_features=True)
                
                if isinstance(expert_output, dict):
                    expert_outputs.append(expert_output['output'])
                    expert_details.append(expert_output)
                else:
                    expert_outputs.append(expert_output)
                    expert_details.append({'output': expert_output})
                    
            except Exception:
                # ç®€åŒ–å¼‚å¸¸å¤„ç†ï¼šä½¿ç”¨é›¶å¡«å……
                fallback_output = torch.zeros(batch_size, seq_len, self.hidden_dim, device=self.device)
                expert_outputs.append(fallback_output)
                expert_details.append({'output': fallback_output, 'error': True})
        
        # === æ­¥éª¤5ï¼šç»“æœèšåˆ åŠ æƒæ±‚å’Œå¾—åˆ°æœ€ç»ˆé¢„æµ‹ ===
        expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch_size, num_experts, seq_len, hidden_dim]
        expert_weights_expanded = expert_weights.unsqueeze(-1).unsqueeze(-1)  # [batch_size, num_experts, 1, 1]
        
        # åŠ æƒæ±‚å’Œ
        fused_output = torch.sum(expert_outputs * expert_weights_expanded, dim=1)  # [batch_size, seq_len, hidden_dim]
        
        # ç‰¹å¾èåˆ
        fused_output = self.feature_fusion(fused_output)
        fused_output = self.layer_norm(fused_output)
        
        # é¢„æµ‹å¤´ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾è¿›è¡Œé¢„æµ‹
        last_hidden = fused_output[:, -1, :]  # [batch_size, hidden_dim]
        predictions = self.prediction_head(last_hidden)  # [batch_size, pred_len * output_dim]
        predictions = predictions.view(batch_size, self.pred_len, self.output_dim)
        predictions = self.output_norm(predictions)
        
        if return_details:
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®ç”Ÿæˆexpert_embeddings
            expert_embeddings = embedding_vector  # [batch_size, 128]
            
            # æ„å»ºå®Œæ•´çš„è¾“å‡ºå­—å…¸
            output_dict = {
                'predictions': predictions,
                'expert_weights': expert_weights,
                'expert_outputs': expert_outputs,
                'expert_details': expert_details,
                'fused_features': fused_output,
                'latent_features': z_latent,
                'embedding_vector': embedding_vector,
                'expert_prototypes': self.expert_prototypes,
                'routing_distances': distances,
                'routing_logits': routing_logits,
                'flow_log_det': flow_log_det,
                'hidden_states': last_hidden,
                'temperature': self.temperature.item(),
                'loss_stats': self.loss_stats,
                'original_input': original_input,  # æ·»åŠ åŸå§‹è¾“å…¥ç”¨äºé‡æ„æŸå¤±
                # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ­£ç¡®çš„expert_embeddingsåˆ°aux_info
                'aux_info': {
                    'expert_weights': expert_weights,
                    'expert_embeddings': expert_embeddings,  # ğŸ”§ å…³é”®ä¿®å¤
                    'flow_embeddings': z_latent,
                    'flow_log_det': flow_log_det,
                    'routing_entropy': -torch.sum(expert_weights * torch.log(expert_weights + 1e-8), dim=-1).mean().item(),
                    'temperature': self.temperature.item(),
                    'num_experts_used': (expert_weights > 0.01).sum(dim=1).float().mean().item(),
                    'prototype_distances': distances.mean(dim=0).tolist(),
                    'original_input': original_input  # æ·»åŠ åŸå§‹è¾“å…¥
                }
            }
            
            return output_dict
        else:
            return {'predictions': predictions}
    
    def compute_loss(self, outputs: Dict, targets: torch.Tensor, epoch: int = 0) -> Dict:
        """
        è®¡ç®—MÂ²-MOEPå¤åˆæŸå¤±å‡½æ•° - ğŸ”§ æŒ‰ç…§æ–‡æ¡£è¦æ±‚é‡æ–°å®ç°
        
        å¤åˆæŸå¤±åŒ…å«ï¼š
        1. é‡æ„æŸå¤± (Lrc): ç¡®ä¿Flowæ¨¡å‹ä¿ç•™åŸå§‹åºåˆ—ä¿¡æ¯
        2. è·¯ç”±æŸå¤± (Lcl): ä¸‰å…ƒç»„æŸå¤±è®­ç»ƒåº¦é‡å­¦ä¹ é—¨æ§
        3. é¢„æµ‹æŸå¤± (Lpr): ä¸»è¦çš„é¢„æµ‹æ€§èƒ½æŒ‡æ ‡
        
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
            targets: ç›®æ ‡å¼ é‡ [batch_size, pred_len, output_dim]
            epoch: å½“å‰è®­ç»ƒè½®æ¬¡
            
        Returns:
            æŸå¤±å­—å…¸
        """
        predictions = outputs['predictions']
        aux_info = outputs.get('aux_info', {})
        
        # ç¡®ä¿é¢„æµ‹å’Œç›®æ ‡åœ¨åŒä¸€è®¾å¤‡
        predictions = predictions.to(targets.device)
        
        # æŸå¤±æƒé‡é…ç½®
        loss_weights = self.training_config.get('loss_weights', {})
        
        # === 1. é¢„æµ‹æŸå¤± (Lpr) - ä¸»è¦æŸå¤± ===
        prediction_loss = F.mse_loss(predictions, targets)
        
        # === 2. é‡æ„æŸå¤± (Lrc) - Flowæ¨¡å‹ä¿çœŸåº¦ ===
        reconstruction_loss = self._compute_flow_reconstruction_loss(
            outputs.get('latent_features'),
            outputs.get('flow_log_det'),
            outputs.get('original_input')  # ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è¾“å…¥
        )
        
        # === 3. è·¯ç”±æŸå¤± (Lcl) - ä¸‰å…ƒç»„æŸå¤± ===
        triplet_loss = self._compute_triplet_routing_loss(
            aux_info.get('expert_embeddings'),
            aux_info.get('expert_weights'),
            predictions,
            targets
        )
        
        # === 4. ä¸“å®¶åŸå‹æ­£åˆ™åŒ–æŸå¤± ===
        prototype_reg_loss = self._compute_prototype_regularization()
        
        # === 5. è´Ÿè½½å‡è¡¡æŸå¤± ===
        load_balance_loss = self._compute_load_balance_loss(
            aux_info.get('expert_weights')
        )
        
        # === å¤åˆæŸå¤±è®¡ç®— ===
        # ä½¿ç”¨æ–‡æ¡£ä¸­æåˆ°çš„ä¸ç¡®å®šæ€§åŠ æƒæ–¹æ³•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        total_loss = (
            loss_weights.get('prediction', 1.0) * prediction_loss +
            loss_weights.get('reconstruction', 0.1) * reconstruction_loss +
            loss_weights.get('triplet', 0.1) * triplet_loss +
            loss_weights.get('prototype_reg', 0.01) * prototype_reg_loss +
            loss_weights.get('load_balance', 0.01) * load_balance_loss
        )
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("âš ï¸ æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œä½¿ç”¨å¤‡ç”¨æŸå¤±")
            total_loss = prediction_loss
        
        # æ›´æ–°æŸå¤±ç»Ÿè®¡
        self.loss_stats.update({
            'total_loss': total_loss.item(),
            'prediction_loss': prediction_loss.item(),
            'reconstruction_loss': reconstruction_loss.item(),
            'triplet_loss': triplet_loss.item(),
            'prototype_reg_loss': prototype_reg_loss.item(),
            'load_balance_loss': load_balance_loss.item(),
            'step_count': self.loss_stats['step_count'] + 1
        })
        
        # è¿”å›æŸå¤±å­—å…¸ - ç»Ÿä¸€é”®åæ ¼å¼
        return {
            'total': total_loss,
            'prediction': prediction_loss,
            'reconstruction': reconstruction_loss,
            'triplet': triplet_loss,
            'prototype': prototype_reg_loss,
            'load_balance': load_balance_loss
        }
    
    def _compute_flow_reconstruction_loss(self, latent_features: torch.Tensor, 
                                        flow_log_det: torch.Tensor, 
                                        original_input: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—Flowæ¨¡å‹é‡æ„æŸå¤± - ä¿®å¤ï¼šä½¿ç”¨åŸå§‹è¾“å…¥ä½œä¸ºå‚è€ƒ"""
        if latent_features is None or original_input is None:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        try:
            # é‡æ„åŸå§‹è¾“å…¥
            batch_size = latent_features.size(0)
            reconstructed = self.flow_model.reconstruct(latent_features)
            
            # å°†é‡æ„ç»“æœreshapeå›åŸå§‹è¾“å…¥å½¢çŠ¶
            reconstructed_input = reconstructed.view(batch_size, self.seq_len, self.input_dim)
            
            # ä½¿ç”¨åŸå§‹è¾“å…¥ä½œä¸ºé‡æ„å‚è€ƒ
            reconstruction_mse = F.mse_loss(reconstructed_input, original_input)
            
            # æ·»åŠ Flowæ¨¡å‹çš„å¯¹æ•°è¡Œåˆ—å¼é¡¹ï¼ˆæ­£åˆ™åŒ–ï¼‰
            if flow_log_det is not None:
                log_det_reg = torch.mean(flow_log_det ** 2) * 0.01
                reconstruction_loss = reconstruction_mse + log_det_reg
            else:
                reconstruction_loss = reconstruction_mse
            
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(reconstruction_loss) or torch.isinf(reconstruction_loss):
                return torch.tensor(0.0, device=self.device, requires_grad=True)
            
            return reconstruction_loss
            
        except Exception as e:
            print(f"âš ï¸ Flowé‡æ„æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def _compute_triplet_routing_loss(self, expert_embeddings: torch.Tensor,
                                    expert_weights: torch.Tensor,
                                    predictions: torch.Tensor,
                                    targets: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä¸‰å…ƒç»„è·¯ç”±æŸå¤± - æŒ‰ç…§æ–‡æ¡£è¦æ±‚å®ç°"""
        if expert_embeddings is None or expert_weights is None:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        try:
            batch_size = expert_embeddings.size(0)
            if batch_size < 3:
                return torch.tensor(0.0, device=self.device, requires_grad=True)
            
            # è®¡ç®—é¢„æµ‹è¯¯å·®ï¼Œç”¨äºæ„å»ºä¸‰å…ƒç»„
            prediction_errors = F.mse_loss(predictions, targets, reduction='none')
            prediction_errors = prediction_errors.mean(dim=(1, 2))  # [batch_size]
            
            # æ ¹æ®é¢„æµ‹è¯¯å·®æ’åºï¼Œæ„å»ºä¸‰å…ƒç»„
            sorted_indices = torch.argsort(prediction_errors)
            
            # é€‰æ‹©é”šç‚¹ã€æ­£æ ·æœ¬ã€è´Ÿæ ·æœ¬
            num_triplets = min(batch_size // 3, 10)  # é™åˆ¶ä¸‰å…ƒç»„æ•°é‡
            if num_triplets == 0:
                return torch.tensor(0.0, device=self.device, requires_grad=True)
            
            anchors = expert_embeddings[sorted_indices[:num_triplets]]
            positives = expert_embeddings[sorted_indices[num_triplets:2*num_triplets]]
            negatives = expert_embeddings[sorted_indices[-num_triplets:]]
            
            # è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
            pos_dist = F.pairwise_distance(anchors, positives, 2)
            neg_dist = F.pairwise_distance(anchors, negatives, 2)
            
            margin = self.model_config.get('triplet', {}).get('margin', 0.5)
            triplet_loss = F.relu(pos_dist - neg_dist + margin).mean()
            
            # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
            if torch.isnan(triplet_loss) or torch.isinf(triplet_loss):
                return torch.tensor(0.0, device=self.device, requires_grad=True)
            
            return triplet_loss
            
        except Exception as e:
            print(f"âš ï¸ ä¸‰å…ƒç»„æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def _compute_prototype_regularization(self) -> torch.Tensor:
        """è®¡ç®—ä¸“å®¶åŸå‹æ­£åˆ™åŒ–æŸå¤±"""
        try:
            # é¼“åŠ±ä¸“å®¶åŸå‹ä¹‹é—´çš„å¤šæ ·æ€§
            prototypes = self.expert_prototypes  # [num_experts, 128]
            
            # è®¡ç®—åŸå‹ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = torch.mm(prototypes, prototypes.t())  # [num_experts, num_experts]
            
            # é™¤å»å¯¹è§’çº¿å…ƒç´ ï¼ˆè‡ªèº«ç›¸ä¼¼åº¦ï¼‰
            mask = torch.eye(self.num_experts, device=self.device)
            off_diagonal = similarity_matrix * (1 - mask)
            
            # æœ€å°åŒ–éå¯¹è§’çº¿å…ƒç´ ï¼ˆé¼“åŠ±åŸå‹å¤šæ ·æ€§ï¼‰
            diversity_loss = torch.mean(off_diagonal ** 2)
            
            # é˜²æ­¢åŸå‹èŒƒæ•°è¿‡å¤§
            norm_reg = torch.mean(torch.norm(prototypes, dim=1) ** 2) * 0.01
            
            return diversity_loss + norm_reg
            
        except Exception as e:
            print(f"âš ï¸ åŸå‹æ­£åˆ™åŒ–æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def _compute_load_balance_loss(self, expert_weights: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±æ‰€æœ‰ä¸“å®¶è¢«ä½¿ç”¨"""
        if expert_weights is None:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        try:
            # è®¡ç®—ä¸“å®¶æƒé‡çš„å‡åŒ€æ€§
            mean_weights = expert_weights.mean(dim=0)  # [num_experts]
            target_weight = 1.0 / self.num_experts
            
            # é¼“åŠ±æƒé‡åˆ†å¸ƒå‡åŒ€
            load_balance_loss = torch.mean((mean_weights - target_weight) ** 2)
            
            return load_balance_loss
            
        except Exception as e:
            print(f"âš ï¸ è´Ÿè½½å‡è¡¡æŸå¤±è®¡ç®—å¤±è´¥: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def update_temperature(self, performance_metric: float, epoch: int):
        """
        æ ¹æ®æŒ‡å®šçš„ç­–ç•¥æ›´æ–°æ¸©åº¦å‚æ•°

        Args:
            performance_metric (float): ç”¨äºå†³ç­–çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆä¾‹å¦‚éªŒè¯æŸå¤±ï¼‰
            epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡
        """
        if self.temp_schedule == 'fixed':
            return  # å›ºå®šæ¸©åº¦ï¼Œä¸æ›´æ–°

        # è®°å½•æ€§èƒ½ï¼Œç”¨äºæœªæ¥å¯èƒ½çš„è‡ªé€‚åº”ç­–ç•¥
        self.temp_stats['performance_history'].append(performance_metric)
        self.temp_stats['adjustments'] += 1

        if self.temp_schedule == 'exponential':
            # æŒ‡æ•°è¡°å‡
            new_temp = self.temperature.item() * self.temp_decay
        elif self.temp_schedule == 'cosine':
            # ä½™å¼¦é€€ç«
            total_epochs = self.training_config.get('epochs', 1)  # é¿å…é™¤ä»¥é›¶
            initial_temp = self.model_config.get('temperature', {}).get('initial', self.temp_max)
            
            cosine_val = 0.5 * (1 + math.cos(math.pi * epoch / total_epochs))
            new_temp = self.temp_min + (initial_temp - self.temp_min) * cosine_val
        else:
            # é»˜è®¤ä¸ºæŒ‡æ•°è¡°å‡
            new_temp = self.temperature.item() * self.temp_decay

        # é™åˆ¶æ¸©åº¦åœ¨é¢„è®¾çš„èŒƒå›´å†…
        clamped_temp = max(self.temp_min, min(new_temp, self.temp_max))
        
        # ä¿®å¤ï¼šç¡®ä¿æ¸©åº¦å‚æ•°åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        self.temperature.data = torch.tensor(clamped_temp, dtype=torch.float32, device=self.temperature.device)
        self.temp_stats['current'] = self.temperature.item()

    def to(self, device):
        """é‡å†™toæ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡"""
        super().to(device)
        
        # ç¡®ä¿æ‰€æœ‰ä¸“å®¶ç½‘ç»œéƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        # å¯¹äºä½¿ç”¨mambaçš„ä¸“å®¶ç½‘ç»œï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†
        for i, expert in enumerate(self.experts):
            try:
                expert.to(device)
            except Exception as e:
                print(f"âš ï¸ ä¸“å®¶{i}ç§»åŠ¨åˆ°è®¾å¤‡{device}æ—¶å‡ºé”™: {e}")
                # å¦‚æœæ˜¯mambaç›¸å…³çš„é”™è¯¯ï¼Œå¯èƒ½éœ€è¦å¼ºåˆ¶åœ¨CUDAä¸Š
                if hasattr(expert, 'use_mamba') and expert.use_mamba:
                    if device.type != 'cuda' and torch.cuda.is_available():
                        print(f"âš ï¸ ä¸“å®¶{i}ä½¿ç”¨mambaï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ°CUDA")
                        expert.to(torch.device('cuda'))
                    else:
                        print(f"âš ï¸ ä¸“å®¶{i}åˆ‡æ¢åˆ°LSTMæ¨¡å¼")
                        expert.use_mamba = False
                        # é‡æ–°åˆå§‹åŒ–ä¸ºLSTM
                        expert._init_lstm_fallback()
                        expert.to(device)
                else:
                    expert.to(device)
        
        self.device = device
        return self
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        expert_params = []
        for i, expert in enumerate(self.experts):
            expert_param_count = sum(p.numel() for p in expert.parameters())
            expert_params.append({
                'expert_id': i,
                'parameters': expert_param_count,
                'use_mamba': getattr(expert, 'use_mamba', False)
            })
        
        return {
            'model_name': 'MÂ²-MOEP',
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'expert_parameters': expert_params,
            'num_experts': self.num_experts,
            'input_dim': self.input_dim,
            'hidden_dim': self.hidden_dim,
            'output_dim': self.output_dim,
            'seq_len': self.seq_len,
            'pred_len': self.pred_len,
            'temperature': self.temperature,
            'device': str(self.device)
        }

    def _ensure_experts_on_device(self):
        """ç¡®ä¿æ‰€æœ‰ä¸“å®¶ç½‘ç»œéƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Šï¼ˆåªåœ¨åˆå§‹åŒ–æ—¶è°ƒç”¨ä¸€æ¬¡ï¼‰"""
        for i, expert in enumerate(self.experts):
            try:
                expert.to(self.device)
            except Exception as e:
                print(f"âš ï¸ ä¸“å®¶{i}ç§»åŠ¨åˆ°è®¾å¤‡{self.device}æ—¶å‡ºé”™: {e}")
                # å¦‚æœæ˜¯mambaç›¸å…³çš„é”™è¯¯ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                if hasattr(expert, 'use_mamba') and expert.use_mamba:
                    if self.device.type != 'cuda' and torch.cuda.is_available():
                        print(f"âš ï¸ ä¸“å®¶{i}ä½¿ç”¨mambaï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ°CUDA")
                        expert.to(torch.device('cuda'))
                    else:
                        print(f"âš ï¸ ä¸“å®¶{i}åˆ‡æ¢åˆ°LSTMæ¨¡å¼")
                        expert.use_mamba = False
                        if hasattr(expert, '_init_lstm_fallback'):
                            expert._init_lstm_fallback()
                        expert.to(self.device)
                else:
                    expert.to(self.device) 